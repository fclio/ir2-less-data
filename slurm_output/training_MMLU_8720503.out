torchrun --nproc_per_node 1 --nnodes 1 --rdzv-id=18412 --rdzv_backend c10d -m less.train.train --do_train True --max_seq_length 2048 --use_fast_tokenizer True --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0.0 --evaluation_strategy no --logging_steps 1 --save_strategy no --num_train_epochs 4 --bf16 True --tf32 False --fp16 False --overwrite_output_dir True --report_to wandb --optim adamw_torch --seed 0 --percentage 1.0 --save_strategy epoch --lora True --lora_r 128 --lora_alpha 512 --lora_dropout 0.1 --lora_target_modules q_proj k_proj v_proj o_proj --learning_rate 2e-05 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --model_name_or_path meta-llama/Llama-2-7b-hf --output_dir /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4 --train_files /scratch-shared/ir2-less/selected_data/llama2-7b-p0.05-lora-seed4/mmlu/top_p0.05.jsonl 2>&1 | tee /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/train.log
[2024-11-27 21:40:11,359] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: colinnyuh (colinnyuh-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/scur2847/.netrc
11/27/2024 21:40:19 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/27/2024 21:40:19 - INFO - __main__ - Training parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
analysis_dataset=bbh,
analysis_mode=False,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=32,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/runs/Nov27_21-40-16_gcn147.local.snellius.surf.nl,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=4.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=/scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=0,
skip_memory_metrics=True,
split_batches=False,
tf32=False,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_dataset_names=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
11/27/2024 21:40:19 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='meta-llama/Llama-2-7b-hf', config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, torch_dtype=None, lora=True, lora_r=128, lora_alpha=512.0, lora_dropout=0.1, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'])
11/27/2024 21:40:19 - INFO - __main__ - Dataset parameters DataArguments(train_files=['/scratch-shared/ir2-less/selected_data/llama2-7b-p0.05-lora-seed4/mmlu/top_p0.05.jsonl'], overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=2048, sample_data_seed=42, percentage=1.0)
/home/scur2847/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[INFO|tokenization_utils_base.py:2026] 2024-11-27 21:40:19,617 >> loading file tokenizer.model from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model
[INFO|tokenization_utils_base.py:2026] 2024-11-27 21:40:19,617 >> loading file tokenizer.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json
[INFO|tokenization_utils_base.py:2026] 2024-11-27 21:40:19,617 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2026] 2024-11-27 21:40:19,617 >> loading file special_tokens_map.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json
[INFO|tokenization_utils_base.py:2026] 2024-11-27 21:40:19,617 >> loading file tokenizer_config.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json
Using custom data configuration default-bd49891f93601701
11/27/2024 21:40:20 - INFO - datasets.builder - Using custom data configuration default-bd49891f93601701
Loading Dataset Infos from /home/scur2847/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
11/27/2024 21:40:20 - INFO - datasets.info - Loading Dataset Infos from /home/scur2847/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
11/27/2024 21:40:20 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
11/27/2024 21:40:20 - INFO - datasets.info - Loading Dataset info from /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
11/27/2024 21:40:20 - INFO - datasets.builder - Found cached dataset json (/home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
11/27/2024 21:40:20 - INFO - datasets.info - Loading Dataset info from /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Process #0 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00000_of_00010.arrow
11/27/2024 21:40:20 - INFO - datasets.arrow_dataset - Process #0 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00000_of_00010.arrow
Process #1 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00001_of_00010.arrow
11/27/2024 21:40:20 - INFO - datasets.arrow_dataset - Process #1 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00001_of_00010.arrow
Process #2 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00002_of_00010.arrow
11/27/2024 21:40:20 - INFO - datasets.arrow_dataset - Process #2 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00002_of_00010.arrow
Process #3 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00003_of_00010.arrow
11/27/2024 21:40:20 - INFO - datasets.arrow_dataset - Process #3 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00003_of_00010.arrow
Process #4 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00004_of_00010.arrow
11/27/2024 21:40:20 - INFO - datasets.arrow_dataset - Process #4 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00004_of_00010.arrow
Process #5 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00005_of_00010.arrow
11/27/2024 21:40:20 - INFO - datasets.arrow_dataset - Process #5 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00005_of_00010.arrow
Process #6 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00006_of_00010.arrow
11/27/2024 21:40:20 - INFO - datasets.arrow_dataset - Process #6 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00006_of_00010.arrow
Process #7 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00007_of_00010.arrow
11/27/2024 21:40:20 - INFO - datasets.arrow_dataset - Process #7 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00007_of_00010.arrow
Process #8 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00008_of_00010.arrow
11/27/2024 21:40:20 - INFO - datasets.arrow_dataset - Process #8 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00008_of_00010.arrow
Process #9 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00009_of_00010.arrow
11/27/2024 21:40:20 - INFO - datasets.arrow_dataset - Process #9 will write at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_00009_of_00010.arrow
Loading cached processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_*_of_00010.arrow
11/27/2024 21:40:20 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-7fe19f185bf10aee_*_of_00010.arrow
Concatenating 10 shards
11/27/2024 21:40:20 - INFO - datasets.arrow_dataset - Concatenating 10 shards
[INFO|configuration_utils.py:739] 2024-11-27 21:40:20,306 >> loading configuration file config.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json
[INFO|configuration_utils.py:802] 2024-11-27 21:40:20,307 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3344] 2024-11-27 21:40:20,396 >> loading weights file model.safetensors from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json
[INFO|configuration_utils.py:826] 2024-11-27 21:40:20,400 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.29s/it]
[INFO|modeling_utils.py:4185] 2024-11-27 21:40:27,056 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4193] 2024-11-27 21:40:27,057 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:781] 2024-11-27 21:40:27,167 >> loading configuration file generation_config.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/generation_config.json
[INFO|configuration_utils.py:826] 2024-11-27 21:40:27,167 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|modeling_utils.py:1813] 2024-11-27 21:40:27,173 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
11/27/2024 21:40:30 - INFO - __main__ - Applied LoRA to model.
trainable params: 134,217,728 || all params: 6,872,641,536 || trainable%: 1.9529278123549145
Loading cached processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-515e854d94e06e99.arrow
11/27/2024 21:40:30 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-bd49891f93601701/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-515e854d94e06e99.arrow
[train set] examples: 3533; # avg tokens: 243.75657653808594
[train set] examples: 3533; # avg completion tokens: 159.47891235351562
11/27/2024 21:40:30 - INFO - __main__ - Sample 3458 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13,  9544,  7420,   278,
         1820, 12651,  1546,  3758,   322,  1939,  4176, 21218, 29889,  1152,
         1269,  4328, 29892,  3867,  6455,   310, 18845,   988,   393,  4328,
          723,  1207,  1269,  2566,   901,  8210, 29889,    13, 29966, 29989,
          465, 22137, 29989, 29958,    13,  8439,   526,  3196,  1820, 12651,
         1546,  3758,   322,  1939,  4176, 21218, 29889,  3758, 21218,   526,
         1591, 29899,  6707, 29892,  1550,  1939,  4176, 21218,   508,   367,
         1842, 29899,  6707, 29892,  1820, 29899,  1767, 11000, 29892,   322,
         3983, 21218, 29889,  3758, 21218,   526,  4837,  1711,  8716,   519,
        29892,  1550,  1939,  4176, 21218,   526,  4029,  6753,   635,  8716,
          519, 29889,  3758, 21218,   505,   263,   758, 12119, 10938, 29892,
         1550,  1939,  4176, 21218,   671,   263,  7343, 10938,   363,   443,
         4984,  2955,   848, 29889, 19814, 29892, 13807,   390,  4051,  4345,
         3913,  3758,  5877,   304,  3787,   322, 10563,   848,   363,  4340,
         1663,  5861, 29892,  1550,  1939,  4176,   427,  2388,   465,   267,
          263,  9377,  3464,   310,  2566,  5722, 11763,   393,   508,  3787,
         2281,  2955, 29892, 12647, 29899,  4984,  2955, 29892,   322,   443,
         4984,  2955,   848, 29889,    13, 10401, 23906,   607,  1134,   310,
         2566,   304,   671,   297,   263,  2183,  6434, 29892,   372,   338,
         4100,   304,  2050,   278,  1134,   310,   848,  1641,  6087, 29889,
         1152,  1342, 29892,   565,   278,   848,   338,  2281,  2955,   322,
         6858,  2473, 29899,   798, 22160,   313, 14565,   408, 18161,  6475,
          511,   769,   385,  3758,  2566,   723,   367,   901,  8210, 29889,
         1551,   278,   916,  1361, 29892,   565,   278,   848,   338,   443,
         4984,  2955,   313, 14565,   408, 10701,   470,  4663, 29897,   470,
         4225,   304,  6287,  9098,  2861,   304,  2919, 26999,   310, 23235,
          848,   313, 14565,   408,  5264,  5745, 11803,   511,   769,   263,
         1939,  4176,  2566,   723,   367,   901, 13907, 29889, 19814, 29892,
          565,  3438, 19201,   338,  4100,   746, 16743,   411,  2919, 26999,
          310,   848,   313, 14565,   408,  1856, 16114,  1199,   511,   769,
          263,  1939,  4176,  2566,  1122,   367,  5821,   519,  2861,   304,
          967, 11509,   304,   671,   844,   397,   537, 12837,   363,  2253,
         4180, 29889,     2, 29871,    13, 29966, 29989,  1792, 29989, 29958,
           13, 12984,   366,  5649,   304,   592,   825,   376,   510,  1545,
          537, 12837, 29908,   338, 29892,   322,   920,   393, 16688,  2253,
         4180,  1135,  1661, 29899,   510,  1545,   537, 12837, 29973,    13,
        29966, 29989,   465, 22137, 29989, 29958,    13,  1523,  1545,   537,
        12837,   338,   263,  1840,   393, 14637,   304, 12837,   393,   338,
         3625,   304,   278,  2498,   970,   322,   338,   451,  7371,   470,
         8688, 10816,   363,   263,  2183,  2280, 29889,   422,  1545,   537,
        12837,   338,  6892,  3109, 19390,  1135,  2888,   470, 24440,   653,
        12837,   322,   508,   367, 20848,   297,  2919, 26855, 29889,  3834,
         6455,   310,   844,   397,   537, 12837,  3160, 12424, 29892, 23226,
        29892, 14379, 29892,  8635,  9224, 29892,   322,  7047, 29889,   422,
         1545,   537, 12837,   508,  5957,  2253,  4180,  1135,  1661, 29899,
          510,  1545,   537, 12837,  1363,   372,   338,  8688,   304,   664,
          411,   263,  9377,  3464,   310,  8324,   322, 13598,  6757, 29889,
          422,  1545,   537, 12837,   338,   884,  5491,  6775,   304,  7344,
          322, 14955,  1135, 24440,   653, 12837, 29889,  2398, 29892,   777,
          844,   397,   537, 12837,   508,   367,  3109, 23279,  1135, 24440,
          653, 12837, 29889,     2, 29871,    13]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  8439,   526,  3196,  1820, 12651,
         1546,  3758,   322,  1939,  4176, 21218, 29889,  3758, 21218,   526,
         1591, 29899,  6707, 29892,  1550,  1939,  4176, 21218,   508,   367,
         1842, 29899,  6707, 29892,  1820, 29899,  1767, 11000, 29892,   322,
         3983, 21218, 29889,  3758, 21218,   526,  4837,  1711,  8716,   519,
        29892,  1550,  1939,  4176, 21218,   526,  4029,  6753,   635,  8716,
          519, 29889,  3758, 21218,   505,   263,   758, 12119, 10938, 29892,
         1550,  1939,  4176, 21218,   671,   263,  7343, 10938,   363,   443,
         4984,  2955,   848, 29889, 19814, 29892, 13807,   390,  4051,  4345,
         3913,  3758,  5877,   304,  3787,   322, 10563,   848,   363,  4340,
         1663,  5861, 29892,  1550,  1939,  4176,   427,  2388,   465,   267,
          263,  9377,  3464,   310,  2566,  5722, 11763,   393,   508,  3787,
         2281,  2955, 29892, 12647, 29899,  4984,  2955, 29892,   322,   443,
         4984,  2955,   848, 29889,    13, 10401, 23906,   607,  1134,   310,
         2566,   304,   671,   297,   263,  2183,  6434, 29892,   372,   338,
         4100,   304,  2050,   278,  1134,   310,   848,  1641,  6087, 29889,
         1152,  1342, 29892,   565,   278,   848,   338,  2281,  2955,   322,
         6858,  2473, 29899,   798, 22160,   313, 14565,   408, 18161,  6475,
          511,   769,   385,  3758,  2566,   723,   367,   901,  8210, 29889,
         1551,   278,   916,  1361, 29892,   565,   278,   848,   338,   443,
         4984,  2955,   313, 14565,   408, 10701,   470,  4663, 29897,   470,
         4225,   304,  6287,  9098,  2861,   304,  2919, 26999,   310, 23235,
          848,   313, 14565,   408,  5264,  5745, 11803,   511,   769,   263,
         1939,  4176,  2566,   723,   367,   901, 13907, 29889, 19814, 29892,
          565,  3438, 19201,   338,  4100,   746, 16743,   411,  2919, 26999,
          310,   848,   313, 14565,   408,  1856, 16114,  1199,   511,   769,
          263,  1939,  4176,  2566,  1122,   367,  5821,   519,  2861,   304,
          967, 11509,   304,   671,   844,   397,   537, 12837,   363,  2253,
         4180, 29889,     2, 29871,    13,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  1523,  1545,   537,
        12837,   338,   263,  1840,   393, 14637,   304, 12837,   393,   338,
         3625,   304,   278,  2498,   970,   322,   338,   451,  7371,   470,
         8688, 10816,   363,   263,  2183,  2280, 29889,   422,  1545,   537,
        12837,   338,  6892,  3109, 19390,  1135,  2888,   470, 24440,   653,
        12837,   322,   508,   367, 20848,   297,  2919, 26855, 29889,  3834,
         6455,   310,   844,   397,   537, 12837,  3160, 12424, 29892, 23226,
        29892, 14379, 29892,  8635,  9224, 29892,   322,  7047, 29889,   422,
         1545,   537, 12837,   508,  5957,  2253,  4180,  1135,  1661, 29899,
          510,  1545,   537, 12837,  1363,   372,   338,  8688,   304,   664,
          411,   263,  9377,  3464,   310,  8324,   322, 13598,  6757, 29889,
          422,  1545,   537, 12837,   338,   884,  5491,  6775,   304,  7344,
          322, 14955,  1135, 24440,   653, 12837, 29889,  2398, 29892,   777,
          844,   397,   537, 12837,   508,   367,  3109, 23279,  1135, 24440,
          653, 12837, 29889,     2, 29871,    13]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1])}.
11/27/2024 21:40:30 - INFO - __main__ - trainable model_params: 134217728
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32001, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
    )
  )
)
/home/scur2847/.local/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
[INFO|trainer.py:568] 2024-11-27 21:40:32,070 >> Using auto half precision backend
start training!!!!
[INFO|trainer.py:1706] 2024-11-27 21:40:33,109 >> ***** Running training *****
[INFO|trainer.py:1707] 2024-11-27 21:40:33,109 >>   Num examples = 3,533
[INFO|trainer.py:1708] 2024-11-27 21:40:33,109 >>   Num Epochs = 4
[INFO|trainer.py:1709] 2024-11-27 21:40:33,109 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1712] 2024-11-27 21:40:33,109 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1713] 2024-11-27 21:40:33,109 >>   Gradient Accumulation steps = 32
[INFO|trainer.py:1714] 2024-11-27 21:40:33,109 >>   Total optimization steps = 440
[INFO|trainer.py:1715] 2024-11-27 21:40:33,112 >>   Number of trainable parameters = 134,217,728
[INFO|integration_utils.py:722] 2024-11-27 21:40:33,115 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /gpfs/home1/scur2847/ir2-less-data/wandb/run-20241127_214033-qiy5oq59
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-smoke-79
wandb: ⭐️ View project at https://wandb.ai/colinnyuh-university-of-amsterdam/huggingface
wandb: 🚀 View run at https://wandb.ai/colinnyuh-university-of-amsterdam/huggingface/runs/qiy5oq59
  0%|          | 0/440 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-27 21:40:33,946 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/440 [00:04<34:15,  4.68s/it]                                               {'loss': 3.0202, 'learning_rate': 1.4285714285714286e-06, 'epoch': 0.01}
  0%|          | 1/440 [00:04<34:15,  4.68s/it]  0%|          | 2/440 [00:08<32:08,  4.40s/it]                                               {'loss': 3.0265, 'learning_rate': 2.8571428571428573e-06, 'epoch': 0.02}
  0%|          | 2/440 [00:08<32:08,  4.40s/it]  1%|          | 3/440 [00:13<31:44,  4.36s/it]                                               {'loss': 3.2799, 'learning_rate': 4.2857142857142855e-06, 'epoch': 0.03}
  1%|          | 3/440 [00:13<31:44,  4.36s/it]  1%|          | 4/440 [00:17<31:37,  4.35s/it]                                               {'loss': 2.3721, 'learning_rate': 5.7142857142857145e-06, 'epoch': 0.04}
  1%|          | 4/440 [00:17<31:37,  4.35s/it]  1%|          | 5/440 [00:21<31:26,  4.34s/it]                                               {'loss': 2.2933, 'learning_rate': 7.1428571428571436e-06, 'epoch': 0.05}
  1%|          | 5/440 [00:21<31:26,  4.34s/it]  1%|▏         | 6/440 [00:26<31:07,  4.30s/it]                                               {'loss': 3.0383, 'learning_rate': 8.571428571428571e-06, 'epoch': 0.05}
  1%|▏         | 6/440 [00:26<31:07,  4.30s/it]  2%|▏         | 7/440 [00:30<30:58,  4.29s/it]                                               {'loss': 2.2609, 'learning_rate': 1e-05, 'epoch': 0.06}
  2%|▏         | 7/440 [00:30<30:58,  4.29s/it]  2%|▏         | 8/440 [00:34<30:44,  4.27s/it]                                               {'loss': 2.5454, 'learning_rate': 1.1428571428571429e-05, 'epoch': 0.07}
  2%|▏         | 8/440 [00:34<30:44,  4.27s/it]  2%|▏         | 9/440 [00:38<30:37,  4.26s/it]                                               {'loss': 2.1892, 'learning_rate': 1.2857142857142859e-05, 'epoch': 0.08}
  2%|▏         | 9/440 [00:38<30:37,  4.26s/it]  2%|▏         | 10/440 [00:43<30:25,  4.24s/it]                                                {'loss': 1.8185, 'learning_rate': 1.4285714285714287e-05, 'epoch': 0.09}
  2%|▏         | 10/440 [00:43<30:25,  4.24s/it]  2%|▎         | 11/440 [00:47<30:18,  4.24s/it]                                                {'loss': 2.2329, 'learning_rate': 1.5714285714285715e-05, 'epoch': 0.1}
  2%|▎         | 11/440 [00:47<30:18,  4.24s/it]  3%|▎         | 12/440 [00:51<30:06,  4.22s/it]                                                {'loss': 1.8184, 'learning_rate': 1.7142857142857142e-05, 'epoch': 0.11}
  3%|▎         | 12/440 [00:51<30:06,  4.22s/it]  3%|▎         | 13/440 [00:55<29:56,  4.21s/it]                                                {'loss': 1.8504, 'learning_rate': 1.8571428571428575e-05, 'epoch': 0.12}
  3%|▎         | 13/440 [00:55<29:56,  4.21s/it]  3%|▎         | 14/440 [00:59<30:04,  4.24s/it]                                                {'loss': 1.4619, 'learning_rate': 2e-05, 'epoch': 0.13}
  3%|▎         | 14/440 [00:59<30:04,  4.24s/it]  3%|▎         | 15/440 [01:04<29:57,  4.23s/it]                                                {'loss': 1.4293, 'learning_rate': 1.995305164319249e-05, 'epoch': 0.14}
  3%|▎         | 15/440 [01:04<29:57,  4.23s/it]  4%|▎         | 16/440 [01:08<29:40,  4.20s/it]                                                {'loss': 1.5904, 'learning_rate': 1.9906103286384977e-05, 'epoch': 0.14}
  4%|▎         | 16/440 [01:08<29:40,  4.20s/it]  4%|▍         | 17/440 [01:12<29:30,  4.19s/it]                                                {'loss': 1.5445, 'learning_rate': 1.9859154929577465e-05, 'epoch': 0.15}
  4%|▍         | 17/440 [01:12<29:30,  4.19s/it]  4%|▍         | 18/440 [01:16<29:33,  4.20s/it]                                                {'loss': 1.516, 'learning_rate': 1.9812206572769953e-05, 'epoch': 0.16}
  4%|▍         | 18/440 [01:16<29:33,  4.20s/it]  4%|▍         | 19/440 [01:20<29:23,  4.19s/it]                                                {'loss': 1.2356, 'learning_rate': 1.9765258215962445e-05, 'epoch': 0.17}
  4%|▍         | 19/440 [01:20<29:23,  4.19s/it]  5%|▍         | 20/440 [01:24<29:12,  4.17s/it]                                                {'loss': 1.2704, 'learning_rate': 1.9718309859154933e-05, 'epoch': 0.18}
  5%|▍         | 20/440 [01:24<29:12,  4.17s/it]  5%|▍         | 21/440 [01:29<29:03,  4.16s/it]                                                {'loss': 1.2112, 'learning_rate': 1.967136150234742e-05, 'epoch': 0.19}
  5%|▍         | 21/440 [01:29<29:03,  4.16s/it]  5%|▌         | 22/440 [01:33<28:58,  4.16s/it]                                                {'loss': 1.1708, 'learning_rate': 1.962441314553991e-05, 'epoch': 0.2}
  5%|▌         | 22/440 [01:33<28:58,  4.16s/it]  5%|▌         | 23/440 [01:37<29:02,  4.18s/it]                                                {'loss': 1.3176, 'learning_rate': 1.9577464788732396e-05, 'epoch': 0.21}
  5%|▌         | 23/440 [01:37<29:02,  4.18s/it]  5%|▌         | 24/440 [01:41<29:04,  4.19s/it]                                                {'loss': 1.4682, 'learning_rate': 1.9530516431924884e-05, 'epoch': 0.22}
  5%|▌         | 24/440 [01:41<29:04,  4.19s/it]  6%|▌         | 25/440 [01:45<29:04,  4.20s/it]                                                {'loss': 1.294, 'learning_rate': 1.9483568075117372e-05, 'epoch': 0.23}
  6%|▌         | 25/440 [01:45<29:04,  4.20s/it]  6%|▌         | 26/440 [01:50<28:53,  4.19s/it]                                                {'loss': 1.2834, 'learning_rate': 1.943661971830986e-05, 'epoch': 0.24}
  6%|▌         | 26/440 [01:50<28:53,  4.19s/it]  6%|▌         | 27/440 [01:54<28:54,  4.20s/it]                                                {'loss': 1.1331, 'learning_rate': 1.9389671361502348e-05, 'epoch': 0.24}
  6%|▌         | 27/440 [01:54<28:54,  4.20s/it]  6%|▋         | 28/440 [01:58<28:59,  4.22s/it]                                                {'loss': 1.3298, 'learning_rate': 1.9342723004694836e-05, 'epoch': 0.25}
  6%|▋         | 28/440 [01:58<28:59,  4.22s/it]  7%|▋         | 29/440 [02:02<28:46,  4.20s/it]                                                {'loss': 1.3409, 'learning_rate': 1.9295774647887327e-05, 'epoch': 0.26}
  7%|▋         | 29/440 [02:02<28:46,  4.20s/it]  7%|▋         | 30/440 [02:07<28:56,  4.24s/it]                                                {'loss': 1.168, 'learning_rate': 1.9248826291079815e-05, 'epoch': 0.27}
  7%|▋         | 30/440 [02:07<28:56,  4.24s/it]  7%|▋         | 31/440 [02:11<29:00,  4.26s/it]                                                {'loss': 1.2466, 'learning_rate': 1.9201877934272303e-05, 'epoch': 0.28}
  7%|▋         | 31/440 [02:11<29:00,  4.26s/it]  7%|▋         | 32/440 [02:15<29:05,  4.28s/it]                                                {'loss': 1.304, 'learning_rate': 1.9154929577464788e-05, 'epoch': 0.29}
  7%|▋         | 32/440 [02:15<29:05,  4.28s/it]  8%|▊         | 33/440 [02:19<28:48,  4.25s/it]                                                {'loss': 1.4456, 'learning_rate': 1.910798122065728e-05, 'epoch': 0.3}
  8%|▊         | 33/440 [02:19<28:48,  4.25s/it]  8%|▊         | 34/440 [02:24<28:43,  4.24s/it]                                                {'loss': 1.0137, 'learning_rate': 1.9061032863849767e-05, 'epoch': 0.31}
  8%|▊         | 34/440 [02:24<28:43,  4.24s/it]  8%|▊         | 35/440 [02:28<28:56,  4.29s/it]                                                {'loss': 1.2968, 'learning_rate': 1.9014084507042255e-05, 'epoch': 0.32}
  8%|▊         | 35/440 [02:28<28:56,  4.29s/it]  8%|▊         | 36/440 [02:32<28:47,  4.28s/it]                                                {'loss': 1.3431, 'learning_rate': 1.8967136150234743e-05, 'epoch': 0.33}
  8%|▊         | 36/440 [02:32<28:47,  4.28s/it]  8%|▊         | 37/440 [02:36<28:37,  4.26s/it]                                                {'loss': 1.2907, 'learning_rate': 1.892018779342723e-05, 'epoch': 0.34}
  8%|▊         | 37/440 [02:36<28:37,  4.26s/it]  9%|▊         | 38/440 [02:41<28:31,  4.26s/it]                                                {'loss': 1.3966, 'learning_rate': 1.887323943661972e-05, 'epoch': 0.34}
  9%|▊         | 38/440 [02:41<28:31,  4.26s/it]  9%|▉         | 39/440 [02:45<28:27,  4.26s/it]                                                {'loss': 1.2487, 'learning_rate': 1.882629107981221e-05, 'epoch': 0.35}
  9%|▉         | 39/440 [02:45<28:27,  4.26s/it]  9%|▉         | 40/440 [02:49<28:22,  4.26s/it]                                                {'loss': 1.2145, 'learning_rate': 1.8779342723004698e-05, 'epoch': 0.36}
  9%|▉         | 40/440 [02:49<28:22,  4.26s/it]  9%|▉         | 41/440 [02:53<28:13,  4.24s/it]                                                {'loss': 1.4922, 'learning_rate': 1.8732394366197186e-05, 'epoch': 0.37}
  9%|▉         | 41/440 [02:53<28:13,  4.24s/it] 10%|▉         | 42/440 [02:58<28:12,  4.25s/it]                                                {'loss': 1.35, 'learning_rate': 1.8685446009389673e-05, 'epoch': 0.38}
 10%|▉         | 42/440 [02:58<28:12,  4.25s/it] 10%|▉         | 43/440 [03:02<28:02,  4.24s/it]                                                {'loss': 1.0865, 'learning_rate': 1.863849765258216e-05, 'epoch': 0.39}
 10%|▉         | 43/440 [03:02<28:02,  4.24s/it] 10%|█         | 44/440 [03:06<27:56,  4.23s/it]                                                {'loss': 1.3859, 'learning_rate': 1.859154929577465e-05, 'epoch': 0.4}
 10%|█         | 44/440 [03:06<27:56,  4.23s/it] 10%|█         | 45/440 [03:10<27:47,  4.22s/it]                                                {'loss': 1.4051, 'learning_rate': 1.8544600938967137e-05, 'epoch': 0.41}
 10%|█         | 45/440 [03:10<27:47,  4.22s/it] 10%|█         | 46/440 [03:15<27:44,  4.22s/it]                                                {'loss': 1.192, 'learning_rate': 1.8497652582159625e-05, 'epoch': 0.42}
 10%|█         | 46/440 [03:15<27:44,  4.22s/it] 11%|█         | 47/440 [03:19<27:47,  4.24s/it]                                                {'loss': 1.2144, 'learning_rate': 1.8450704225352113e-05, 'epoch': 0.43}
 11%|█         | 47/440 [03:19<27:47,  4.24s/it] 11%|█         | 48/440 [03:23<27:42,  4.24s/it]                                                {'loss': 1.1911, 'learning_rate': 1.84037558685446e-05, 'epoch': 0.43}
 11%|█         | 48/440 [03:23<27:42,  4.24s/it] 11%|█         | 49/440 [03:27<27:30,  4.22s/it]                                                {'loss': 1.4048, 'learning_rate': 1.8356807511737092e-05, 'epoch': 0.44}
 11%|█         | 49/440 [03:27<27:30,  4.22s/it] 11%|█▏        | 50/440 [03:31<27:23,  4.22s/it]                                                {'loss': 1.1353, 'learning_rate': 1.830985915492958e-05, 'epoch': 0.45}
 11%|█▏        | 50/440 [03:31<27:23,  4.22s/it] 12%|█▏        | 51/440 [03:36<27:26,  4.23s/it]                                                {'loss': 1.1936, 'learning_rate': 1.8262910798122068e-05, 'epoch': 0.46}
 12%|█▏        | 51/440 [03:36<27:26,  4.23s/it] 12%|█▏        | 52/440 [03:40<27:16,  4.22s/it]                                                {'loss': 1.3769, 'learning_rate': 1.8215962441314556e-05, 'epoch': 0.47}
 12%|█▏        | 52/440 [03:40<27:16,  4.22s/it] 12%|█▏        | 53/440 [03:44<27:24,  4.25s/it]                                                {'loss': 1.343, 'learning_rate': 1.8169014084507044e-05, 'epoch': 0.48}
 12%|█▏        | 53/440 [03:44<27:24,  4.25s/it] 12%|█▏        | 54/440 [03:48<27:16,  4.24s/it]                                                {'loss': 1.0744, 'learning_rate': 1.8122065727699532e-05, 'epoch': 0.49}
 12%|█▏        | 54/440 [03:48<27:16,  4.24s/it] 12%|█▎        | 55/440 [03:53<27:10,  4.24s/it]                                                {'loss': 1.3141, 'learning_rate': 1.807511737089202e-05, 'epoch': 0.5}
 12%|█▎        | 55/440 [03:53<27:10,  4.24s/it] 13%|█▎        | 56/440 [03:57<27:15,  4.26s/it]                                                {'loss': 1.307, 'learning_rate': 1.8028169014084508e-05, 'epoch': 0.51}
 13%|█▎        | 56/440 [03:57<27:15,  4.26s/it] 13%|█▎        | 57/440 [04:01<27:10,  4.26s/it]                                                {'loss': 1.3169, 'learning_rate': 1.7981220657276996e-05, 'epoch': 0.52}
 13%|█▎        | 57/440 [04:01<27:10,  4.26s/it] 13%|█▎        | 58/440 [04:06<27:10,  4.27s/it]                                                {'loss': 1.1, 'learning_rate': 1.7934272300469484e-05, 'epoch': 0.53}
 13%|█▎        | 58/440 [04:06<27:10,  4.27s/it] 13%|█▎        | 59/440 [04:10<27:15,  4.29s/it]                                                {'loss': 1.5188, 'learning_rate': 1.7887323943661975e-05, 'epoch': 0.53}
 13%|█▎        | 59/440 [04:10<27:15,  4.29s/it] 14%|█▎        | 60/440 [04:14<26:57,  4.26s/it]                                                {'loss': 1.2804, 'learning_rate': 1.7840375586854463e-05, 'epoch': 0.54}
 14%|█▎        | 60/440 [04:14<26:57,  4.26s/it] 14%|█▍        | 61/440 [04:18<26:48,  4.24s/it]                                                {'loss': 1.0656, 'learning_rate': 1.779342723004695e-05, 'epoch': 0.55}
 14%|█▍        | 61/440 [04:18<26:48,  4.24s/it] 14%|█▍        | 62/440 [04:22<26:40,  4.23s/it]                                                {'loss': 1.2669, 'learning_rate': 1.774647887323944e-05, 'epoch': 0.56}
 14%|█▍        | 62/440 [04:22<26:40,  4.23s/it] 14%|█▍        | 63/440 [04:27<26:50,  4.27s/it]                                                {'loss': 1.4379, 'learning_rate': 1.7699530516431927e-05, 'epoch': 0.57}
 14%|█▍        | 63/440 [04:27<26:50,  4.27s/it] 15%|█▍        | 64/440 [04:31<26:38,  4.25s/it]                                                {'loss': 1.3337, 'learning_rate': 1.7652582159624414e-05, 'epoch': 0.58}
 15%|█▍        | 64/440 [04:31<26:38,  4.25s/it] 15%|█▍        | 65/440 [04:35<26:26,  4.23s/it]                                                {'loss': 1.0231, 'learning_rate': 1.7605633802816902e-05, 'epoch': 0.59}
 15%|█▍        | 65/440 [04:35<26:26,  4.23s/it] 15%|█▌        | 66/440 [04:39<26:24,  4.24s/it]                                                {'loss': 1.2612, 'learning_rate': 1.755868544600939e-05, 'epoch': 0.6}
 15%|█▌        | 66/440 [04:39<26:24,  4.24s/it] 15%|█▌        | 67/440 [04:44<26:15,  4.22s/it]                                                {'loss': 1.2859, 'learning_rate': 1.7511737089201878e-05, 'epoch': 0.61}
 15%|█▌        | 67/440 [04:44<26:15,  4.22s/it] 15%|█▌        | 68/440 [04:48<26:08,  4.22s/it]                                                {'loss': 1.2791, 'learning_rate': 1.7464788732394366e-05, 'epoch': 0.62}
 15%|█▌        | 68/440 [04:48<26:08,  4.22s/it] 16%|█▌        | 69/440 [04:52<26:08,  4.23s/it]                                                {'loss': 1.3011, 'learning_rate': 1.7417840375586857e-05, 'epoch': 0.62}
 16%|█▌        | 69/440 [04:52<26:08,  4.23s/it] 16%|█▌        | 70/440 [04:57<26:26,  4.29s/it]                                                {'loss': 1.3963, 'learning_rate': 1.7370892018779345e-05, 'epoch': 0.63}
 16%|█▌        | 70/440 [04:57<26:26,  4.29s/it] 16%|█▌        | 71/440 [05:01<26:31,  4.31s/it]                                                {'loss': 1.3976, 'learning_rate': 1.732394366197183e-05, 'epoch': 0.64}
 16%|█▌        | 71/440 [05:01<26:31,  4.31s/it] 16%|█▋        | 72/440 [05:05<26:21,  4.30s/it]                                                {'loss': 1.1063, 'learning_rate': 1.727699530516432e-05, 'epoch': 0.65}
 16%|█▋        | 72/440 [05:05<26:21,  4.30s/it] 17%|█▋        | 73/440 [05:09<26:18,  4.30s/it]                                                {'loss': 1.1001, 'learning_rate': 1.723004694835681e-05, 'epoch': 0.66}
 17%|█▋        | 73/440 [05:09<26:18,  4.30s/it] 17%|█▋        | 74/440 [05:14<26:13,  4.30s/it]                                                {'loss': 1.0777, 'learning_rate': 1.7183098591549297e-05, 'epoch': 0.67}
 17%|█▋        | 74/440 [05:14<26:13,  4.30s/it] 17%|█▋        | 75/440 [05:18<25:59,  4.27s/it]                                                {'loss': 1.2819, 'learning_rate': 1.7136150234741785e-05, 'epoch': 0.68}
 17%|█▋        | 75/440 [05:18<25:59,  4.27s/it] 17%|█▋        | 76/440 [05:22<25:44,  4.24s/it]                                                {'loss': 1.3154, 'learning_rate': 1.7089201877934273e-05, 'epoch': 0.69}
 17%|█▋        | 76/440 [05:22<25:44,  4.24s/it] 18%|█▊        | 77/440 [05:26<25:41,  4.25s/it]                                                {'loss': 1.3784, 'learning_rate': 1.704225352112676e-05, 'epoch': 0.7}
 18%|█▊        | 77/440 [05:26<25:41,  4.25s/it] 18%|█▊        | 78/440 [05:31<25:51,  4.29s/it]                                                {'loss': 1.2934, 'learning_rate': 1.6995305164319252e-05, 'epoch': 0.71}
 18%|█▊        | 78/440 [05:31<25:51,  4.29s/it] 18%|█▊        | 79/440 [05:35<25:37,  4.26s/it]                                                {'loss': 1.4003, 'learning_rate': 1.694835680751174e-05, 'epoch': 0.72}
 18%|█▊        | 79/440 [05:35<25:37,  4.26s/it] 18%|█▊        | 80/440 [05:39<25:38,  4.27s/it]                                                {'loss': 1.5302, 'learning_rate': 1.6901408450704228e-05, 'epoch': 0.72}
 18%|█▊        | 80/440 [05:39<25:38,  4.27s/it] 18%|█▊        | 81/440 [05:44<25:30,  4.26s/it]                                                {'loss': 1.167, 'learning_rate': 1.6854460093896712e-05, 'epoch': 0.73}
 18%|█▊        | 81/440 [05:44<25:30,  4.26s/it] 19%|█▊        | 82/440 [05:48<25:19,  4.24s/it]                                                {'loss': 1.2414, 'learning_rate': 1.6807511737089204e-05, 'epoch': 0.74}
 19%|█▊        | 82/440 [05:48<25:19,  4.24s/it] 19%|█▉        | 83/440 [05:52<25:13,  4.24s/it]                                                {'loss': 1.2666, 'learning_rate': 1.676056338028169e-05, 'epoch': 0.75}
 19%|█▉        | 83/440 [05:52<25:13,  4.24s/it] 19%|█▉        | 84/440 [05:56<25:20,  4.27s/it]                                                {'loss': 1.3373, 'learning_rate': 1.671361502347418e-05, 'epoch': 0.76}
 19%|█▉        | 84/440 [05:56<25:20,  4.27s/it] 19%|█▉        | 85/440 [06:01<25:23,  4.29s/it]                                                {'loss': 1.1412, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.77}
 19%|█▉        | 85/440 [06:01<25:23,  4.29s/it] 20%|█▉        | 86/440 [06:05<25:06,  4.26s/it]                                                {'loss': 1.1697, 'learning_rate': 1.6619718309859155e-05, 'epoch': 0.78}
 20%|█▉        | 86/440 [06:05<25:06,  4.26s/it] 20%|█▉        | 87/440 [06:09<24:57,  4.24s/it]                                                {'loss': 1.4702, 'learning_rate': 1.6572769953051643e-05, 'epoch': 0.79}
 20%|█▉        | 87/440 [06:09<24:57,  4.24s/it] 20%|██        | 88/440 [06:13<24:55,  4.25s/it]                                                {'loss': 1.218, 'learning_rate': 1.6525821596244135e-05, 'epoch': 0.8}
 20%|██        | 88/440 [06:13<24:55,  4.25s/it] 20%|██        | 89/440 [06:18<25:13,  4.31s/it]                                                {'loss': 1.0724, 'learning_rate': 1.6478873239436623e-05, 'epoch': 0.81}
 20%|██        | 89/440 [06:18<25:13,  4.31s/it] 20%|██        | 90/440 [06:22<24:54,  4.27s/it]                                                {'loss': 1.611, 'learning_rate': 1.643192488262911e-05, 'epoch': 0.82}
 20%|██        | 90/440 [06:22<24:54,  4.27s/it] 21%|██        | 91/440 [06:26<24:43,  4.25s/it]                                                {'loss': 1.3561, 'learning_rate': 1.6384976525821595e-05, 'epoch': 0.82}
 21%|██        | 91/440 [06:26<24:43,  4.25s/it] 21%|██        | 92/440 [06:30<24:35,  4.24s/it]                                                {'loss': 1.266, 'learning_rate': 1.6338028169014086e-05, 'epoch': 0.83}
 21%|██        | 92/440 [06:30<24:35,  4.24s/it] 21%|██        | 93/440 [06:35<24:30,  4.24s/it]                                                {'loss': 1.1054, 'learning_rate': 1.6291079812206574e-05, 'epoch': 0.84}
 21%|██        | 93/440 [06:35<24:30,  4.24s/it] 21%|██▏       | 94/440 [06:39<24:24,  4.23s/it]                                                {'loss': 1.2318, 'learning_rate': 1.6244131455399062e-05, 'epoch': 0.85}
 21%|██▏       | 94/440 [06:39<24:24,  4.23s/it] 22%|██▏       | 95/440 [06:43<24:28,  4.26s/it]                                                {'loss': 1.4357, 'learning_rate': 1.619718309859155e-05, 'epoch': 0.86}
 22%|██▏       | 95/440 [06:43<24:28,  4.26s/it] 22%|██▏       | 96/440 [06:47<24:22,  4.25s/it]                                                {'loss': 1.2167, 'learning_rate': 1.6150234741784038e-05, 'epoch': 0.87}
 22%|██▏       | 96/440 [06:47<24:22,  4.25s/it] 22%|██▏       | 97/440 [06:52<24:22,  4.26s/it]                                                {'loss': 1.3253, 'learning_rate': 1.6103286384976526e-05, 'epoch': 0.88}
 22%|██▏       | 97/440 [06:52<24:22,  4.26s/it] 22%|██▏       | 98/440 [06:56<24:16,  4.26s/it]                                                {'loss': 1.1246, 'learning_rate': 1.6056338028169017e-05, 'epoch': 0.89}
 22%|██▏       | 98/440 [06:56<24:16,  4.26s/it] 22%|██▎       | 99/440 [07:00<24:03,  4.23s/it]                                                {'loss': 1.2495, 'learning_rate': 1.6009389671361505e-05, 'epoch': 0.9}
 22%|██▎       | 99/440 [07:00<24:03,  4.23s/it] 23%|██▎       | 100/440 [07:04<23:53,  4.22s/it]                                                 {'loss': 1.0796, 'learning_rate': 1.5962441314553993e-05, 'epoch': 0.91}
 23%|██▎       | 100/440 [07:04<23:53,  4.22s/it] 23%|██▎       | 101/440 [07:08<23:49,  4.22s/it]                                                 {'loss': 1.2838, 'learning_rate': 1.591549295774648e-05, 'epoch': 0.91}
 23%|██▎       | 101/440 [07:08<23:49,  4.22s/it] 23%|██▎       | 102/440 [07:13<23:49,  4.23s/it]                                                 {'loss': 1.1925, 'learning_rate': 1.586854460093897e-05, 'epoch': 0.92}
 23%|██▎       | 102/440 [07:13<23:49,  4.23s/it] 23%|██▎       | 103/440 [07:17<23:38,  4.21s/it]                                                 {'loss': 1.3475, 'learning_rate': 1.5821596244131457e-05, 'epoch': 0.93}
 23%|██▎       | 103/440 [07:17<23:38,  4.21s/it] 24%|██▎       | 104/440 [07:21<23:35,  4.21s/it]                                                 {'loss': 1.2862, 'learning_rate': 1.5774647887323945e-05, 'epoch': 0.94}
 24%|██▎       | 104/440 [07:21<23:35,  4.21s/it] 24%|██▍       | 105/440 [07:26<23:51,  4.27s/it]                                                 {'loss': 1.2769, 'learning_rate': 1.5727699530516433e-05, 'epoch': 0.95}
 24%|██▍       | 105/440 [07:26<23:51,  4.27s/it] 24%|██▍       | 106/440 [07:30<24:14,  4.36s/it]                                                 {'loss': 1.2281, 'learning_rate': 1.568075117370892e-05, 'epoch': 0.96}
 24%|██▍       | 106/440 [07:30<24:14,  4.36s/it] 24%|██▍       | 107/440 [07:34<24:00,  4.33s/it]                                                 {'loss': 1.4352, 'learning_rate': 1.563380281690141e-05, 'epoch': 0.97}
 24%|██▍       | 107/440 [07:34<24:00,  4.33s/it] 25%|██▍       | 108/440 [07:39<23:42,  4.29s/it]                                                 {'loss': 1.2327, 'learning_rate': 1.55868544600939e-05, 'epoch': 0.98}
 25%|██▍       | 108/440 [07:39<23:42,  4.29s/it] 25%|██▍       | 109/440 [07:43<23:28,  4.26s/it]                                                 {'loss': 1.2026, 'learning_rate': 1.5539906103286388e-05, 'epoch': 0.99}
 25%|██▍       | 109/440 [07:43<23:28,  4.26s/it] 25%|██▌       | 110/440 [07:47<23:20,  4.24s/it]                                                 {'loss': 1.1378, 'learning_rate': 1.5492957746478872e-05, 'epoch': 1.0}
 25%|██▌       | 110/440 [07:47<23:20,  4.24s/it][INFO|trainer.py:2889] 2024-11-27 21:48:23,120 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tmp-checkpoint-110
[INFO|tokenization_utils_base.py:2432] 2024-11-27 21:48:23,735 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tmp-checkpoint-110/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 21:48:23,736 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tmp-checkpoint-110/special_tokens_map.json
 25%|██▌       | 111/440 [07:52<25:25,  4.64s/it]                                                 {'loss': 1.1073, 'learning_rate': 1.5446009389671363e-05, 'epoch': 1.01}
 25%|██▌       | 111/440 [07:52<25:25,  4.64s/it] 25%|██▌       | 112/440 [07:57<24:48,  4.54s/it]                                                 {'loss': 1.301, 'learning_rate': 1.539906103286385e-05, 'epoch': 1.01}
 25%|██▌       | 112/440 [07:57<24:48,  4.54s/it] 26%|██▌       | 113/440 [08:01<24:23,  4.47s/it]                                                 {'loss': 1.1619, 'learning_rate': 1.535211267605634e-05, 'epoch': 1.02}
 26%|██▌       | 113/440 [08:01<24:23,  4.47s/it] 26%|██▌       | 114/440 [08:05<24:01,  4.42s/it]                                                 {'loss': 1.1155, 'learning_rate': 1.5305164319248827e-05, 'epoch': 1.03}
 26%|██▌       | 114/440 [08:05<24:01,  4.42s/it] 26%|██▌       | 115/440 [08:10<23:38,  4.36s/it]                                                 {'loss': 1.1576, 'learning_rate': 1.5258215962441317e-05, 'epoch': 1.04}
 26%|██▌       | 115/440 [08:10<23:38,  4.36s/it] 26%|██▋       | 116/440 [08:14<23:18,  4.32s/it]                                                 {'loss': 1.2985, 'learning_rate': 1.5211267605633803e-05, 'epoch': 1.05}
 26%|██▋       | 116/440 [08:14<23:18,  4.32s/it] 27%|██▋       | 117/440 [08:18<23:02,  4.28s/it]                                                 {'loss': 1.1676, 'learning_rate': 1.5164319248826291e-05, 'epoch': 1.06}
 27%|██▋       | 117/440 [08:18<23:02,  4.28s/it] 27%|██▋       | 118/440 [08:22<23:03,  4.30s/it]                                                 {'loss': 1.1199, 'learning_rate': 1.511737089201878e-05, 'epoch': 1.07}
 27%|██▋       | 118/440 [08:22<23:03,  4.30s/it] 27%|██▋       | 119/440 [08:27<22:49,  4.27s/it]                                                 {'loss': 1.4229, 'learning_rate': 1.5070422535211269e-05, 'epoch': 1.08}
 27%|██▋       | 119/440 [08:27<22:49,  4.27s/it] 27%|██▋       | 120/440 [08:31<22:55,  4.30s/it]                                                 {'loss': 1.1769, 'learning_rate': 1.5023474178403756e-05, 'epoch': 1.09}
 27%|██▋       | 120/440 [08:31<22:55,  4.30s/it] 28%|██▊       | 121/440 [08:35<22:45,  4.28s/it]                                                 {'loss': 1.3222, 'learning_rate': 1.4976525821596246e-05, 'epoch': 1.1}
 28%|██▊       | 121/440 [08:35<22:45,  4.28s/it] 28%|██▊       | 122/440 [08:40<22:48,  4.30s/it]                                                 {'loss': 1.0239, 'learning_rate': 1.4929577464788734e-05, 'epoch': 1.11}
 28%|██▊       | 122/440 [08:40<22:48,  4.30s/it] 28%|██▊       | 123/440 [08:44<22:54,  4.33s/it]                                                 {'loss': 0.9215, 'learning_rate': 1.4882629107981222e-05, 'epoch': 1.11}
 28%|██▊       | 123/440 [08:44<22:54,  4.33s/it] 28%|██▊       | 124/440 [08:48<22:36,  4.29s/it]                                                 {'loss': 1.1526, 'learning_rate': 1.4835680751173711e-05, 'epoch': 1.12}
 28%|██▊       | 124/440 [08:48<22:36,  4.29s/it] 28%|██▊       | 125/440 [08:52<22:28,  4.28s/it]                                                 {'loss': 1.2845, 'learning_rate': 1.47887323943662e-05, 'epoch': 1.13}
 28%|██▊       | 125/440 [08:52<22:28,  4.28s/it] 29%|██▊       | 126/440 [08:57<22:32,  4.31s/it]                                                 {'loss': 1.0825, 'learning_rate': 1.4741784037558686e-05, 'epoch': 1.14}
 29%|██▊       | 126/440 [08:57<22:32,  4.31s/it] 29%|██▉       | 127/440 [09:01<22:16,  4.27s/it]                                                 {'loss': 1.3107, 'learning_rate': 1.4694835680751174e-05, 'epoch': 1.15}
 29%|██▉       | 127/440 [09:01<22:16,  4.27s/it] 29%|██▉       | 128/440 [09:05<22:05,  4.25s/it]                                                 {'loss': 1.0912, 'learning_rate': 1.4647887323943663e-05, 'epoch': 1.16}
 29%|██▉       | 128/440 [09:05<22:05,  4.25s/it] 29%|██▉       | 129/440 [09:09<21:57,  4.24s/it]                                                 {'loss': 1.2611, 'learning_rate': 1.4600938967136151e-05, 'epoch': 1.17}
 29%|██▉       | 129/440 [09:09<21:57,  4.24s/it] 30%|██▉       | 130/440 [09:14<21:55,  4.24s/it]                                                 {'loss': 1.0115, 'learning_rate': 1.4553990610328639e-05, 'epoch': 1.18}
 30%|██▉       | 130/440 [09:14<21:55,  4.24s/it] 30%|██▉       | 131/440 [09:18<21:49,  4.24s/it]                                                 {'loss': 1.2555, 'learning_rate': 1.4507042253521129e-05, 'epoch': 1.19}
 30%|██▉       | 131/440 [09:18<21:49,  4.24s/it] 30%|███       | 132/440 [09:22<21:41,  4.23s/it]                                                 {'loss': 1.2903, 'learning_rate': 1.4460093896713617e-05, 'epoch': 1.2}
 30%|███       | 132/440 [09:22<21:41,  4.23s/it] 30%|███       | 133/440 [09:26<21:38,  4.23s/it]                                                 {'loss': 1.405, 'learning_rate': 1.4413145539906104e-05, 'epoch': 1.2}
 30%|███       | 133/440 [09:26<21:38,  4.23s/it] 30%|███       | 134/440 [09:30<21:31,  4.22s/it]                                                 {'loss': 1.2545, 'learning_rate': 1.4366197183098594e-05, 'epoch': 1.21}
 30%|███       | 134/440 [09:30<21:31,  4.22s/it] 31%|███       | 135/440 [09:35<21:27,  4.22s/it]                                                 {'loss': 1.2271, 'learning_rate': 1.4319248826291082e-05, 'epoch': 1.22}
 31%|███       | 135/440 [09:35<21:27,  4.22s/it] 31%|███       | 136/440 [09:39<21:16,  4.20s/it]                                                 {'loss': 1.2395, 'learning_rate': 1.4272300469483568e-05, 'epoch': 1.23}
 31%|███       | 136/440 [09:39<21:16,  4.20s/it] 31%|███       | 137/440 [09:43<21:31,  4.26s/it]                                                 {'loss': 1.1722, 'learning_rate': 1.4225352112676058e-05, 'epoch': 1.24}
 31%|███       | 137/440 [09:43<21:31,  4.26s/it] 31%|███▏      | 138/440 [09:48<21:26,  4.26s/it]                                                 {'loss': 1.2001, 'learning_rate': 1.4178403755868546e-05, 'epoch': 1.25}
 31%|███▏      | 138/440 [09:48<21:26,  4.26s/it] 32%|███▏      | 139/440 [09:52<21:16,  4.24s/it]                                                 {'loss': 1.2962, 'learning_rate': 1.4131455399061034e-05, 'epoch': 1.26}
 32%|███▏      | 139/440 [09:52<21:16,  4.24s/it] 32%|███▏      | 140/440 [09:56<21:28,  4.29s/it]                                                 {'loss': 1.3619, 'learning_rate': 1.4084507042253522e-05, 'epoch': 1.27}
 32%|███▏      | 140/440 [09:56<21:28,  4.29s/it] 32%|███▏      | 141/440 [10:00<21:13,  4.26s/it]                                                 {'loss': 1.2274, 'learning_rate': 1.4037558685446011e-05, 'epoch': 1.28}
 32%|███▏      | 141/440 [10:00<21:13,  4.26s/it] 32%|███▏      | 142/440 [10:05<21:15,  4.28s/it]                                                 {'loss': 1.3481, 'learning_rate': 1.3990610328638499e-05, 'epoch': 1.29}
 32%|███▏      | 142/440 [10:05<21:15,  4.28s/it] 32%|███▎      | 143/440 [10:09<21:09,  4.28s/it]                                                 {'loss': 1.1371, 'learning_rate': 1.3943661971830987e-05, 'epoch': 1.3}
 32%|███▎      | 143/440 [10:09<21:09,  4.28s/it] 33%|███▎      | 144/440 [10:13<21:07,  4.28s/it]                                                 {'loss': 1.2225, 'learning_rate': 1.3896713615023477e-05, 'epoch': 1.3}
 33%|███▎      | 144/440 [10:13<21:07,  4.28s/it] 33%|███▎      | 145/440 [10:17<21:02,  4.28s/it]                                                 {'loss': 1.0808, 'learning_rate': 1.3849765258215963e-05, 'epoch': 1.31}
 33%|███▎      | 145/440 [10:17<21:02,  4.28s/it] 33%|███▎      | 146/440 [10:22<20:48,  4.25s/it]                                                 {'loss': 1.0862, 'learning_rate': 1.380281690140845e-05, 'epoch': 1.32}
 33%|███▎      | 146/440 [10:22<20:48,  4.25s/it] 33%|███▎      | 147/440 [10:26<20:42,  4.24s/it]                                                 {'loss': 1.0993, 'learning_rate': 1.375586854460094e-05, 'epoch': 1.33}
 33%|███▎      | 147/440 [10:26<20:42,  4.24s/it] 34%|███▎      | 148/440 [10:30<20:45,  4.27s/it]                                                 {'loss': 1.3692, 'learning_rate': 1.3708920187793428e-05, 'epoch': 1.34}
 34%|███▎      | 148/440 [10:30<20:45,  4.27s/it] 34%|███▍      | 149/440 [10:34<20:38,  4.26s/it]                                                 {'loss': 1.1645, 'learning_rate': 1.3661971830985916e-05, 'epoch': 1.35}
 34%|███▍      | 149/440 [10:34<20:38,  4.26s/it] 34%|███▍      | 150/440 [10:39<20:25,  4.23s/it]                                                 {'loss': 1.1302, 'learning_rate': 1.3615023474178406e-05, 'epoch': 1.36}
 34%|███▍      | 150/440 [10:39<20:25,  4.23s/it] 34%|███▍      | 151/440 [10:43<20:24,  4.24s/it]                                                 {'loss': 1.2869, 'learning_rate': 1.3568075117370894e-05, 'epoch': 1.37}
 34%|███▍      | 151/440 [10:43<20:24,  4.24s/it] 35%|███▍      | 152/440 [10:47<20:16,  4.22s/it]                                                 {'loss': 1.1591, 'learning_rate': 1.3521126760563382e-05, 'epoch': 1.38}
 35%|███▍      | 152/440 [10:47<20:16,  4.22s/it] 35%|███▍      | 153/440 [10:51<20:16,  4.24s/it]                                                 {'loss': 1.075, 'learning_rate': 1.347417840375587e-05, 'epoch': 1.39}
 35%|███▍      | 153/440 [10:51<20:16,  4.24s/it] 35%|███▌      | 154/440 [10:56<20:14,  4.25s/it]                                                 {'loss': 1.1216, 'learning_rate': 1.342723004694836e-05, 'epoch': 1.39}
 35%|███▌      | 154/440 [10:56<20:14,  4.25s/it] 35%|███▌      | 155/440 [11:00<20:32,  4.32s/it]                                                 {'loss': 1.1464, 'learning_rate': 1.3380281690140845e-05, 'epoch': 1.4}
 35%|███▌      | 155/440 [11:00<20:32,  4.32s/it] 35%|███▌      | 156/440 [11:04<20:17,  4.29s/it]                                                 {'loss': 1.2038, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.41}
 35%|███▌      | 156/440 [11:04<20:17,  4.29s/it] 36%|███▌      | 157/440 [11:09<20:18,  4.30s/it]                                                 {'loss': 1.2792, 'learning_rate': 1.3286384976525823e-05, 'epoch': 1.42}
 36%|███▌      | 157/440 [11:09<20:18,  4.30s/it] 36%|███▌      | 158/440 [11:13<20:08,  4.28s/it]                                                 {'loss': 1.087, 'learning_rate': 1.323943661971831e-05, 'epoch': 1.43}
 36%|███▌      | 158/440 [11:13<20:08,  4.28s/it] 36%|███▌      | 159/440 [11:17<19:56,  4.26s/it]                                                 {'loss': 1.2806, 'learning_rate': 1.3192488262910799e-05, 'epoch': 1.44}
 36%|███▌      | 159/440 [11:17<19:56,  4.26s/it] 36%|███▋      | 160/440 [11:21<19:47,  4.24s/it]                                                 {'loss': 1.2891, 'learning_rate': 1.3145539906103288e-05, 'epoch': 1.45}
 36%|███▋      | 160/440 [11:21<19:47,  4.24s/it] 37%|███▋      | 161/440 [11:25<19:43,  4.24s/it]                                                 {'loss': 1.1546, 'learning_rate': 1.3098591549295776e-05, 'epoch': 1.46}
 37%|███▋      | 161/440 [11:26<19:43,  4.24s/it] 37%|███▋      | 162/440 [11:30<19:33,  4.22s/it]                                                 {'loss': 0.9686, 'learning_rate': 1.3051643192488264e-05, 'epoch': 1.47}
 37%|███▋      | 162/440 [11:30<19:33,  4.22s/it] 37%|███▋      | 163/440 [11:34<19:38,  4.25s/it]                                                 {'loss': 1.2343, 'learning_rate': 1.300469483568075e-05, 'epoch': 1.48}
 37%|███▋      | 163/440 [11:34<19:38,  4.25s/it] 37%|███▋      | 164/440 [11:38<19:35,  4.26s/it]                                                 {'loss': 1.1504, 'learning_rate': 1.2957746478873242e-05, 'epoch': 1.49}
 37%|███▋      | 164/440 [11:38<19:35,  4.26s/it] 38%|███▊      | 165/440 [11:42<19:26,  4.24s/it]                                                 {'loss': 1.0323, 'learning_rate': 1.2910798122065728e-05, 'epoch': 1.49}
 38%|███▊      | 165/440 [11:42<19:26,  4.24s/it] 38%|███▊      | 166/440 [11:47<19:29,  4.27s/it]                                                 {'loss': 1.2408, 'learning_rate': 1.2863849765258216e-05, 'epoch': 1.5}
 38%|███▊      | 166/440 [11:47<19:29,  4.27s/it] 38%|███▊      | 167/440 [11:51<19:23,  4.26s/it]                                                 {'loss': 1.3314, 'learning_rate': 1.2816901408450705e-05, 'epoch': 1.51}
 38%|███▊      | 167/440 [11:51<19:23,  4.26s/it] 38%|███▊      | 168/440 [11:55<19:10,  4.23s/it]                                                 {'loss': 1.2473, 'learning_rate': 1.2769953051643193e-05, 'epoch': 1.52}
 38%|███▊      | 168/440 [11:55<19:10,  4.23s/it] 38%|███▊      | 169/440 [11:59<18:52,  4.18s/it]                                                 {'loss': 1.2004, 'learning_rate': 1.2723004694835681e-05, 'epoch': 1.53}
 38%|███▊      | 169/440 [11:59<18:52,  4.18s/it] 39%|███▊      | 170/440 [12:03<18:46,  4.17s/it]                                                 {'loss': 1.2782, 'learning_rate': 1.2676056338028171e-05, 'epoch': 1.54}
 39%|███▊      | 170/440 [12:03<18:46,  4.17s/it] 39%|███▉      | 171/440 [12:08<19:02,  4.25s/it]                                                 {'loss': 1.3052, 'learning_rate': 1.2629107981220659e-05, 'epoch': 1.55}
 39%|███▉      | 171/440 [12:08<19:02,  4.25s/it] 39%|███▉      | 172/440 [12:12<18:40,  4.18s/it]                                                 {'loss': 1.125, 'learning_rate': 1.2582159624413147e-05, 'epoch': 1.56}
 39%|███▉      | 172/440 [12:12<18:40,  4.18s/it] 39%|███▉      | 173/440 [12:16<18:31,  4.16s/it]                                                 {'loss': 0.9759, 'learning_rate': 1.2535211267605636e-05, 'epoch': 1.57}
 39%|███▉      | 173/440 [12:16<18:31,  4.16s/it] 40%|███▉      | 174/440 [12:20<18:19,  4.13s/it]                                                 {'loss': 0.9208, 'learning_rate': 1.2488262910798124e-05, 'epoch': 1.58}
 40%|███▉      | 174/440 [12:20<18:19,  4.13s/it] 40%|███▉      | 175/440 [12:24<18:13,  4.13s/it]                                                 {'loss': 1.2225, 'learning_rate': 1.244131455399061e-05, 'epoch': 1.59}
 40%|███▉      | 175/440 [12:24<18:13,  4.13s/it] 40%|████      | 176/440 [12:28<18:09,  4.13s/it]                                                 {'loss': 1.2541, 'learning_rate': 1.2394366197183098e-05, 'epoch': 1.59}
 40%|████      | 176/440 [12:28<18:09,  4.13s/it] 40%|████      | 177/440 [12:32<18:03,  4.12s/it]                                                 {'loss': 1.2345, 'learning_rate': 1.2347417840375588e-05, 'epoch': 1.6}
 40%|████      | 177/440 [12:32<18:03,  4.12s/it] 40%|████      | 178/440 [12:37<17:58,  4.12s/it]                                                 {'loss': 1.2959, 'learning_rate': 1.2300469483568076e-05, 'epoch': 1.61}
 40%|████      | 178/440 [12:37<17:58,  4.12s/it] 41%|████      | 179/440 [12:41<17:55,  4.12s/it]                                                 {'loss': 1.2084, 'learning_rate': 1.2253521126760564e-05, 'epoch': 1.62}
 41%|████      | 179/440 [12:41<17:55,  4.12s/it] 41%|████      | 180/440 [12:45<17:49,  4.11s/it]                                                 {'loss': 1.1944, 'learning_rate': 1.2206572769953053e-05, 'epoch': 1.63}
 41%|████      | 180/440 [12:45<17:49,  4.11s/it] 41%|████      | 181/440 [12:49<17:43,  4.11s/it]                                                 {'loss': 1.1943, 'learning_rate': 1.2159624413145541e-05, 'epoch': 1.64}
 41%|████      | 181/440 [12:49<17:43,  4.11s/it] 41%|████▏     | 182/440 [12:53<17:44,  4.12s/it]                                                 {'loss': 0.9997, 'learning_rate': 1.211267605633803e-05, 'epoch': 1.65}
 41%|████▏     | 182/440 [12:53<17:44,  4.12s/it] 42%|████▏     | 183/440 [12:57<17:40,  4.12s/it]                                                 {'loss': 1.1759, 'learning_rate': 1.2065727699530519e-05, 'epoch': 1.66}
 42%|████▏     | 183/440 [12:57<17:40,  4.12s/it] 42%|████▏     | 184/440 [13:01<17:45,  4.16s/it]                                                 {'loss': 1.1018, 'learning_rate': 1.2018779342723005e-05, 'epoch': 1.67}
 42%|████▏     | 184/440 [13:01<17:45,  4.16s/it] 42%|████▏     | 185/440 [13:05<17:31,  4.12s/it]                                                 {'loss': 1.0367, 'learning_rate': 1.1971830985915493e-05, 'epoch': 1.68}
 42%|████▏     | 185/440 [13:05<17:31,  4.12s/it] 42%|████▏     | 186/440 [13:10<17:47,  4.20s/it]                                                 {'loss': 0.9432, 'learning_rate': 1.1924882629107981e-05, 'epoch': 1.68}
 42%|████▏     | 186/440 [13:10<17:47,  4.20s/it] 42%|████▎     | 187/440 [13:14<17:33,  4.16s/it]                                                 {'loss': 1.0637, 'learning_rate': 1.187793427230047e-05, 'epoch': 1.69}
 42%|████▎     | 187/440 [13:14<17:33,  4.16s/it] 43%|████▎     | 188/440 [13:18<17:30,  4.17s/it]                                                 {'loss': 1.2758, 'learning_rate': 1.1830985915492958e-05, 'epoch': 1.7}
 43%|████▎     | 188/440 [13:18<17:30,  4.17s/it] 43%|████▎     | 189/440 [13:22<17:25,  4.17s/it]                                                 {'loss': 1.0645, 'learning_rate': 1.1784037558685446e-05, 'epoch': 1.71}
 43%|████▎     | 189/440 [13:22<17:25,  4.17s/it] 43%|████▎     | 190/440 [13:26<17:13,  4.13s/it]                                                 {'loss': 1.1649, 'learning_rate': 1.1737089201877936e-05, 'epoch': 1.72}
 43%|████▎     | 190/440 [13:26<17:13,  4.13s/it] 43%|████▎     | 191/440 [13:30<17:02,  4.11s/it]                                                 {'loss': 1.1585, 'learning_rate': 1.1690140845070424e-05, 'epoch': 1.73}
 43%|████▎     | 191/440 [13:30<17:02,  4.11s/it] 44%|████▎     | 192/440 [13:34<17:01,  4.12s/it]                                                 {'loss': 1.1933, 'learning_rate': 1.1643192488262912e-05, 'epoch': 1.74}
 44%|████▎     | 192/440 [13:34<17:01,  4.12s/it] 44%|████▍     | 193/440 [13:39<16:52,  4.10s/it]                                                 {'loss': 1.0783, 'learning_rate': 1.1596244131455401e-05, 'epoch': 1.75}
 44%|████▍     | 193/440 [13:39<16:52,  4.10s/it] 44%|████▍     | 194/440 [13:43<16:47,  4.10s/it]                                                 {'loss': 1.1605, 'learning_rate': 1.1549295774647888e-05, 'epoch': 1.76}
 44%|████▍     | 194/440 [13:43<16:47,  4.10s/it] 44%|████▍     | 195/440 [13:47<16:42,  4.09s/it]                                                 {'loss': 1.2815, 'learning_rate': 1.1502347417840376e-05, 'epoch': 1.77}
 44%|████▍     | 195/440 [13:47<16:42,  4.09s/it] 45%|████▍     | 196/440 [13:51<16:39,  4.10s/it]                                                 {'loss': 0.9937, 'learning_rate': 1.1455399061032865e-05, 'epoch': 1.78}
 45%|████▍     | 196/440 [13:51<16:39,  4.10s/it] 45%|████▍     | 197/440 [13:55<16:37,  4.10s/it]                                                 {'loss': 1.2245, 'learning_rate': 1.1408450704225353e-05, 'epoch': 1.78}
 45%|████▍     | 197/440 [13:55<16:37,  4.10s/it] 45%|████▌     | 198/440 [13:59<16:33,  4.10s/it]                                                 {'loss': 1.1768, 'learning_rate': 1.1361502347417841e-05, 'epoch': 1.79}
 45%|████▌     | 198/440 [13:59<16:33,  4.10s/it] 45%|████▌     | 199/440 [14:03<16:29,  4.11s/it]                                                 {'loss': 1.018, 'learning_rate': 1.1314553990610329e-05, 'epoch': 1.8}
 45%|████▌     | 199/440 [14:03<16:29,  4.11s/it] 45%|████▌     | 200/440 [14:07<16:21,  4.09s/it]                                                 {'loss': 1.1654, 'learning_rate': 1.1267605633802819e-05, 'epoch': 1.81}
 45%|████▌     | 200/440 [14:07<16:21,  4.09s/it] 46%|████▌     | 201/440 [14:11<16:15,  4.08s/it]                                                 {'loss': 1.2923, 'learning_rate': 1.1220657276995307e-05, 'epoch': 1.82}
 46%|████▌     | 201/440 [14:11<16:15,  4.08s/it] 46%|████▌     | 202/440 [14:15<16:06,  4.06s/it]                                                 {'loss': 1.276, 'learning_rate': 1.1173708920187793e-05, 'epoch': 1.83}
 46%|████▌     | 202/440 [14:15<16:06,  4.06s/it] 46%|████▌     | 203/440 [14:19<16:07,  4.08s/it]                                                 {'loss': 1.1549, 'learning_rate': 1.1126760563380284e-05, 'epoch': 1.84}
 46%|████▌     | 203/440 [14:19<16:07,  4.08s/it] 46%|████▋     | 204/440 [14:24<16:16,  4.14s/it]                                                 {'loss': 0.8835, 'learning_rate': 1.107981220657277e-05, 'epoch': 1.85}
 46%|████▋     | 204/440 [14:24<16:16,  4.14s/it] 47%|████▋     | 205/440 [14:28<16:06,  4.11s/it]                                                 {'loss': 1.0453, 'learning_rate': 1.1032863849765258e-05, 'epoch': 1.86}
 47%|████▋     | 205/440 [14:28<16:06,  4.11s/it] 47%|████▋     | 206/440 [14:32<16:13,  4.16s/it]                                                 {'loss': 1.2141, 'learning_rate': 1.0985915492957748e-05, 'epoch': 1.87}
 47%|████▋     | 206/440 [14:32<16:13,  4.16s/it] 47%|████▋     | 207/440 [14:36<16:03,  4.13s/it]                                                 {'loss': 1.1004, 'learning_rate': 1.0938967136150236e-05, 'epoch': 1.87}
 47%|████▋     | 207/440 [14:36<16:03,  4.13s/it] 47%|████▋     | 208/440 [14:40<15:53,  4.11s/it]                                                 {'loss': 0.8757, 'learning_rate': 1.0892018779342724e-05, 'epoch': 1.88}
 47%|████▋     | 208/440 [14:40<15:53,  4.11s/it] 48%|████▊     | 209/440 [14:44<15:50,  4.11s/it]                                                 {'loss': 1.0032, 'learning_rate': 1.0845070422535213e-05, 'epoch': 1.89}
 48%|████▊     | 209/440 [14:44<15:50,  4.11s/it] 48%|████▊     | 210/440 [14:48<15:42,  4.10s/it]                                                 {'loss': 1.0389, 'learning_rate': 1.0798122065727701e-05, 'epoch': 1.9}
 48%|████▊     | 210/440 [14:48<15:42,  4.10s/it] 48%|████▊     | 211/440 [14:52<15:35,  4.09s/it]                                                 {'loss': 0.7978, 'learning_rate': 1.0751173708920189e-05, 'epoch': 1.91}
 48%|████▊     | 211/440 [14:52<15:35,  4.09s/it] 48%|████▊     | 212/440 [14:56<15:34,  4.10s/it]                                                 {'loss': 1.0649, 'learning_rate': 1.0704225352112675e-05, 'epoch': 1.92}
 48%|████▊     | 212/440 [14:56<15:34,  4.10s/it] 48%|████▊     | 213/440 [15:01<15:30,  4.10s/it]                                                 {'loss': 1.0273, 'learning_rate': 1.0657276995305167e-05, 'epoch': 1.93}
 48%|████▊     | 213/440 [15:01<15:30,  4.10s/it] 49%|████▊     | 214/440 [15:05<15:28,  4.11s/it]                                                 {'loss': 1.0609, 'learning_rate': 1.0610328638497653e-05, 'epoch': 1.94}
 49%|████▊     | 214/440 [15:05<15:28,  4.11s/it] 49%|████▉     | 215/440 [15:09<15:20,  4.09s/it]                                                 {'loss': 1.4584, 'learning_rate': 1.056338028169014e-05, 'epoch': 1.95}
 49%|████▉     | 215/440 [15:09<15:20,  4.09s/it] 49%|████▉     | 216/440 [15:13<15:17,  4.10s/it]                                                 {'loss': 1.2408, 'learning_rate': 1.051643192488263e-05, 'epoch': 1.96}
 49%|████▉     | 216/440 [15:13<15:17,  4.10s/it] 49%|████▉     | 217/440 [15:17<15:25,  4.15s/it]                                                 {'loss': 1.1964, 'learning_rate': 1.0469483568075118e-05, 'epoch': 1.97}
 49%|████▉     | 217/440 [15:17<15:25,  4.15s/it] 50%|████▉     | 218/440 [15:21<15:29,  4.19s/it]                                                 {'loss': 1.2476, 'learning_rate': 1.0422535211267606e-05, 'epoch': 1.97}
 50%|████▉     | 218/440 [15:21<15:29,  4.19s/it] 50%|████▉     | 219/440 [15:25<15:14,  4.14s/it]                                                 {'loss': 1.0456, 'learning_rate': 1.0375586854460096e-05, 'epoch': 1.98}
 50%|████▉     | 219/440 [15:25<15:14,  4.14s/it] 50%|█████     | 220/440 [15:30<15:15,  4.16s/it]                                                 {'loss': 1.155, 'learning_rate': 1.0328638497652584e-05, 'epoch': 1.99}
 50%|█████     | 220/440 [15:30<15:15,  4.16s/it][INFO|trainer.py:2889] 2024-11-27 21:56:07,407 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tmp-checkpoint-220
[INFO|tokenization_utils_base.py:2432] 2024-11-27 21:56:07,972 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tmp-checkpoint-220/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 21:56:07,973 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tmp-checkpoint-220/special_tokens_map.json
 50%|█████     | 221/440 [15:35<16:35,  4.55s/it]                                                 {'loss': 1.1431, 'learning_rate': 1.0281690140845072e-05, 'epoch': 2.0}
 50%|█████     | 221/440 [15:35<16:35,  4.55s/it] 50%|█████     | 222/440 [15:39<15:58,  4.40s/it]                                                 {'loss': 1.1392, 'learning_rate': 1.0234741784037558e-05, 'epoch': 2.01}
 50%|█████     | 222/440 [15:39<15:58,  4.40s/it] 51%|█████     | 223/440 [15:43<15:34,  4.30s/it]                                                 {'loss': 1.1951, 'learning_rate': 1.0187793427230047e-05, 'epoch': 2.02}
 51%|█████     | 223/440 [15:43<15:34,  4.30s/it] 51%|█████     | 224/440 [15:47<15:13,  4.23s/it]                                                 {'loss': 0.9777, 'learning_rate': 1.0140845070422535e-05, 'epoch': 2.03}
 51%|█████     | 224/440 [15:47<15:13,  4.23s/it] 51%|█████     | 225/440 [15:51<15:01,  4.19s/it]                                                 {'loss': 1.1645, 'learning_rate': 1.0093896713615023e-05, 'epoch': 2.04}
 51%|█████     | 225/440 [15:51<15:01,  4.19s/it] 51%|█████▏    | 226/440 [15:55<14:48,  4.15s/it]                                                 {'loss': 0.9303, 'learning_rate': 1.0046948356807513e-05, 'epoch': 2.05}
 51%|█████▏    | 226/440 [15:55<14:48,  4.15s/it] 52%|█████▏    | 227/440 [16:00<14:43,  4.15s/it]                                                 {'loss': 1.1128, 'learning_rate': 1e-05, 'epoch': 2.06}
 52%|█████▏    | 227/440 [16:00<14:43,  4.15s/it] 52%|█████▏    | 228/440 [16:04<14:41,  4.16s/it]                                                 {'loss': 1.0238, 'learning_rate': 9.953051643192489e-06, 'epoch': 2.07}
 52%|█████▏    | 228/440 [16:04<14:41,  4.16s/it] 52%|█████▏    | 229/440 [16:08<14:35,  4.15s/it]                                                 {'loss': 0.8937, 'learning_rate': 9.906103286384977e-06, 'epoch': 2.07}
 52%|█████▏    | 229/440 [16:08<14:35,  4.15s/it] 52%|█████▏    | 230/440 [16:12<14:31,  4.15s/it]                                                 {'loss': 1.2561, 'learning_rate': 9.859154929577466e-06, 'epoch': 2.08}
 52%|█████▏    | 230/440 [16:12<14:31,  4.15s/it] 52%|█████▎    | 231/440 [16:16<14:22,  4.13s/it]                                                 {'loss': 1.0884, 'learning_rate': 9.812206572769954e-06, 'epoch': 2.09}
 52%|█████▎    | 231/440 [16:16<14:22,  4.13s/it] 53%|█████▎    | 232/440 [16:20<14:13,  4.10s/it]                                                 {'loss': 0.9742, 'learning_rate': 9.765258215962442e-06, 'epoch': 2.1}
 53%|█████▎    | 232/440 [16:20<14:13,  4.10s/it] 53%|█████▎    | 233/440 [16:24<14:11,  4.11s/it]                                                 {'loss': 1.1272, 'learning_rate': 9.71830985915493e-06, 'epoch': 2.11}
 53%|█████▎    | 233/440 [16:24<14:11,  4.11s/it] 53%|█████▎    | 234/440 [16:28<14:10,  4.13s/it]                                                 {'loss': 1.082, 'learning_rate': 9.671361502347418e-06, 'epoch': 2.12}
 53%|█████▎    | 234/440 [16:28<14:10,  4.13s/it] 53%|█████▎    | 235/440 [16:33<14:06,  4.13s/it]                                                 {'loss': 1.1007, 'learning_rate': 9.624413145539908e-06, 'epoch': 2.13}
 53%|█████▎    | 235/440 [16:33<14:06,  4.13s/it] 54%|█████▎    | 236/440 [16:37<13:55,  4.10s/it]                                                 {'loss': 1.2468, 'learning_rate': 9.577464788732394e-06, 'epoch': 2.14}
 54%|█████▎    | 236/440 [16:37<13:55,  4.10s/it] 54%|█████▍    | 237/440 [16:41<13:46,  4.07s/it]                                                 {'loss': 0.9507, 'learning_rate': 9.530516431924883e-06, 'epoch': 2.15}
 54%|█████▍    | 237/440 [16:41<13:46,  4.07s/it] 54%|█████▍    | 238/440 [16:45<13:48,  4.10s/it]                                                 {'loss': 1.0197, 'learning_rate': 9.483568075117371e-06, 'epoch': 2.16}
 54%|█████▍    | 238/440 [16:45<13:48,  4.10s/it] 54%|█████▍    | 239/440 [16:49<13:47,  4.12s/it]                                                 {'loss': 1.1421, 'learning_rate': 9.43661971830986e-06, 'epoch': 2.16}
 54%|█████▍    | 239/440 [16:49<13:47,  4.12s/it] 55%|█████▍    | 240/440 [16:53<13:37,  4.09s/it]                                                 {'loss': 1.031, 'learning_rate': 9.389671361502349e-06, 'epoch': 2.17}
 55%|█████▍    | 240/440 [16:53<13:37,  4.09s/it] 55%|█████▍    | 241/440 [16:57<13:31,  4.08s/it]                                                 {'loss': 1.1459, 'learning_rate': 9.342723004694837e-06, 'epoch': 2.18}
 55%|█████▍    | 241/440 [16:57<13:31,  4.08s/it] 55%|█████▌    | 242/440 [17:01<13:32,  4.10s/it]                                                 {'loss': 1.1485, 'learning_rate': 9.295774647887325e-06, 'epoch': 2.19}
 55%|█████▌    | 242/440 [17:01<13:32,  4.10s/it] 55%|█████▌    | 243/440 [17:05<13:26,  4.09s/it]                                                 {'loss': 1.0221, 'learning_rate': 9.248826291079813e-06, 'epoch': 2.2}
 55%|█████▌    | 243/440 [17:05<13:26,  4.09s/it] 55%|█████▌    | 244/440 [17:09<13:28,  4.13s/it]                                                 {'loss': 1.1461, 'learning_rate': 9.2018779342723e-06, 'epoch': 2.21}
 55%|█████▌    | 244/440 [17:09<13:28,  4.13s/it] 56%|█████▌    | 245/440 [17:14<13:25,  4.13s/it]                                                 {'loss': 0.9425, 'learning_rate': 9.15492957746479e-06, 'epoch': 2.22}
 56%|█████▌    | 245/440 [17:14<13:25,  4.13s/it] 56%|█████▌    | 246/440 [17:18<13:27,  4.16s/it]                                                 {'loss': 1.0607, 'learning_rate': 9.107981220657278e-06, 'epoch': 2.23}
 56%|█████▌    | 246/440 [17:18<13:27,  4.16s/it] 56%|█████▌    | 247/440 [17:22<13:20,  4.15s/it]                                                 {'loss': 1.0935, 'learning_rate': 9.061032863849766e-06, 'epoch': 2.24}
 56%|█████▌    | 247/440 [17:22<13:20,  4.15s/it] 56%|█████▋    | 248/440 [17:26<13:13,  4.13s/it]                                                 {'loss': 0.9612, 'learning_rate': 9.014084507042254e-06, 'epoch': 2.25}
 56%|█████▋    | 248/440 [17:26<13:13,  4.13s/it] 57%|█████▋    | 249/440 [17:30<13:16,  4.17s/it]                                                 {'loss': 1.0513, 'learning_rate': 8.967136150234742e-06, 'epoch': 2.26}
 57%|█████▋    | 249/440 [17:30<13:16,  4.17s/it] 57%|█████▋    | 250/440 [17:34<13:07,  4.14s/it]                                                 {'loss': 1.0791, 'learning_rate': 8.920187793427231e-06, 'epoch': 2.26}
 57%|█████▋    | 250/440 [17:34<13:07,  4.14s/it] 57%|█████▋    | 251/440 [17:39<13:01,  4.13s/it]                                                 {'loss': 1.1115, 'learning_rate': 8.87323943661972e-06, 'epoch': 2.27}
 57%|█████▋    | 251/440 [17:39<13:01,  4.13s/it] 57%|█████▋    | 252/440 [17:43<12:55,  4.12s/it]                                                 {'loss': 1.1984, 'learning_rate': 8.826291079812207e-06, 'epoch': 2.28}
 57%|█████▋    | 252/440 [17:43<12:55,  4.12s/it] 57%|█████▊    | 253/440 [17:47<12:57,  4.16s/it]                                                 {'loss': 1.1108, 'learning_rate': 8.779342723004695e-06, 'epoch': 2.29}
 57%|█████▊    | 253/440 [17:47<12:57,  4.16s/it] 58%|█████▊    | 254/440 [17:51<12:52,  4.15s/it]                                                 {'loss': 1.0918, 'learning_rate': 8.732394366197183e-06, 'epoch': 2.3}
 58%|█████▊    | 254/440 [17:51<12:52,  4.15s/it] 58%|█████▊    | 255/440 [17:55<12:46,  4.14s/it]                                                 {'loss': 1.0813, 'learning_rate': 8.685446009389673e-06, 'epoch': 2.31}
 58%|█████▊    | 255/440 [17:55<12:46,  4.14s/it] 58%|█████▊    | 256/440 [17:59<12:53,  4.20s/it]                                                 {'loss': 1.0902, 'learning_rate': 8.63849765258216e-06, 'epoch': 2.32}
 58%|█████▊    | 256/440 [17:59<12:53,  4.20s/it] 58%|█████▊    | 257/440 [18:04<12:49,  4.20s/it]                                                 {'loss': 1.2395, 'learning_rate': 8.591549295774648e-06, 'epoch': 2.33}
 58%|█████▊    | 257/440 [18:04<12:49,  4.20s/it] 59%|█████▊    | 258/440 [18:08<12:40,  4.18s/it]                                                 {'loss': 0.9851, 'learning_rate': 8.544600938967136e-06, 'epoch': 2.34}
 59%|█████▊    | 258/440 [18:08<12:40,  4.18s/it] 59%|█████▉    | 259/440 [18:12<12:30,  4.15s/it]                                                 {'loss': 1.0828, 'learning_rate': 8.497652582159626e-06, 'epoch': 2.35}
 59%|█████▉    | 259/440 [18:12<12:30,  4.15s/it] 59%|█████▉    | 260/440 [18:16<12:28,  4.16s/it]                                                 {'loss': 1.1703, 'learning_rate': 8.450704225352114e-06, 'epoch': 2.35}
 59%|█████▉    | 260/440 [18:16<12:28,  4.16s/it] 59%|█████▉    | 261/440 [18:20<12:21,  4.14s/it]                                                 {'loss': 0.8608, 'learning_rate': 8.403755868544602e-06, 'epoch': 2.36}
 59%|█████▉    | 261/440 [18:20<12:21,  4.14s/it] 60%|█████▉    | 262/440 [18:24<12:16,  4.14s/it]                                                 {'loss': 1.0388, 'learning_rate': 8.35680751173709e-06, 'epoch': 2.37}
 60%|█████▉    | 262/440 [18:24<12:16,  4.14s/it] 60%|█████▉    | 263/440 [18:28<12:15,  4.16s/it]                                                 {'loss': 1.1877, 'learning_rate': 8.309859154929578e-06, 'epoch': 2.38}
 60%|█████▉    | 263/440 [18:28<12:15,  4.16s/it] 60%|██████    | 264/440 [18:33<12:19,  4.20s/it]                                                 {'loss': 1.2161, 'learning_rate': 8.262910798122067e-06, 'epoch': 2.39}
 60%|██████    | 264/440 [18:33<12:19,  4.20s/it] 60%|██████    | 265/440 [18:37<12:08,  4.16s/it]                                                 {'loss': 1.0099, 'learning_rate': 8.215962441314555e-06, 'epoch': 2.4}
 60%|██████    | 265/440 [18:37<12:08,  4.16s/it] 60%|██████    | 266/440 [18:41<12:00,  4.14s/it]                                                 {'loss': 1.0347, 'learning_rate': 8.169014084507043e-06, 'epoch': 2.41}
 60%|██████    | 266/440 [18:41<12:00,  4.14s/it] 61%|██████    | 267/440 [18:45<11:55,  4.13s/it]                                                 {'loss': 0.9586, 'learning_rate': 8.122065727699531e-06, 'epoch': 2.42}
 61%|██████    | 267/440 [18:45<11:55,  4.13s/it] 61%|██████    | 268/440 [18:49<11:47,  4.12s/it]                                                 {'loss': 1.005, 'learning_rate': 8.075117370892019e-06, 'epoch': 2.43}
 61%|██████    | 268/440 [18:49<11:47,  4.12s/it] 61%|██████    | 269/440 [18:53<11:43,  4.11s/it]                                                 {'loss': 1.0511, 'learning_rate': 8.028169014084509e-06, 'epoch': 2.44}
 61%|██████    | 269/440 [18:53<11:43,  4.11s/it] 61%|██████▏   | 270/440 [18:57<11:38,  4.11s/it]                                                 {'loss': 1.1932, 'learning_rate': 7.981220657276996e-06, 'epoch': 2.45}
 61%|██████▏   | 270/440 [18:57<11:38,  4.11s/it] 62%|██████▏   | 271/440 [19:01<11:35,  4.12s/it]                                                 {'loss': 0.9184, 'learning_rate': 7.934272300469484e-06, 'epoch': 2.45}
 62%|██████▏   | 271/440 [19:01<11:35,  4.12s/it] 62%|██████▏   | 272/440 [19:06<11:36,  4.15s/it]                                                 {'loss': 1.0861, 'learning_rate': 7.887323943661972e-06, 'epoch': 2.46}
 62%|██████▏   | 272/440 [19:06<11:36,  4.15s/it] 62%|██████▏   | 273/440 [19:10<11:32,  4.15s/it]                                                 {'loss': 0.9438, 'learning_rate': 7.84037558685446e-06, 'epoch': 2.47}
 62%|██████▏   | 273/440 [19:10<11:32,  4.15s/it] 62%|██████▏   | 274/440 [19:14<11:23,  4.12s/it]                                                 {'loss': 1.1091, 'learning_rate': 7.79342723004695e-06, 'epoch': 2.48}
 62%|██████▏   | 274/440 [19:14<11:23,  4.12s/it] 62%|██████▎   | 275/440 [19:18<11:15,  4.09s/it]                                                 {'loss': 1.1683, 'learning_rate': 7.746478873239436e-06, 'epoch': 2.49}
 62%|██████▎   | 275/440 [19:18<11:15,  4.09s/it] 63%|██████▎   | 276/440 [19:22<11:09,  4.08s/it]                                                 {'loss': 0.9298, 'learning_rate': 7.699530516431926e-06, 'epoch': 2.5}
 63%|██████▎   | 276/440 [19:22<11:09,  4.08s/it] 63%|██████▎   | 277/440 [19:26<11:04,  4.07s/it]                                                 {'loss': 1.0273, 'learning_rate': 7.652582159624414e-06, 'epoch': 2.51}
 63%|██████▎   | 277/440 [19:26<11:04,  4.07s/it] 63%|██████▎   | 278/440 [19:30<11:01,  4.09s/it]                                                 {'loss': 1.0825, 'learning_rate': 7.6056338028169015e-06, 'epoch': 2.52}
 63%|██████▎   | 278/440 [19:30<11:01,  4.09s/it] 63%|██████▎   | 279/440 [19:34<10:55,  4.07s/it]                                                 {'loss': 1.2117, 'learning_rate': 7.55868544600939e-06, 'epoch': 2.53}
 63%|██████▎   | 279/440 [19:34<10:55,  4.07s/it] 64%|██████▎   | 280/440 [19:38<10:51,  4.07s/it]                                                 {'loss': 0.9806, 'learning_rate': 7.511737089201878e-06, 'epoch': 2.54}
 64%|██████▎   | 280/440 [19:38<10:51,  4.07s/it] 64%|██████▍   | 281/440 [19:43<10:59,  4.15s/it]                                                 {'loss': 1.0376, 'learning_rate': 7.464788732394367e-06, 'epoch': 2.55}
 64%|██████▍   | 281/440 [19:43<10:59,  4.15s/it] 64%|██████▍   | 282/440 [19:47<10:49,  4.11s/it]                                                 {'loss': 0.846, 'learning_rate': 7.417840375586856e-06, 'epoch': 2.55}
 64%|██████▍   | 282/440 [19:47<10:49,  4.11s/it] 64%|██████▍   | 283/440 [19:51<10:45,  4.11s/it]                                                 {'loss': 1.0628, 'learning_rate': 7.370892018779343e-06, 'epoch': 2.56}
 64%|██████▍   | 283/440 [19:51<10:45,  4.11s/it] 65%|██████▍   | 284/440 [19:55<10:40,  4.10s/it]                                                 {'loss': 0.9942, 'learning_rate': 7.3239436619718316e-06, 'epoch': 2.57}
 65%|██████▍   | 284/440 [19:55<10:40,  4.10s/it] 65%|██████▍   | 285/440 [19:59<10:37,  4.11s/it]                                                 {'loss': 1.2681, 'learning_rate': 7.2769953051643195e-06, 'epoch': 2.58}
 65%|██████▍   | 285/440 [19:59<10:37,  4.11s/it] 65%|██████▌   | 286/440 [20:03<10:36,  4.13s/it]                                                 {'loss': 1.1394, 'learning_rate': 7.230046948356808e-06, 'epoch': 2.59}
 65%|██████▌   | 286/440 [20:03<10:36,  4.13s/it] 65%|██████▌   | 287/440 [20:07<10:28,  4.11s/it]                                                 {'loss': 1.0259, 'learning_rate': 7.183098591549297e-06, 'epoch': 2.6}
 65%|██████▌   | 287/440 [20:07<10:28,  4.11s/it] 65%|██████▌   | 288/440 [20:11<10:32,  4.16s/it]                                                 {'loss': 1.1998, 'learning_rate': 7.136150234741784e-06, 'epoch': 2.61}
 65%|██████▌   | 288/440 [20:11<10:32,  4.16s/it] 66%|██████▌   | 289/440 [20:16<10:26,  4.15s/it]                                                 {'loss': 0.9995, 'learning_rate': 7.089201877934273e-06, 'epoch': 2.62}
 66%|██████▌   | 289/440 [20:16<10:26,  4.15s/it] 66%|██████▌   | 290/440 [20:20<10:17,  4.12s/it]                                                 {'loss': 1.1058, 'learning_rate': 7.042253521126761e-06, 'epoch': 2.63}
 66%|██████▌   | 290/440 [20:20<10:17,  4.12s/it] 66%|██████▌   | 291/440 [20:24<10:20,  4.16s/it]                                                 {'loss': 1.0279, 'learning_rate': 6.9953051643192495e-06, 'epoch': 2.64}
 66%|██████▌   | 291/440 [20:24<10:20,  4.16s/it] 66%|██████▋   | 292/440 [20:28<10:12,  4.14s/it]                                                 {'loss': 1.0403, 'learning_rate': 6.948356807511738e-06, 'epoch': 2.64}
 66%|██████▋   | 292/440 [20:28<10:12,  4.14s/it] 67%|██████▋   | 293/440 [20:32<10:07,  4.13s/it]                                                 {'loss': 1.2395, 'learning_rate': 6.901408450704225e-06, 'epoch': 2.65}
 67%|██████▋   | 293/440 [20:32<10:07,  4.13s/it] 67%|██████▋   | 294/440 [20:36<10:03,  4.13s/it]                                                 {'loss': 1.0598, 'learning_rate': 6.854460093896714e-06, 'epoch': 2.66}
 67%|██████▋   | 294/440 [20:36<10:03,  4.13s/it] 67%|██████▋   | 295/440 [20:40<09:58,  4.13s/it]                                                 {'loss': 1.182, 'learning_rate': 6.807511737089203e-06, 'epoch': 2.67}
 67%|██████▋   | 295/440 [20:40<09:58,  4.13s/it] 67%|██████▋   | 296/440 [20:44<09:52,  4.12s/it]                                                 {'loss': 1.2624, 'learning_rate': 6.760563380281691e-06, 'epoch': 2.68}
 67%|██████▋   | 296/440 [20:44<09:52,  4.12s/it] 68%|██████▊   | 297/440 [20:49<09:51,  4.13s/it]                                                 {'loss': 1.144, 'learning_rate': 6.71361502347418e-06, 'epoch': 2.69}
 68%|██████▊   | 297/440 [20:49<09:51,  4.13s/it] 68%|██████▊   | 298/440 [20:53<09:47,  4.14s/it]                                                 {'loss': 1.1851, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.7}
 68%|██████▊   | 298/440 [20:53<09:47,  4.14s/it] 68%|██████▊   | 299/440 [20:57<09:40,  4.11s/it]                                                 {'loss': 1.1186, 'learning_rate': 6.619718309859155e-06, 'epoch': 2.71}
 68%|██████▊   | 299/440 [20:57<09:40,  4.11s/it] 68%|██████▊   | 300/440 [21:01<09:33,  4.09s/it]                                                 {'loss': 1.1059, 'learning_rate': 6.572769953051644e-06, 'epoch': 2.72}
 68%|██████▊   | 300/440 [21:01<09:33,  4.09s/it] 68%|██████▊   | 301/440 [21:05<09:27,  4.08s/it]                                                 {'loss': 0.9563, 'learning_rate': 6.525821596244132e-06, 'epoch': 2.73}
 68%|██████▊   | 301/440 [21:05<09:27,  4.08s/it] 69%|██████▊   | 302/440 [21:09<09:21,  4.07s/it]                                                 {'loss': 1.1621, 'learning_rate': 6.478873239436621e-06, 'epoch': 2.74}
 69%|██████▊   | 302/440 [21:09<09:21,  4.07s/it] 69%|██████▉   | 303/440 [21:13<09:28,  4.15s/it]                                                 {'loss': 1.1823, 'learning_rate': 6.431924882629108e-06, 'epoch': 2.74}
 69%|██████▉   | 303/440 [21:13<09:28,  4.15s/it] 69%|██████▉   | 304/440 [21:17<09:20,  4.12s/it]                                                 {'loss': 1.1972, 'learning_rate': 6.384976525821597e-06, 'epoch': 2.75}
 69%|██████▉   | 304/440 [21:17<09:20,  4.12s/it] 69%|██████▉   | 305/440 [21:21<09:14,  4.11s/it]                                                 {'loss': 1.0593, 'learning_rate': 6.3380281690140855e-06, 'epoch': 2.76}
 69%|██████▉   | 305/440 [21:21<09:14,  4.11s/it] 70%|██████▉   | 306/440 [21:26<09:10,  4.11s/it]                                                 {'loss': 1.2472, 'learning_rate': 6.291079812206573e-06, 'epoch': 2.77}
 70%|██████▉   | 306/440 [21:26<09:10,  4.11s/it] 70%|██████▉   | 307/440 [21:30<09:10,  4.14s/it]                                                 {'loss': 1.0032, 'learning_rate': 6.244131455399062e-06, 'epoch': 2.78}
 70%|██████▉   | 307/440 [21:30<09:10,  4.14s/it] 70%|███████   | 308/440 [21:34<09:05,  4.13s/it]                                                 {'loss': 1.0342, 'learning_rate': 6.197183098591549e-06, 'epoch': 2.79}
 70%|███████   | 308/440 [21:34<09:05,  4.13s/it] 70%|███████   | 309/440 [21:38<09:02,  4.14s/it]                                                 {'loss': 1.106, 'learning_rate': 6.150234741784038e-06, 'epoch': 2.8}
 70%|███████   | 309/440 [21:38<09:02,  4.14s/it] 70%|███████   | 310/440 [21:42<09:02,  4.17s/it]                                                 {'loss': 0.9956, 'learning_rate': 6.103286384976527e-06, 'epoch': 2.81}
 70%|███████   | 310/440 [21:42<09:02,  4.17s/it] 71%|███████   | 311/440 [21:46<08:56,  4.16s/it]                                                 {'loss': 0.9393, 'learning_rate': 6.056338028169015e-06, 'epoch': 2.82}
 71%|███████   | 311/440 [21:46<08:56,  4.16s/it] 71%|███████   | 312/440 [21:50<08:47,  4.12s/it]                                                 {'loss': 1.0406, 'learning_rate': 6.0093896713615026e-06, 'epoch': 2.83}
 71%|███████   | 312/440 [21:50<08:47,  4.12s/it] 71%|███████   | 313/440 [21:55<08:45,  4.14s/it]                                                 {'loss': 1.2774, 'learning_rate': 5.9624413145539905e-06, 'epoch': 2.83}
 71%|███████   | 313/440 [21:55<08:45,  4.14s/it] 71%|███████▏  | 314/440 [21:59<08:39,  4.12s/it]                                                 {'loss': 1.0987, 'learning_rate': 5.915492957746479e-06, 'epoch': 2.84}
 71%|███████▏  | 314/440 [21:59<08:39,  4.12s/it] 72%|███████▏  | 315/440 [22:03<08:38,  4.15s/it]                                                 {'loss': 1.1595, 'learning_rate': 5.868544600938968e-06, 'epoch': 2.85}
 72%|███████▏  | 315/440 [22:03<08:38,  4.15s/it] 72%|███████▏  | 316/440 [22:07<08:34,  4.15s/it]                                                 {'loss': 0.9404, 'learning_rate': 5.821596244131456e-06, 'epoch': 2.86}
 72%|███████▏  | 316/440 [22:07<08:34,  4.15s/it] 72%|███████▏  | 317/440 [22:11<08:38,  4.22s/it]                                                 {'loss': 1.0426, 'learning_rate': 5.774647887323944e-06, 'epoch': 2.87}
 72%|███████▏  | 317/440 [22:11<08:38,  4.22s/it] 72%|███████▏  | 318/440 [22:16<08:31,  4.20s/it]                                                 {'loss': 1.239, 'learning_rate': 5.727699530516433e-06, 'epoch': 2.88}
 72%|███████▏  | 318/440 [22:16<08:31,  4.20s/it] 72%|███████▎  | 319/440 [22:20<08:26,  4.18s/it]                                                 {'loss': 1.1435, 'learning_rate': 5.6807511737089205e-06, 'epoch': 2.89}
 72%|███████▎  | 319/440 [22:20<08:26,  4.18s/it] 73%|███████▎  | 320/440 [22:24<08:20,  4.17s/it]                                                 {'loss': 1.0644, 'learning_rate': 5.633802816901409e-06, 'epoch': 2.9}
 73%|███████▎  | 320/440 [22:24<08:20,  4.17s/it] 73%|███████▎  | 321/440 [22:28<08:14,  4.16s/it]                                                 {'loss': 1.0487, 'learning_rate': 5.586854460093896e-06, 'epoch': 2.91}
 73%|███████▎  | 321/440 [22:28<08:14,  4.16s/it] 73%|███████▎  | 322/440 [22:32<08:05,  4.11s/it]                                                 {'loss': 1.1993, 'learning_rate': 5.539906103286385e-06, 'epoch': 2.92}
 73%|███████▎  | 322/440 [22:32<08:05,  4.11s/it] 73%|███████▎  | 323/440 [22:36<07:58,  4.09s/it]                                                 {'loss': 1.232, 'learning_rate': 5.492957746478874e-06, 'epoch': 2.93}
 73%|███████▎  | 323/440 [22:36<07:58,  4.09s/it] 74%|███████▎  | 324/440 [22:40<08:00,  4.14s/it]                                                 {'loss': 1.0362, 'learning_rate': 5.446009389671362e-06, 'epoch': 2.93}
 74%|███████▎  | 324/440 [22:40<08:00,  4.14s/it] 74%|███████▍  | 325/440 [22:44<07:53,  4.12s/it]                                                 {'loss': 0.9795, 'learning_rate': 5.3990610328638506e-06, 'epoch': 2.94}
 74%|███████▍  | 325/440 [22:44<07:53,  4.12s/it] 74%|███████▍  | 326/440 [22:48<07:48,  4.11s/it]                                                 {'loss': 1.0918, 'learning_rate': 5.352112676056338e-06, 'epoch': 2.95}
 74%|███████▍  | 326/440 [22:48<07:48,  4.11s/it] 74%|███████▍  | 327/440 [22:53<07:43,  4.10s/it]                                                 {'loss': 0.9924, 'learning_rate': 5.305164319248826e-06, 'epoch': 2.96}
 74%|███████▍  | 327/440 [22:53<07:43,  4.10s/it] 75%|███████▍  | 328/440 [22:57<07:39,  4.10s/it]                                                 {'loss': 1.3977, 'learning_rate': 5.258215962441315e-06, 'epoch': 2.97}
 75%|███████▍  | 328/440 [22:57<07:39,  4.10s/it] 75%|███████▍  | 329/440 [23:01<07:35,  4.10s/it]                                                 {'loss': 1.0764, 'learning_rate': 5.211267605633803e-06, 'epoch': 2.98}
 75%|███████▍  | 329/440 [23:01<07:35,  4.10s/it] 75%|███████▌  | 330/440 [23:05<07:30,  4.10s/it]                                                 {'loss': 1.2627, 'learning_rate': 5.164319248826292e-06, 'epoch': 2.99}
 75%|███████▌  | 330/440 [23:05<07:30,  4.10s/it] 75%|███████▌  | 331/440 [23:09<07:23,  4.07s/it]                                                 {'loss': 1.1354, 'learning_rate': 5.117370892018779e-06, 'epoch': 3.0}
 75%|███████▌  | 331/440 [23:09<07:23,  4.07s/it][INFO|trainer.py:2889] 2024-11-27 22:03:44,264 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tmp-checkpoint-331
[INFO|tokenization_utils_base.py:2432] 2024-11-27 22:03:46,305 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tmp-checkpoint-331/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 22:03:46,305 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tmp-checkpoint-331/special_tokens_map.json
 75%|███████▌  | 332/440 [23:17<09:46,  5.43s/it]                                                 {'loss': 1.0697, 'learning_rate': 5.070422535211268e-06, 'epoch': 3.01}
 75%|███████▌  | 332/440 [23:17<09:46,  5.43s/it] 76%|███████▌  | 333/440 [23:22<08:57,  5.02s/it]                                                 {'loss': 1.3787, 'learning_rate': 5.0234741784037565e-06, 'epoch': 3.02}
 76%|███████▌  | 333/440 [23:22<08:57,  5.02s/it] 76%|███████▌  | 334/440 [23:26<08:20,  4.72s/it]                                                 {'loss': 1.0841, 'learning_rate': 4.976525821596244e-06, 'epoch': 3.03}
 76%|███████▌  | 334/440 [23:26<08:20,  4.72s/it] 76%|███████▌  | 335/440 [23:30<07:55,  4.52s/it]                                                 {'loss': 0.9825, 'learning_rate': 4.929577464788733e-06, 'epoch': 3.03}
 76%|███████▌  | 335/440 [23:30<07:55,  4.52s/it] 76%|███████▋  | 336/440 [23:34<07:36,  4.39s/it]                                                 {'loss': 0.6981, 'learning_rate': 4.882629107981221e-06, 'epoch': 3.04}
 76%|███████▋  | 336/440 [23:34<07:36,  4.39s/it] 77%|███████▋  | 337/440 [23:38<07:30,  4.37s/it]                                                 {'loss': 1.1153, 'learning_rate': 4.835680751173709e-06, 'epoch': 3.05}
 77%|███████▋  | 337/440 [23:38<07:30,  4.37s/it] 77%|███████▋  | 338/440 [23:42<07:18,  4.30s/it]                                                 {'loss': 1.0324, 'learning_rate': 4.788732394366197e-06, 'epoch': 3.06}
 77%|███████▋  | 338/440 [23:42<07:18,  4.30s/it] 77%|███████▋  | 339/440 [23:46<07:08,  4.24s/it]                                                 {'loss': 0.9843, 'learning_rate': 4.741784037558686e-06, 'epoch': 3.07}
 77%|███████▋  | 339/440 [23:46<07:08,  4.24s/it] 77%|███████▋  | 340/440 [23:50<06:59,  4.19s/it]                                                 {'loss': 0.9983, 'learning_rate': 4.694835680751174e-06, 'epoch': 3.08}
 77%|███████▋  | 340/440 [23:50<06:59,  4.19s/it] 78%|███████▊  | 341/440 [23:54<06:53,  4.18s/it]                                                 {'loss': 0.9531, 'learning_rate': 4.647887323943662e-06, 'epoch': 3.09}
 78%|███████▊  | 341/440 [23:54<06:53,  4.18s/it] 78%|███████▊  | 342/440 [23:59<06:48,  4.16s/it]                                                 {'loss': 0.9947, 'learning_rate': 4.60093896713615e-06, 'epoch': 3.1}
 78%|███████▊  | 342/440 [23:59<06:48,  4.16s/it] 78%|███████▊  | 343/440 [24:03<06:43,  4.16s/it]                                                 {'loss': 1.1963, 'learning_rate': 4.553990610328639e-06, 'epoch': 3.11}
 78%|███████▊  | 343/440 [24:03<06:43,  4.16s/it] 78%|███████▊  | 344/440 [24:07<06:41,  4.18s/it]                                                 {'loss': 1.0652, 'learning_rate': 4.507042253521127e-06, 'epoch': 3.12}
 78%|███████▊  | 344/440 [24:07<06:41,  4.18s/it] 78%|███████▊  | 345/440 [24:11<06:33,  4.14s/it]                                                 {'loss': 1.159, 'learning_rate': 4.460093896713616e-06, 'epoch': 3.12}
 78%|███████▊  | 345/440 [24:11<06:33,  4.14s/it] 79%|███████▊  | 346/440 [24:15<06:28,  4.13s/it]                                                 {'loss': 0.9306, 'learning_rate': 4.413145539906104e-06, 'epoch': 3.13}
 79%|███████▊  | 346/440 [24:15<06:28,  4.13s/it] 79%|███████▉  | 347/440 [24:19<06:23,  4.12s/it]                                                 {'loss': 1.0461, 'learning_rate': 4.3661971830985915e-06, 'epoch': 3.14}
 79%|███████▉  | 347/440 [24:19<06:23,  4.12s/it] 79%|███████▉  | 348/440 [24:23<06:17,  4.10s/it]                                                 {'loss': 1.4573, 'learning_rate': 4.31924882629108e-06, 'epoch': 3.15}
 79%|███████▉  | 348/440 [24:23<06:17,  4.10s/it] 79%|███████▉  | 349/440 [24:27<06:12,  4.10s/it]                                                 {'loss': 1.0754, 'learning_rate': 4.272300469483568e-06, 'epoch': 3.16}
 79%|███████▉  | 349/440 [24:27<06:12,  4.10s/it] 80%|███████▉  | 350/440 [24:31<06:07,  4.09s/it]                                                 {'loss': 1.1215, 'learning_rate': 4.225352112676057e-06, 'epoch': 3.17}
 80%|███████▉  | 350/440 [24:31<06:07,  4.09s/it] 80%|███████▉  | 351/440 [24:36<06:05,  4.10s/it]                                                 {'loss': 1.1693, 'learning_rate': 4.178403755868545e-06, 'epoch': 3.18}
 80%|███████▉  | 351/440 [24:36<06:05,  4.10s/it] 80%|████████  | 352/440 [24:40<06:00,  4.10s/it]                                                 {'loss': 1.0073, 'learning_rate': 4.131455399061034e-06, 'epoch': 3.19}
 80%|████████  | 352/440 [24:40<06:00,  4.10s/it] 80%|████████  | 353/440 [24:44<06:02,  4.16s/it]                                                 {'loss': 0.9868, 'learning_rate': 4.0845070422535216e-06, 'epoch': 3.2}
 80%|████████  | 353/440 [24:44<06:02,  4.16s/it] 80%|████████  | 354/440 [24:48<05:59,  4.18s/it]                                                 {'loss': 1.026, 'learning_rate': 4.0375586854460095e-06, 'epoch': 3.21}
 80%|████████  | 354/440 [24:48<05:59,  4.18s/it] 81%|████████  | 355/440 [24:52<05:51,  4.13s/it]                                                 {'loss': 1.2808, 'learning_rate': 3.990610328638498e-06, 'epoch': 3.22}
 81%|████████  | 355/440 [24:52<05:51,  4.13s/it] 81%|████████  | 356/440 [24:56<05:47,  4.13s/it]                                                 {'loss': 1.0691, 'learning_rate': 3.943661971830986e-06, 'epoch': 3.22}
 81%|████████  | 356/440 [24:56<05:47,  4.13s/it] 81%|████████  | 357/440 [25:01<05:46,  4.17s/it]                                                 {'loss': 1.1146, 'learning_rate': 3.896713615023475e-06, 'epoch': 3.23}
 81%|████████  | 357/440 [25:01<05:46,  4.17s/it] 81%|████████▏ | 358/440 [25:05<05:41,  4.16s/it]                                                 {'loss': 1.0505, 'learning_rate': 3.849765258215963e-06, 'epoch': 3.24}
 81%|████████▏ | 358/440 [25:05<05:41,  4.16s/it] 82%|████████▏ | 359/440 [25:09<05:35,  4.14s/it]                                                 {'loss': 1.1151, 'learning_rate': 3.8028169014084508e-06, 'epoch': 3.25}
 82%|████████▏ | 359/440 [25:09<05:35,  4.14s/it] 82%|████████▏ | 360/440 [25:13<05:29,  4.12s/it]                                                 {'loss': 0.9674, 'learning_rate': 3.755868544600939e-06, 'epoch': 3.26}
 82%|████████▏ | 360/440 [25:13<05:29,  4.12s/it] 82%|████████▏ | 361/440 [25:17<05:29,  4.17s/it]                                                 {'loss': 1.0773, 'learning_rate': 3.708920187793428e-06, 'epoch': 3.27}
 82%|████████▏ | 361/440 [25:17<05:29,  4.17s/it] 82%|████████▏ | 362/440 [25:21<05:21,  4.13s/it]                                                 {'loss': 0.8257, 'learning_rate': 3.6619718309859158e-06, 'epoch': 3.28}
 82%|████████▏ | 362/440 [25:21<05:21,  4.13s/it] 82%|████████▎ | 363/440 [25:25<05:18,  4.14s/it]                                                 {'loss': 1.035, 'learning_rate': 3.615023474178404e-06, 'epoch': 3.29}
 82%|████████▎ | 363/440 [25:25<05:18,  4.14s/it] 83%|████████▎ | 364/440 [25:29<05:13,  4.12s/it]                                                 {'loss': 0.9718, 'learning_rate': 3.568075117370892e-06, 'epoch': 3.3}
 83%|████████▎ | 364/440 [25:29<05:13,  4.12s/it] 83%|████████▎ | 365/440 [25:34<05:08,  4.11s/it]                                                 {'loss': 1.1441, 'learning_rate': 3.5211267605633804e-06, 'epoch': 3.31}
 83%|████████▎ | 365/440 [25:34<05:08,  4.11s/it] 83%|████████▎ | 366/440 [25:38<05:04,  4.11s/it]                                                 {'loss': 1.2418, 'learning_rate': 3.474178403755869e-06, 'epoch': 3.32}
 83%|████████▎ | 366/440 [25:38<05:04,  4.11s/it] 83%|████████▎ | 367/440 [25:42<05:00,  4.12s/it]                                                 {'loss': 1.0221, 'learning_rate': 3.427230046948357e-06, 'epoch': 3.32}
 83%|████████▎ | 367/440 [25:42<05:00,  4.12s/it] 84%|████████▎ | 368/440 [25:46<04:55,  4.10s/it]                                                 {'loss': 0.882, 'learning_rate': 3.3802816901408454e-06, 'epoch': 3.33}
 84%|████████▎ | 368/440 [25:46<04:55,  4.10s/it] 84%|████████▍ | 369/440 [25:50<04:53,  4.13s/it]                                                 {'loss': 1.1497, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.34}
 84%|████████▍ | 369/440 [25:50<04:53,  4.13s/it] 84%|████████▍ | 370/440 [25:54<04:49,  4.13s/it]                                                 {'loss': 1.1029, 'learning_rate': 3.286384976525822e-06, 'epoch': 3.35}
 84%|████████▍ | 370/440 [25:54<04:49,  4.13s/it] 84%|████████▍ | 371/440 [25:58<04:44,  4.12s/it]                                                 {'loss': 1.0458, 'learning_rate': 3.2394366197183104e-06, 'epoch': 3.36}
 84%|████████▍ | 371/440 [25:58<04:44,  4.12s/it] 85%|████████▍ | 372/440 [26:02<04:39,  4.12s/it]                                                 {'loss': 0.7383, 'learning_rate': 3.1924882629107983e-06, 'epoch': 3.37}
 85%|████████▍ | 372/440 [26:02<04:39,  4.12s/it] 85%|████████▍ | 373/440 [26:07<04:35,  4.11s/it]                                                 {'loss': 1.2101, 'learning_rate': 3.1455399061032867e-06, 'epoch': 3.38}
 85%|████████▍ | 373/440 [26:07<04:35,  4.11s/it] 85%|████████▌ | 374/440 [26:11<04:32,  4.12s/it]                                                 {'loss': 1.063, 'learning_rate': 3.0985915492957746e-06, 'epoch': 3.39}
 85%|████████▌ | 374/440 [26:11<04:32,  4.12s/it] 85%|████████▌ | 375/440 [26:15<04:26,  4.11s/it]                                                 {'loss': 1.0617, 'learning_rate': 3.0516431924882634e-06, 'epoch': 3.4}
 85%|████████▌ | 375/440 [26:15<04:26,  4.11s/it] 85%|████████▌ | 376/440 [26:19<04:22,  4.10s/it]                                                 {'loss': 0.9889, 'learning_rate': 3.0046948356807513e-06, 'epoch': 3.41}
 85%|████████▌ | 376/440 [26:19<04:22,  4.10s/it] 86%|████████▌ | 377/440 [26:23<04:21,  4.14s/it]                                                 {'loss': 1.0377, 'learning_rate': 2.9577464788732396e-06, 'epoch': 3.41}
 86%|████████▌ | 377/440 [26:23<04:21,  4.14s/it] 86%|████████▌ | 378/440 [26:27<04:15,  4.12s/it]                                                 {'loss': 1.1636, 'learning_rate': 2.910798122065728e-06, 'epoch': 3.42}
 86%|████████▌ | 378/440 [26:27<04:15,  4.12s/it] 86%|████████▌ | 379/440 [26:31<04:09,  4.09s/it]                                                 {'loss': 1.0376, 'learning_rate': 2.8638497652582163e-06, 'epoch': 3.43}
 86%|████████▌ | 379/440 [26:31<04:09,  4.09s/it] 86%|████████▋ | 380/440 [26:35<04:07,  4.13s/it]                                                 {'loss': 0.8805, 'learning_rate': 2.8169014084507046e-06, 'epoch': 3.44}
 86%|████████▋ | 380/440 [26:35<04:07,  4.13s/it] 87%|████████▋ | 381/440 [26:40<04:06,  4.18s/it]                                                 {'loss': 1.0234, 'learning_rate': 2.7699530516431926e-06, 'epoch': 3.45}
 87%|████████▋ | 381/440 [26:40<04:06,  4.18s/it] 87%|████████▋ | 382/440 [26:44<04:00,  4.15s/it]                                                 {'loss': 1.0802, 'learning_rate': 2.723004694835681e-06, 'epoch': 3.46}
 87%|████████▋ | 382/440 [26:44<04:00,  4.15s/it] 87%|████████▋ | 383/440 [26:48<03:55,  4.13s/it]                                                 {'loss': 1.2221, 'learning_rate': 2.676056338028169e-06, 'epoch': 3.47}
 87%|████████▋ | 383/440 [26:48<03:55,  4.13s/it] 87%|████████▋ | 384/440 [26:52<03:49,  4.10s/it]                                                 {'loss': 1.0618, 'learning_rate': 2.6291079812206576e-06, 'epoch': 3.48}
 87%|████████▋ | 384/440 [26:52<03:49,  4.10s/it] 88%|████████▊ | 385/440 [26:56<03:45,  4.11s/it]                                                 {'loss': 1.0117, 'learning_rate': 2.582159624413146e-06, 'epoch': 3.49}
 88%|████████▊ | 385/440 [26:56<03:45,  4.11s/it] 88%|████████▊ | 386/440 [27:00<03:41,  4.11s/it]                                                 {'loss': 1.1443, 'learning_rate': 2.535211267605634e-06, 'epoch': 3.5}
 88%|████████▊ | 386/440 [27:00<03:41,  4.11s/it] 88%|████████▊ | 387/440 [27:04<03:37,  4.10s/it]                                                 {'loss': 0.9823, 'learning_rate': 2.488262910798122e-06, 'epoch': 3.51}
 88%|████████▊ | 387/440 [27:04<03:37,  4.10s/it] 88%|████████▊ | 388/440 [27:08<03:33,  4.10s/it]                                                 {'loss': 0.9561, 'learning_rate': 2.4413145539906105e-06, 'epoch': 3.51}
 88%|████████▊ | 388/440 [27:08<03:33,  4.10s/it] 88%|████████▊ | 389/440 [27:12<03:29,  4.10s/it]                                                 {'loss': 1.0684, 'learning_rate': 2.3943661971830984e-06, 'epoch': 3.52}
 88%|████████▊ | 389/440 [27:12<03:29,  4.10s/it] 89%|████████▊ | 390/440 [27:16<03:24,  4.08s/it]                                                 {'loss': 0.9979, 'learning_rate': 2.347417840375587e-06, 'epoch': 3.53}
 89%|████████▊ | 390/440 [27:16<03:24,  4.08s/it] 89%|████████▉ | 391/440 [27:20<03:19,  4.08s/it]                                                 {'loss': 1.1826, 'learning_rate': 2.300469483568075e-06, 'epoch': 3.54}
 89%|████████▉ | 391/440 [27:20<03:19,  4.08s/it] 89%|████████▉ | 392/440 [27:25<03:18,  4.13s/it]                                                 {'loss': 0.9166, 'learning_rate': 2.2535211267605635e-06, 'epoch': 3.55}
 89%|████████▉ | 392/440 [27:25<03:18,  4.13s/it] 89%|████████▉ | 393/440 [27:29<03:12,  4.10s/it]                                                 {'loss': 1.2647, 'learning_rate': 2.206572769953052e-06, 'epoch': 3.56}
 89%|████████▉ | 393/440 [27:29<03:12,  4.10s/it] 90%|████████▉ | 394/440 [27:33<03:08,  4.09s/it]                                                 {'loss': 1.0956, 'learning_rate': 2.15962441314554e-06, 'epoch': 3.57}
 90%|████████▉ | 394/440 [27:33<03:08,  4.09s/it] 90%|████████▉ | 395/440 [27:37<03:03,  4.09s/it]                                                 {'loss': 0.8763, 'learning_rate': 2.1126760563380285e-06, 'epoch': 3.58}
 90%|████████▉ | 395/440 [27:37<03:03,  4.09s/it] 90%|█████████ | 396/440 [27:41<03:01,  4.12s/it]                                                 {'loss': 1.0758, 'learning_rate': 2.065727699530517e-06, 'epoch': 3.59}
 90%|█████████ | 396/440 [27:41<03:01,  4.12s/it] 90%|█████████ | 397/440 [27:45<02:57,  4.13s/it]                                                 {'loss': 1.1896, 'learning_rate': 2.0187793427230047e-06, 'epoch': 3.6}
 90%|█████████ | 397/440 [27:45<02:57,  4.13s/it] 90%|█████████ | 398/440 [27:49<02:52,  4.10s/it]                                                 {'loss': 0.9129, 'learning_rate': 1.971830985915493e-06, 'epoch': 3.6}
 90%|█████████ | 398/440 [27:49<02:52,  4.10s/it] 91%|█████████ | 399/440 [27:53<02:47,  4.08s/it]                                                 {'loss': 1.1322, 'learning_rate': 1.9248826291079814e-06, 'epoch': 3.61}
 91%|█████████ | 399/440 [27:53<02:47,  4.08s/it] 91%|█████████ | 400/440 [27:57<02:43,  4.10s/it]                                                 {'loss': 1.0498, 'learning_rate': 1.8779342723004696e-06, 'epoch': 3.62}
 91%|█████████ | 400/440 [27:57<02:43,  4.10s/it] 91%|█████████ | 401/440 [28:02<02:40,  4.13s/it]                                                 {'loss': 1.0064, 'learning_rate': 1.8309859154929579e-06, 'epoch': 3.63}
 91%|█████████ | 401/440 [28:02<02:40,  4.13s/it] 91%|█████████▏| 402/440 [28:06<02:36,  4.11s/it]                                                 {'loss': 0.9325, 'learning_rate': 1.784037558685446e-06, 'epoch': 3.64}
 91%|█████████▏| 402/440 [28:06<02:36,  4.11s/it] 92%|█████████▏| 403/440 [28:10<02:33,  4.14s/it]                                                 {'loss': 0.9589, 'learning_rate': 1.7370892018779346e-06, 'epoch': 3.65}
 92%|█████████▏| 403/440 [28:10<02:33,  4.14s/it] 92%|█████████▏| 404/440 [28:14<02:31,  4.20s/it]                                                 {'loss': 0.9864, 'learning_rate': 1.6901408450704227e-06, 'epoch': 3.66}
 92%|█████████▏| 404/440 [28:14<02:31,  4.20s/it] 92%|█████████▏| 405/440 [28:18<02:26,  4.19s/it]                                                 {'loss': 1.0619, 'learning_rate': 1.643192488262911e-06, 'epoch': 3.67}
 92%|█████████▏| 405/440 [28:18<02:26,  4.19s/it] 92%|█████████▏| 406/440 [28:23<02:21,  4.15s/it]                                                 {'loss': 1.1874, 'learning_rate': 1.5962441314553992e-06, 'epoch': 3.68}
 92%|█████████▏| 406/440 [28:23<02:21,  4.15s/it] 92%|█████████▎| 407/440 [28:27<02:17,  4.16s/it]                                                 {'loss': 0.9448, 'learning_rate': 1.5492957746478873e-06, 'epoch': 3.69}
 92%|█████████▎| 407/440 [28:27<02:17,  4.16s/it] 93%|█████████▎| 408/440 [28:31<02:13,  4.17s/it]                                                 {'loss': 0.9865, 'learning_rate': 1.5023474178403756e-06, 'epoch': 3.7}
 93%|█████████▎| 408/440 [28:31<02:13,  4.17s/it] 93%|█████████▎| 409/440 [28:35<02:08,  4.16s/it]                                                 {'loss': 0.9865, 'learning_rate': 1.455399061032864e-06, 'epoch': 3.7}
 93%|█████████▎| 409/440 [28:35<02:08,  4.16s/it] 93%|█████████▎| 410/440 [28:39<02:04,  4.16s/it]                                                 {'loss': 0.9503, 'learning_rate': 1.4084507042253523e-06, 'epoch': 3.71}
 93%|█████████▎| 410/440 [28:39<02:04,  4.16s/it] 93%|█████████▎| 411/440 [28:44<02:02,  4.23s/it]                                                 {'loss': 0.9942, 'learning_rate': 1.3615023474178405e-06, 'epoch': 3.72}
 93%|█████████▎| 411/440 [28:44<02:02,  4.23s/it] 94%|█████████▎| 412/440 [28:48<01:56,  4.17s/it]                                                 {'loss': 0.9494, 'learning_rate': 1.3145539906103288e-06, 'epoch': 3.73}
 94%|█████████▎| 412/440 [28:48<01:56,  4.17s/it] 94%|█████████▍| 413/440 [28:52<01:51,  4.14s/it]                                                 {'loss': 1.0738, 'learning_rate': 1.267605633802817e-06, 'epoch': 3.74}
 94%|█████████▍| 413/440 [28:52<01:51,  4.14s/it] 94%|█████████▍| 414/440 [28:56<01:48,  4.18s/it]                                                 {'loss': 1.2487, 'learning_rate': 1.2206572769953053e-06, 'epoch': 3.75}
 94%|█████████▍| 414/440 [28:56<01:48,  4.18s/it] 94%|█████████▍| 415/440 [29:00<01:44,  4.20s/it]                                                 {'loss': 0.8482, 'learning_rate': 1.1737089201877936e-06, 'epoch': 3.76}
 94%|█████████▍| 415/440 [29:00<01:44,  4.20s/it] 95%|█████████▍| 416/440 [29:04<01:40,  4.19s/it]                                                 {'loss': 0.9933, 'learning_rate': 1.1267605633802817e-06, 'epoch': 3.77}
 95%|█████████▍| 416/440 [29:04<01:40,  4.19s/it] 95%|█████████▍| 417/440 [29:08<01:35,  4.15s/it]                                                 {'loss': 1.0119, 'learning_rate': 1.07981220657277e-06, 'epoch': 3.78}
 95%|█████████▍| 417/440 [29:08<01:35,  4.15s/it] 95%|█████████▌| 418/440 [29:12<01:30,  4.12s/it]                                                 {'loss': 0.8928, 'learning_rate': 1.0328638497652584e-06, 'epoch': 3.79}
 95%|█████████▌| 418/440 [29:12<01:30,  4.12s/it] 95%|█████████▌| 419/440 [29:17<01:26,  4.10s/it]                                                 {'loss': 0.8934, 'learning_rate': 9.859154929577465e-07, 'epoch': 3.8}
 95%|█████████▌| 419/440 [29:17<01:26,  4.10s/it] 95%|█████████▌| 420/440 [29:21<01:22,  4.11s/it]                                                 {'loss': 0.9202, 'learning_rate': 9.389671361502348e-07, 'epoch': 3.8}
 95%|█████████▌| 420/440 [29:21<01:22,  4.11s/it] 96%|█████████▌| 421/440 [29:25<01:18,  4.12s/it]                                                 {'loss': 1.024, 'learning_rate': 8.92018779342723e-07, 'epoch': 3.81}
 96%|█████████▌| 421/440 [29:25<01:18,  4.12s/it] 96%|█████████▌| 422/440 [29:29<01:13,  4.09s/it]                                                 {'loss': 0.9615, 'learning_rate': 8.450704225352114e-07, 'epoch': 3.82}
 96%|█████████▌| 422/440 [29:29<01:13,  4.09s/it] 96%|█████████▌| 423/440 [29:33<01:09,  4.10s/it]                                                 {'loss': 0.8468, 'learning_rate': 7.981220657276996e-07, 'epoch': 3.83}
 96%|█████████▌| 423/440 [29:33<01:09,  4.10s/it] 96%|█████████▋| 424/440 [29:37<01:05,  4.07s/it]                                                 {'loss': 1.0955, 'learning_rate': 7.511737089201878e-07, 'epoch': 3.84}
 96%|█████████▋| 424/440 [29:37<01:05,  4.07s/it] 97%|█████████▋| 425/440 [29:41<01:01,  4.08s/it]                                                 {'loss': 1.2094, 'learning_rate': 7.042253521126762e-07, 'epoch': 3.85}
 97%|█████████▋| 425/440 [29:41<01:01,  4.08s/it] 97%|█████████▋| 426/440 [29:45<00:58,  4.14s/it]                                                 {'loss': 1.2298, 'learning_rate': 6.572769953051644e-07, 'epoch': 3.86}
 97%|█████████▋| 426/440 [29:45<00:58,  4.14s/it] 97%|█████████▋| 427/440 [29:49<00:53,  4.15s/it]                                                 {'loss': 1.2244, 'learning_rate': 6.103286384976526e-07, 'epoch': 3.87}
 97%|█████████▋| 427/440 [29:49<00:53,  4.15s/it] 97%|█████████▋| 428/440 [29:54<00:50,  4.19s/it]                                                 {'loss': 1.1307, 'learning_rate': 5.633802816901409e-07, 'epoch': 3.88}
 97%|█████████▋| 428/440 [29:54<00:50,  4.19s/it] 98%|█████████▊| 429/440 [29:58<00:45,  4.18s/it]                                                 {'loss': 0.8946, 'learning_rate': 5.164319248826292e-07, 'epoch': 3.89}
 98%|█████████▊| 429/440 [29:58<00:45,  4.18s/it] 98%|█████████▊| 430/440 [30:02<00:42,  4.21s/it]                                                 {'loss': 1.1538, 'learning_rate': 4.694835680751174e-07, 'epoch': 3.89}
 98%|█████████▊| 430/440 [30:02<00:42,  4.21s/it] 98%|█████████▊| 431/440 [30:06<00:37,  4.18s/it]                                                 {'loss': 1.047, 'learning_rate': 4.225352112676057e-07, 'epoch': 3.9}
 98%|█████████▊| 431/440 [30:06<00:37,  4.18s/it] 98%|█████████▊| 432/440 [30:10<00:33,  4.14s/it]                                                 {'loss': 1.1979, 'learning_rate': 3.755868544600939e-07, 'epoch': 3.91}
 98%|█████████▊| 432/440 [30:10<00:33,  4.14s/it] 98%|█████████▊| 433/440 [30:14<00:28,  4.12s/it]                                                 {'loss': 0.8595, 'learning_rate': 3.286384976525822e-07, 'epoch': 3.92}
 98%|█████████▊| 433/440 [30:14<00:28,  4.12s/it] 99%|█████████▊| 434/440 [30:18<00:24,  4.10s/it]                                                 {'loss': 0.8678, 'learning_rate': 2.8169014084507043e-07, 'epoch': 3.93}
 99%|█████████▊| 434/440 [30:18<00:24,  4.10s/it] 99%|█████████▉| 435/440 [30:23<00:20,  4.09s/it]                                                 {'loss': 1.1679, 'learning_rate': 2.347417840375587e-07, 'epoch': 3.94}
 99%|█████████▉| 435/440 [30:23<00:20,  4.09s/it] 99%|█████████▉| 436/440 [30:27<00:16,  4.08s/it]                                                 {'loss': 0.9067, 'learning_rate': 1.8779342723004696e-07, 'epoch': 3.95}
 99%|█████████▉| 436/440 [30:27<00:16,  4.08s/it] 99%|█████████▉| 437/440 [30:31<00:12,  4.11s/it]                                                 {'loss': 1.0944, 'learning_rate': 1.4084507042253522e-07, 'epoch': 3.96}
 99%|█████████▉| 437/440 [30:31<00:12,  4.11s/it]100%|█████████▉| 438/440 [30:35<00:08,  4.08s/it]                                                 {'loss': 0.7216, 'learning_rate': 9.389671361502348e-08, 'epoch': 3.97}
100%|█████████▉| 438/440 [30:35<00:08,  4.08s/it]100%|█████████▉| 439/440 [30:39<00:04,  4.09s/it]                                                 {'loss': 1.1064, 'learning_rate': 4.694835680751174e-08, 'epoch': 3.98}
100%|█████████▉| 439/440 [30:39<00:04,  4.09s/it]100%|██████████| 440/440 [30:43<00:00,  4.13s/it]                                                 {'loss': 0.8931, 'learning_rate': 0.0, 'epoch': 3.99}
100%|██████████| 440/440 [30:43<00:00,  4.13s/it][INFO|trainer.py:2889] 2024-11-27 22:11:17,591 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tmp-checkpoint-440
[INFO|tokenization_utils_base.py:2432] 2024-11-27 22:11:18,063 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tmp-checkpoint-440/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 22:11:18,064 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tmp-checkpoint-440/special_tokens_map.json
[INFO|trainer.py:1947] 2024-11-27 22:11:18,781 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 1845.6695, 'train_samples_per_second': 7.657, 'train_steps_per_second': 0.238, 'train_loss': 1.1790268474004486, 'epoch': 3.99}
100%|██████████| 440/440 [30:44<00:00,  4.13s/it]100%|██████████| 440/440 [30:44<00:00,  4.19s/it]
after training done!!!!
[INFO|trainer.py:2889] 2024-11-27 22:11:18,784 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4
[INFO|tokenization_utils_base.py:2432] 2024-11-27 22:11:19,292 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 22:11:19,293 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-7b-less-p0.05-lora-seed4/special_tokens_map.json
***** train metrics *****
  epoch                    =       3.99
  train_loss               =      1.179
  train_runtime            = 0:30:45.66
  train_samples            =       3533
  train_samples_per_second =      7.657
  train_steps_per_second   =      0.238
[1;34mwandb[0m: 🚀 View run [33mbalmy-smoke-79[0m at: [34mhttps://wandb.ai/colinnyuh-university-of-amsterdam/huggingface/runs/qiy5oq59[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241127_214033-qiy5oq59/logs[0m

JOB STATISTICS
==============
Job ID: 8720503
Cluster: snellius
User/Group: scur2847/scur2847
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 16
CPU Utilized: 00:31:58
CPU Efficiency: 6.35% of 08:23:12 core-walltime
Job Wall-clock time: 00:31:27
Memory Utilized: 2.17 GB
Memory Efficiency: 1.20% of 180.00 GB
