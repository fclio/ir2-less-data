torchrun --nproc_per_node 2 --nnodes 1 --rdzv-id=13577 --rdzv_backend c10d -m less.train.train --do_train True --max_seq_length 2048 --use_fast_tokenizer True --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0.0 --evaluation_strategy no --logging_steps 1 --save_strategy no --num_train_epochs 4 --bf16 True --tf32 False --fp16 False --overwrite_output_dir True --report_to wandb --optim adamw_torch --seed 0 --percentage 1.0 --save_strategy epoch --lora True --lora_r 128 --lora_alpha 512 --lora_dropout 0.1 --lora_target_modules q_proj k_proj v_proj o_proj --learning_rate 2e-05 --per_device_train_batch_size 1 --gradient_accumulation_steps 32 --fsdp 'full_shard auto_wrap' --fsdp_config llama2_13b_finetune --model_name_or_path meta-llama/Llama-2-13b-hf --output_dir /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4 --train_files /scratch-shared/ir2-less/selected_data/llama2-13b-p0.05-lora-seed4/bbh/top_p0.05.jsonl 2>&1 | tee /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/train.log
[2024-11-30 15:39:51,213] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
11/30/2024 15:39:57 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
/home/scur2847/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: colinnyuh (colinnyuh-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/scur2847/.netrc
11/30/2024 15:39:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/30/2024 15:39:58 - INFO - __main__ - Training parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
analysis_dataset=bbh,
analysis_mode=False,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],
fsdp_config={'fsdp_transformer_layer_cls_to_wrap': ['LlamaDecoderLayer'], 'fsdp_backward_prefetch': 'backward_pre', 'limit_all_gathers': 'true', 'use_orig_params': 'true', 'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=32,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/runs/Nov30_15-39-54_gcn101.local.snellius.surf.nl,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=4.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=/scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=0,
skip_memory_metrics=True,
split_batches=False,
tf32=False,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_dataset_names=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
11/30/2024 15:39:58 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='meta-llama/Llama-2-13b-hf', config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, torch_dtype=None, lora=True, lora_r=128, lora_alpha=512.0, lora_dropout=0.1, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'])
11/30/2024 15:39:58 - INFO - __main__ - Dataset parameters DataArguments(train_files=['/scratch-shared/ir2-less/selected_data/llama2-13b-p0.05-lora-seed4/bbh/top_p0.05.jsonl'], overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=2048, sample_data_seed=42, percentage=1.0)
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/scur2847/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[INFO|tokenization_utils_base.py:2026] 2024-11-30 15:39:58,902 >> loading file tokenizer.model from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/tokenizer.model
[INFO|tokenization_utils_base.py:2026] 2024-11-30 15:39:58,902 >> loading file tokenizer.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/tokenizer.json
[INFO|tokenization_utils_base.py:2026] 2024-11-30 15:39:58,902 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2026] 2024-11-30 15:39:58,902 >> loading file special_tokens_map.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/special_tokens_map.json
[INFO|tokenization_utils_base.py:2026] 2024-11-30 15:39:58,902 >> loading file tokenizer_config.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/tokenizer_config.json
Using custom data configuration default-ad55c11fca635a87
11/30/2024 15:39:59 - INFO - datasets.builder - Using custom data configuration default-ad55c11fca635a87
Loading Dataset Infos from /home/scur2847/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
11/30/2024 15:39:59 - INFO - datasets.info - Loading Dataset Infos from /home/scur2847/.local/lib/python3.11/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
11/30/2024 15:39:59 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
11/30/2024 15:39:59 - INFO - datasets.info - Loading Dataset info from /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
11/30/2024 15:39:59 - INFO - datasets.builder - Found cached dataset json (/home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
11/30/2024 15:39:59 - INFO - datasets.info - Loading Dataset info from /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Process #0 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00000_of_00010.arrow
11/30/2024 15:39:59 - INFO - datasets.arrow_dataset - Process #0 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00000_of_00010.arrow
Process #1 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00001_of_00010.arrow
11/30/2024 15:39:59 - INFO - datasets.arrow_dataset - Process #1 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00001_of_00010.arrow
Process #2 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00002_of_00010.arrow
11/30/2024 15:39:59 - INFO - datasets.arrow_dataset - Process #2 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00002_of_00010.arrow
Process #3 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00003_of_00010.arrow
11/30/2024 15:39:59 - INFO - datasets.arrow_dataset - Process #3 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00003_of_00010.arrow
Process #4 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00004_of_00010.arrow
11/30/2024 15:39:59 - INFO - datasets.arrow_dataset - Process #4 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00004_of_00010.arrow
Process #5 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00005_of_00010.arrow
11/30/2024 15:39:59 - INFO - datasets.arrow_dataset - Process #5 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00005_of_00010.arrow
Process #6 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00006_of_00010.arrow
11/30/2024 15:39:59 - INFO - datasets.arrow_dataset - Process #6 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00006_of_00010.arrow
Process #7 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00007_of_00010.arrow
11/30/2024 15:39:59 - INFO - datasets.arrow_dataset - Process #7 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00007_of_00010.arrow
Process #8 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00008_of_00010.arrow
11/30/2024 15:39:59 - INFO - datasets.arrow_dataset - Process #8 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00008_of_00010.arrow
Process #9 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00009_of_00010.arrow
11/30/2024 15:39:59 - INFO - datasets.arrow_dataset - Process #9 will write at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_00009_of_00010.arrow
Loading cached processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_*_of_00010.arrow
11/30/2024 15:39:59 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-03053f631c13724d_*_of_00010.arrow
Concatenating 10 shards
11/30/2024 15:39:59 - INFO - datasets.arrow_dataset - Concatenating 10 shards
[INFO|configuration_utils.py:739] 2024-11-30 15:39:59,455 >> loading configuration file config.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/config.json
[INFO|configuration_utils.py:802] 2024-11-30 15:39:59,456 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-13b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.36.2",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3344] 2024-11-30 15:39:59,513 >> loading weights file model.safetensors from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/model.safetensors.index.json
[INFO|configuration_utils.py:826] 2024-11-30 15:39:59,514 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.06it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:03<00:02,  2.02s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.78s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.23s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.98s/it]
[INFO|modeling_utils.py:4185] 2024-11-30 15:40:07,359 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4193] 2024-11-30 15:40:07,359 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-13b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:781] 2024-11-30 15:40:07,468 >> loading configuration file generation_config.json from cache at /home/scur2847/.cache/huggingface/hub/models--meta-llama--Llama-2-13b-hf/snapshots/5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1/generation_config.json
[INFO|configuration_utils.py:826] 2024-11-30 15:40:07,468 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|modeling_utils.py:1813] 2024-11-30 15:40:07,473 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trainable params: 209,715,200 || all params: 13,225,589,760 || trainable%: 1.5856774919351497
[train set] examples: 13533; # avg tokens: 401.1299743652344
[train set] examples: 13533; # avg completion tokens: 50.52952194213867
/home/scur2847/.local/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
start training!!!!
11/30/2024 15:40:10 - INFO - __main__ - Applied LoRA to model.
trainable params: 209,715,200 || all params: 13,225,589,760 || trainable%: 1.5856774919351497
Loading cached processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-17465b6ed483c300.arrow
11/30/2024 15:40:10 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/scur2847/.cache/huggingface/datasets/json/default-ad55c11fca635a87/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-17465b6ed483c300.arrow
[train set] examples: 13533; # avg tokens: 401.1299743652344
[train set] examples: 13533; # avg completion tokens: 50.52952194213867
11/30/2024 15:40:10 - INFO - __main__ - Sample 6311 of the training set: {'input_ids': tensor([    1,   529, 29989,  1792, 29989, 29958,    13, 29984, 29901,  8809,
          436,   310,  1438, 25260,  1838, 29915, 29873,  1207,  4060, 29973,
           13,  5856, 29901,    13, 29899, 28048,   663,   319, 29901,   376,
        29956,   287,  8497,  6263,   508,  3799,  2748,  1432,  1629,  1213,
           13, 29899, 28048,   663,   350, 29901,   376, 29933,  7515,  3250,
         6263,   508,  3799,  2748,  1432,  1629,  1213,    13,    13, 29909,
        29901, 14804,  2305,   505,   871,   697, 14837,  8497,  6263,   297,
         1009, 25423, 29889,    13, 29961, 29903,   296,   663,   319, 29962,
           13,    13,    13,    13, 29984, 29901,  6716,   310,   278,  1494,
        25260,   338,   302,   787,   575,   936, 29889,  8449,   697,   338,
          372, 29973,    13,  5856, 29901,    13, 29899, 28048,   663,   319,
        29901,   376,  3868, 14387,   431,  2580,   670,   274,   627,   375,
        29908,    13, 29899, 28048,   663,   350, 29901,   376,  3868, 14387,
          431,  2580,   670,  6795,  3332, 29879,  1213,    13,    13, 29909,
        29901,  3492,   508,   451, 14387,   431,   263,   274,   627,   375,
         1728,  2805,   544, 17840, 29889,    13, 29961, 29903,   296,   663,
          319, 29962,    13,    13,    13,    13, 29984, 29901,  2776,   278,
         2400, 25260, 29892,   607,   697,   947,   334,  1333, 29930,  1207,
         4060, 29973,    13,  5856, 29901,    13, 29899, 28048,   663,   319,
        29901,   376,  3421, 22169,  7124,   278, 29416,  8287,   964,   278,
        16188,   411,   670,  9885, 29891, 21573, 17152,   636, 29908,    13,
        29899, 28048,   663,   350, 29901,   376,  3421, 22169,  7124,   278,
        29416,  8287,   964,   278, 16188,   411,   670,  9885, 29891, 29416,
         4402,  1213,    13,    13, 29909, 29901,    13, 29966, 29989,   465,
        22137, 29989, 29958,    13,  5160,  2135,   289,  1446,   526,  8688,
          304,  7124,   871, 21573, 29879,   322,  4964,  2135, 29879, 29889,
           13, 29961, 29903,   296,   663,   319, 29962,     2, 29871,    13]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  5160,  2135,   289,  1446,   526,  8688,
          304,  7124,   871, 21573, 29879,   322,  4964,  2135, 29879, 29889,
           13, 29961, 29903,   296,   663,   319, 29962,     2, 29871,    13]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1])}.
11/30/2024 15:40:10 - INFO - __main__ - trainable model_params: 209715200
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32001, 5120)
        (layers): ModuleList(
          (0-39): 40 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=5120, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=5120, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear(
                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=5120, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=5120, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=5120, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=5120, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=5120, out_features=5120, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=5120, out_features=128, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=128, out_features=5120, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)
              (up_proj): Linear(in_features=5120, out_features=13824, bias=False)
              (down_proj): Linear(in_features=13824, out_features=5120, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=5120, out_features=32001, bias=False)
    )
  )
)
/home/scur2847/.local/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
[INFO|trainer.py:568] 2024-11-30 15:40:10,820 >> Using auto half precision backend
start training!!!!
[INFO|trainer.py:1706] 2024-11-30 15:40:15,720 >> ***** Running training *****
[INFO|trainer.py:1707] 2024-11-30 15:40:15,720 >>   Num examples = 13,533
[INFO|trainer.py:1708] 2024-11-30 15:40:15,720 >>   Num Epochs = 4
[INFO|trainer.py:1709] 2024-11-30 15:40:15,720 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1712] 2024-11-30 15:40:15,720 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1713] 2024-11-30 15:40:15,720 >>   Gradient Accumulation steps = 32
[INFO|trainer.py:1714] 2024-11-30 15:40:15,720 >>   Total optimization steps = 844
[INFO|trainer.py:1715] 2024-11-30 15:40:15,722 >>   Number of trainable parameters = 209,715,200
[INFO|integration_utils.py:722] 2024-11-30 15:40:15,727 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /gpfs/home1/scur2847/ir2-less-data/wandb/run-20241130_154015-6wynsd12
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-resonance-91
wandb: ⭐️ View project at https://wandb.ai/colinnyuh-university-of-amsterdam/huggingface
wandb: 🚀 View run at https://wandb.ai/colinnyuh-university-of-amsterdam/huggingface/runs/6wynsd12
  0%|          | 0/844 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-30 15:40:16,275 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[WARNING|logging.py:314] 2024-11-30 15:40:16,275 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/844 [00:13<3:11:03, 13.60s/it]                                                 {'loss': 2.4764, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.0}
  0%|          | 1/844 [00:13<3:11:03, 13.60s/it]  0%|          | 2/844 [00:27<3:13:32, 13.79s/it]                                                 {'loss': 2.5226, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.01}
  0%|          | 2/844 [00:27<3:13:32, 13.79s/it]  0%|          | 3/844 [00:41<3:16:30, 14.02s/it]                                                 {'loss': 2.199, 'learning_rate': 2.307692307692308e-06, 'epoch': 0.01}
  0%|          | 3/844 [00:41<3:16:30, 14.02s/it]  0%|          | 4/844 [00:55<3:16:54, 14.06s/it]                                                 {'loss': 2.4162, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.02}
  0%|          | 4/844 [00:55<3:16:54, 14.06s/it]  1%|          | 5/844 [01:10<3:16:56, 14.08s/it]                                                 {'loss': 2.3803, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.02}
  1%|          | 5/844 [01:10<3:16:56, 14.08s/it]  1%|          | 6/844 [01:24<3:16:03, 14.04s/it]                                                 {'loss': 2.3865, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.03}
  1%|          | 6/844 [01:24<3:16:03, 14.04s/it]  1%|          | 7/844 [01:37<3:15:04, 13.98s/it]                                                 {'loss': 2.6114, 'learning_rate': 5.384615384615385e-06, 'epoch': 0.03}
  1%|          | 7/844 [01:37<3:15:04, 13.98s/it]  1%|          | 8/844 [01:51<3:12:05, 13.79s/it]                                                 {'loss': 2.8806, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.04}
  1%|          | 8/844 [01:51<3:12:05, 13.79s/it]  1%|          | 9/844 [02:05<3:13:16, 13.89s/it]                                                 {'loss': 3.0066, 'learning_rate': 6.923076923076923e-06, 'epoch': 0.04}
  1%|          | 9/844 [02:05<3:13:16, 13.89s/it]  1%|          | 10/844 [02:19<3:13:33, 13.93s/it]                                                  {'loss': 2.4412, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.05}
  1%|          | 10/844 [02:19<3:13:33, 13.93s/it]  1%|▏         | 11/844 [02:33<3:13:14, 13.92s/it]                                                  {'loss': 2.182, 'learning_rate': 8.461538461538462e-06, 'epoch': 0.05}
  1%|▏         | 11/844 [02:33<3:13:14, 13.92s/it]  1%|▏         | 12/844 [02:46<3:11:52, 13.84s/it]                                                  {'loss': 2.4832, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.06}
  1%|▏         | 12/844 [02:46<3:11:52, 13.84s/it]  2%|▏         | 13/844 [03:00<3:11:59, 13.86s/it]                                                  {'loss': 2.4849, 'learning_rate': 1e-05, 'epoch': 0.06}
  2%|▏         | 13/844 [03:00<3:11:59, 13.86s/it]  2%|▏         | 14/844 [03:14<3:11:48, 13.87s/it]                                                  {'loss': 2.4594, 'learning_rate': 1.076923076923077e-05, 'epoch': 0.07}
  2%|▏         | 14/844 [03:14<3:11:48, 13.87s/it]  2%|▏         | 15/844 [03:28<3:11:32, 13.86s/it]                                                  {'loss': 2.2613, 'learning_rate': 1.1538461538461538e-05, 'epoch': 0.07}
  2%|▏         | 15/844 [03:28<3:11:32, 13.86s/it]  2%|▏         | 16/844 [03:41<3:09:21, 13.72s/it]                                                  {'loss': 2.4901, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.08}
  2%|▏         | 16/844 [03:41<3:09:21, 13.72s/it]  2%|▏         | 17/844 [03:55<3:09:12, 13.73s/it]                                                  {'loss': 2.1597, 'learning_rate': 1.3076923076923078e-05, 'epoch': 0.08}
  2%|▏         | 17/844 [03:55<3:09:12, 13.73s/it]  2%|▏         | 18/844 [04:09<3:09:00, 13.73s/it]                                                  {'loss': 2.7891, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.09}
  2%|▏         | 18/844 [04:09<3:09:00, 13.73s/it]  2%|▏         | 19/844 [04:22<3:07:44, 13.65s/it]                                                  {'loss': 2.3439, 'learning_rate': 1.4615384615384615e-05, 'epoch': 0.09}
  2%|▏         | 19/844 [04:22<3:07:44, 13.65s/it]  2%|▏         | 20/844 [04:36<3:06:48, 13.60s/it]                                                  {'loss': 2.2096, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.09}
  2%|▏         | 20/844 [04:36<3:06:48, 13.60s/it]  2%|▏         | 21/844 [04:50<3:08:04, 13.71s/it]                                                  {'loss': 2.2376, 'learning_rate': 1.6153846153846154e-05, 'epoch': 0.1}
  2%|▏         | 21/844 [04:50<3:08:04, 13.71s/it]  3%|▎         | 22/844 [05:03<3:06:45, 13.63s/it]                                                  {'loss': 2.1869, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.1}
  3%|▎         | 22/844 [05:03<3:06:45, 13.63s/it]  3%|▎         | 23/844 [05:17<3:07:36, 13.71s/it]                                                  {'loss': 2.1959, 'learning_rate': 1.7692307692307694e-05, 'epoch': 0.11}
  3%|▎         | 23/844 [05:17<3:07:36, 13.71s/it]  3%|▎         | 24/844 [05:31<3:07:33, 13.72s/it]                                                  {'loss': 2.4922, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.11}
  3%|▎         | 24/844 [05:31<3:07:33, 13.72s/it]  3%|▎         | 25/844 [05:45<3:06:45, 13.68s/it]                                                  {'loss': 2.5106, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.12}
  3%|▎         | 25/844 [05:45<3:06:45, 13.68s/it]  3%|▎         | 26/844 [05:58<3:05:36, 13.61s/it]                                                  {'loss': 2.3473, 'learning_rate': 2e-05, 'epoch': 0.12}
  3%|▎         | 26/844 [05:58<3:05:36, 13.61s/it]  3%|▎         | 27/844 [06:12<3:05:28, 13.62s/it]                                                  {'loss': 2.2123, 'learning_rate': 1.997555012224939e-05, 'epoch': 0.13}
  3%|▎         | 27/844 [06:12<3:05:28, 13.62s/it]  3%|▎         | 28/844 [06:26<3:06:24, 13.71s/it]                                                  {'loss': 2.3666, 'learning_rate': 1.995110024449878e-05, 'epoch': 0.13}
  3%|▎         | 28/844 [06:26<3:06:24, 13.71s/it]  3%|▎         | 29/844 [06:39<3:05:50, 13.68s/it]                                                  {'loss': 2.1356, 'learning_rate': 1.992665036674817e-05, 'epoch': 0.14}
  3%|▎         | 29/844 [06:39<3:05:50, 13.68s/it]  4%|▎         | 30/844 [06:53<3:05:02, 13.64s/it]                                                  {'loss': 2.053, 'learning_rate': 1.9902200488997556e-05, 'epoch': 0.14}
  4%|▎         | 30/844 [06:53<3:05:02, 13.64s/it]  4%|▎         | 31/844 [07:07<3:05:41, 13.70s/it]                                                  {'loss': 2.3782, 'learning_rate': 1.9877750611246945e-05, 'epoch': 0.15}
  4%|▎         | 31/844 [07:07<3:05:41, 13.70s/it]  4%|▍         | 32/844 [07:21<3:08:08, 13.90s/it]                                                  {'loss': 2.268, 'learning_rate': 1.9853300733496334e-05, 'epoch': 0.15}
  4%|▍         | 32/844 [07:21<3:08:08, 13.90s/it]  4%|▍         | 33/844 [07:35<3:06:39, 13.81s/it]                                                  {'loss': 2.0918, 'learning_rate': 1.9828850855745724e-05, 'epoch': 0.16}
  4%|▍         | 33/844 [07:35<3:06:39, 13.81s/it]  4%|▍         | 34/844 [07:48<3:06:54, 13.84s/it]                                                  {'loss': 2.1157, 'learning_rate': 1.980440097799511e-05, 'epoch': 0.16}
  4%|▍         | 34/844 [07:48<3:06:54, 13.84s/it]  4%|▍         | 35/844 [08:02<3:06:47, 13.85s/it]                                                  {'loss': 2.2292, 'learning_rate': 1.9779951100244502e-05, 'epoch': 0.17}
  4%|▍         | 35/844 [08:02<3:06:47, 13.85s/it]  4%|▍         | 36/844 [08:16<3:06:17, 13.83s/it]                                                  {'loss': 1.7035, 'learning_rate': 1.975550122249389e-05, 'epoch': 0.17}
  4%|▍         | 36/844 [08:16<3:06:17, 13.83s/it]  4%|▍         | 37/844 [08:30<3:06:50, 13.89s/it]                                                  {'loss': 1.9529, 'learning_rate': 1.9731051344743278e-05, 'epoch': 0.17}
  4%|▍         | 37/844 [08:30<3:06:50, 13.89s/it]  5%|▍         | 38/844 [08:44<3:06:02, 13.85s/it]                                                  {'loss': 2.0806, 'learning_rate': 1.9706601466992667e-05, 'epoch': 0.18}
  5%|▍         | 38/844 [08:44<3:06:02, 13.85s/it]  5%|▍         | 39/844 [08:58<3:05:27, 13.82s/it]                                                  {'loss': 2.1443, 'learning_rate': 1.9682151589242056e-05, 'epoch': 0.18}
  5%|▍         | 39/844 [08:58<3:05:27, 13.82s/it]  5%|▍         | 40/844 [09:12<3:05:21, 13.83s/it]                                                  {'loss': 2.1338, 'learning_rate': 1.9657701711491442e-05, 'epoch': 0.19}
  5%|▍         | 40/844 [09:12<3:05:21, 13.83s/it]  5%|▍         | 41/844 [09:25<3:04:11, 13.76s/it]                                                  {'loss': 2.0432, 'learning_rate': 1.9633251833740835e-05, 'epoch': 0.19}
  5%|▍         | 41/844 [09:25<3:04:11, 13.76s/it]  5%|▍         | 42/844 [09:39<3:03:19, 13.71s/it]                                                  {'loss': 2.0778, 'learning_rate': 1.960880195599022e-05, 'epoch': 0.2}
  5%|▍         | 42/844 [09:39<3:03:19, 13.71s/it]  5%|▌         | 43/844 [09:52<3:01:06, 13.57s/it]                                                  {'loss': 1.9498, 'learning_rate': 1.958435207823961e-05, 'epoch': 0.2}
  5%|▌         | 43/844 [09:52<3:01:06, 13.57s/it]  5%|▌         | 44/844 [10:05<3:00:13, 13.52s/it]                                                  {'loss': 2.1293, 'learning_rate': 1.9559902200489e-05, 'epoch': 0.21}
  5%|▌         | 44/844 [10:05<3:00:13, 13.52s/it]  5%|▌         | 45/844 [10:19<3:00:17, 13.54s/it]                                                  {'loss': 2.0395, 'learning_rate': 1.953545232273839e-05, 'epoch': 0.21}
  5%|▌         | 45/844 [10:19<3:00:17, 13.54s/it]  5%|▌         | 46/844 [10:33<3:01:13, 13.63s/it]                                                  {'loss': 1.8693, 'learning_rate': 1.9511002444987775e-05, 'epoch': 0.22}
  5%|▌         | 46/844 [10:33<3:01:13, 13.63s/it]  6%|▌         | 47/844 [10:46<3:01:11, 13.64s/it]                                                  {'loss': 2.2897, 'learning_rate': 1.9486552567237164e-05, 'epoch': 0.22}
  6%|▌         | 47/844 [10:46<3:01:11, 13.64s/it]  6%|▌         | 48/844 [11:00<3:02:03, 13.72s/it]                                                  {'loss': 2.2415, 'learning_rate': 1.9462102689486554e-05, 'epoch': 0.23}
  6%|▌         | 48/844 [11:00<3:02:03, 13.72s/it]  6%|▌         | 49/844 [11:14<3:03:13, 13.83s/it]                                                  {'loss': 2.1184, 'learning_rate': 1.9437652811735943e-05, 'epoch': 0.23}
  6%|▌         | 49/844 [11:14<3:03:13, 13.83s/it]  6%|▌         | 50/844 [11:28<3:02:46, 13.81s/it]                                                  {'loss': 2.245, 'learning_rate': 1.9413202933985333e-05, 'epoch': 0.24}
  6%|▌         | 50/844 [11:28<3:02:46, 13.81s/it]  6%|▌         | 51/844 [11:42<3:02:26, 13.80s/it]                                                  {'loss': 2.1061, 'learning_rate': 1.9388753056234722e-05, 'epoch': 0.24}
  6%|▌         | 51/844 [11:42<3:02:26, 13.80s/it]  6%|▌         | 52/844 [11:56<3:01:38, 13.76s/it]                                                  {'loss': 1.7763, 'learning_rate': 1.936430317848411e-05, 'epoch': 0.25}
  6%|▌         | 52/844 [11:56<3:01:38, 13.76s/it]  6%|▋         | 53/844 [12:09<3:00:56, 13.72s/it]                                                  {'loss': 1.9987, 'learning_rate': 1.9339853300733497e-05, 'epoch': 0.25}
  6%|▋         | 53/844 [12:09<3:00:56, 13.72s/it]  6%|▋         | 54/844 [12:23<3:00:31, 13.71s/it]                                                  {'loss': 2.003, 'learning_rate': 1.9315403422982887e-05, 'epoch': 0.26}
  6%|▋         | 54/844 [12:23<3:00:31, 13.71s/it]  7%|▋         | 55/844 [12:37<3:00:13, 13.71s/it]                                                  {'loss': 1.8958, 'learning_rate': 1.9290953545232276e-05, 'epoch': 0.26}
  7%|▋         | 55/844 [12:37<3:00:13, 13.71s/it]  7%|▋         | 56/844 [12:51<3:01:10, 13.79s/it]                                                  {'loss': 1.7986, 'learning_rate': 1.9266503667481665e-05, 'epoch': 0.26}
  7%|▋         | 56/844 [12:51<3:01:10, 13.79s/it]  7%|▋         | 57/844 [13:04<3:00:07, 13.73s/it]                                                  {'loss': 1.8891, 'learning_rate': 1.924205378973105e-05, 'epoch': 0.27}
  7%|▋         | 57/844 [13:04<3:00:07, 13.73s/it]  7%|▋         | 58/844 [13:18<2:59:23, 13.69s/it]                                                  {'loss': 1.8146, 'learning_rate': 1.9217603911980444e-05, 'epoch': 0.27}
  7%|▋         | 58/844 [13:18<2:59:23, 13.69s/it]  7%|▋         | 59/844 [13:32<2:59:38, 13.73s/it]                                                  {'loss': 1.8426, 'learning_rate': 1.919315403422983e-05, 'epoch': 0.28}
  7%|▋         | 59/844 [13:32<2:59:38, 13.73s/it]  7%|▋         | 60/844 [13:46<3:00:09, 13.79s/it]                                                  {'loss': 1.5888, 'learning_rate': 1.916870415647922e-05, 'epoch': 0.28}
  7%|▋         | 60/844 [13:46<3:00:09, 13.79s/it]  7%|▋         | 61/844 [13:59<2:58:37, 13.69s/it]                                                  {'loss': 1.5072, 'learning_rate': 1.914425427872861e-05, 'epoch': 0.29}
  7%|▋         | 61/844 [13:59<2:58:37, 13.69s/it]  7%|▋         | 62/844 [14:13<3:00:32, 13.85s/it]                                                  {'loss': 1.5138, 'learning_rate': 1.9119804400977998e-05, 'epoch': 0.29}
  7%|▋         | 62/844 [14:13<3:00:32, 13.85s/it]  7%|▋         | 63/844 [14:27<2:59:20, 13.78s/it]                                                  {'loss': 1.2412, 'learning_rate': 1.9095354523227384e-05, 'epoch': 0.3}
  7%|▋         | 63/844 [14:27<2:59:20, 13.78s/it]  8%|▊         | 64/844 [14:40<2:58:18, 13.72s/it]                                                  {'loss': 1.2222, 'learning_rate': 1.9070904645476773e-05, 'epoch': 0.3}
  8%|▊         | 64/844 [14:40<2:58:18, 13.72s/it]  8%|▊         | 65/844 [14:54<2:58:46, 13.77s/it]                                                  {'loss': 1.1765, 'learning_rate': 1.9046454767726163e-05, 'epoch': 0.31}
  8%|▊         | 65/844 [14:54<2:58:46, 13.77s/it]  8%|▊         | 66/844 [15:08<2:57:33, 13.69s/it]                                                  {'loss': 1.1029, 'learning_rate': 1.9022004889975552e-05, 'epoch': 0.31}
  8%|▊         | 66/844 [15:08<2:57:33, 13.69s/it]  8%|▊         | 67/844 [15:22<2:57:43, 13.72s/it]                                                  {'loss': 1.1393, 'learning_rate': 1.899755501222494e-05, 'epoch': 0.32}
  8%|▊         | 67/844 [15:22<2:57:43, 13.72s/it]  8%|▊         | 68/844 [15:35<2:56:45, 13.67s/it]                                                  {'loss': 0.8616, 'learning_rate': 1.897310513447433e-05, 'epoch': 0.32}
  8%|▊         | 68/844 [15:35<2:56:45, 13.67s/it]  8%|▊         | 69/844 [15:49<2:57:46, 13.76s/it]                                                  {'loss': 0.928, 'learning_rate': 1.8948655256723717e-05, 'epoch': 0.33}
  8%|▊         | 69/844 [15:49<2:57:46, 13.76s/it]  8%|▊         | 70/844 [16:03<2:56:06, 13.65s/it]                                                  {'loss': 1.0226, 'learning_rate': 1.8924205378973106e-05, 'epoch': 0.33}
  8%|▊         | 70/844 [16:03<2:56:06, 13.65s/it]  8%|▊         | 71/844 [16:16<2:55:57, 13.66s/it]                                                  {'loss': 0.8965, 'learning_rate': 1.8899755501222495e-05, 'epoch': 0.34}
  8%|▊         | 71/844 [16:16<2:55:57, 13.66s/it]  9%|▊         | 72/844 [16:30<2:56:19, 13.70s/it]                                                  {'loss': 0.9474, 'learning_rate': 1.8875305623471885e-05, 'epoch': 0.34}
  9%|▊         | 72/844 [16:30<2:56:19, 13.70s/it]  9%|▊         | 73/844 [16:44<2:56:25, 13.73s/it]                                                  {'loss': 0.9897, 'learning_rate': 1.8850855745721274e-05, 'epoch': 0.35}
  9%|▊         | 73/844 [16:44<2:56:25, 13.73s/it]  9%|▉         | 74/844 [16:58<2:55:57, 13.71s/it]                                                  {'loss': 0.8594, 'learning_rate': 1.882640586797066e-05, 'epoch': 0.35}
  9%|▉         | 74/844 [16:58<2:55:57, 13.71s/it]  9%|▉         | 75/844 [17:11<2:54:39, 13.63s/it]                                                  {'loss': 0.9371, 'learning_rate': 1.880195599022005e-05, 'epoch': 0.35}
  9%|▉         | 75/844 [17:11<2:54:39, 13.63s/it]  9%|▉         | 76/844 [17:25<2:55:42, 13.73s/it]                                                  {'loss': 0.9229, 'learning_rate': 1.877750611246944e-05, 'epoch': 0.36}
  9%|▉         | 76/844 [17:25<2:55:42, 13.73s/it]  9%|▉         | 77/844 [17:38<2:54:49, 13.68s/it]                                                  {'loss': 0.8657, 'learning_rate': 1.8753056234718828e-05, 'epoch': 0.36}
  9%|▉         | 77/844 [17:38<2:54:49, 13.68s/it]  9%|▉         | 78/844 [17:52<2:54:21, 13.66s/it]                                                  {'loss': 0.8595, 'learning_rate': 1.8728606356968217e-05, 'epoch': 0.37}
  9%|▉         | 78/844 [17:52<2:54:21, 13.66s/it]  9%|▉         | 79/844 [18:06<2:55:19, 13.75s/it]                                                  {'loss': 0.7998, 'learning_rate': 1.8704156479217607e-05, 'epoch': 0.37}
  9%|▉         | 79/844 [18:06<2:55:19, 13.75s/it]  9%|▉         | 80/844 [18:20<2:54:55, 13.74s/it]                                                  {'loss': 0.921, 'learning_rate': 1.8679706601466993e-05, 'epoch': 0.38}
  9%|▉         | 80/844 [18:20<2:54:55, 13.74s/it] 10%|▉         | 81/844 [18:33<2:53:42, 13.66s/it]                                                  {'loss': 0.8526, 'learning_rate': 1.8655256723716385e-05, 'epoch': 0.38}
 10%|▉         | 81/844 [18:33<2:53:42, 13.66s/it] 10%|▉         | 82/844 [18:46<2:51:28, 13.50s/it]                                                  {'loss': 0.9128, 'learning_rate': 1.863080684596577e-05, 'epoch': 0.39}
 10%|▉         | 82/844 [18:46<2:51:28, 13.50s/it] 10%|▉         | 83/844 [19:00<2:50:54, 13.48s/it]                                                  {'loss': 0.9633, 'learning_rate': 1.860635696821516e-05, 'epoch': 0.39}
 10%|▉         | 83/844 [19:00<2:50:54, 13.48s/it] 10%|▉         | 84/844 [19:15<2:56:02, 13.90s/it]                                                  {'loss': 0.8204, 'learning_rate': 1.8581907090464547e-05, 'epoch': 0.4}
 10%|▉         | 84/844 [19:15<2:56:02, 13.90s/it] 10%|█         | 85/844 [19:28<2:55:28, 13.87s/it]                                                  {'loss': 0.8409, 'learning_rate': 1.855745721271394e-05, 'epoch': 0.4}
 10%|█         | 85/844 [19:28<2:55:28, 13.87s/it] 10%|█         | 86/844 [19:43<2:56:30, 13.97s/it]                                                  {'loss': 0.9922, 'learning_rate': 1.8533007334963325e-05, 'epoch': 0.41}
 10%|█         | 86/844 [19:43<2:56:30, 13.97s/it] 10%|█         | 87/844 [19:56<2:55:03, 13.88s/it]                                                  {'loss': 0.7543, 'learning_rate': 1.8508557457212715e-05, 'epoch': 0.41}
 10%|█         | 87/844 [19:56<2:55:03, 13.88s/it] 10%|█         | 88/844 [20:10<2:52:51, 13.72s/it]                                                  {'loss': 1.0092, 'learning_rate': 1.8484107579462104e-05, 'epoch': 0.42}
 10%|█         | 88/844 [20:10<2:52:51, 13.72s/it] 11%|█         | 89/844 [20:23<2:52:31, 13.71s/it]                                                  {'loss': 0.8361, 'learning_rate': 1.8459657701711494e-05, 'epoch': 0.42}
 11%|█         | 89/844 [20:23<2:52:31, 13.71s/it] 11%|█         | 90/844 [20:37<2:50:40, 13.58s/it]                                                  {'loss': 0.8236, 'learning_rate': 1.843520782396088e-05, 'epoch': 0.43}
 11%|█         | 90/844 [20:37<2:50:40, 13.58s/it] 11%|█         | 91/844 [20:50<2:50:58, 13.62s/it]                                                  {'loss': 0.9008, 'learning_rate': 1.8410757946210272e-05, 'epoch': 0.43}
 11%|█         | 91/844 [20:50<2:50:58, 13.62s/it] 11%|█         | 92/844 [21:03<2:48:16, 13.43s/it]                                                  {'loss': 0.7448, 'learning_rate': 1.8386308068459658e-05, 'epoch': 0.44}
 11%|█         | 92/844 [21:03<2:48:16, 13.43s/it] 11%|█         | 93/844 [21:17<2:48:51, 13.49s/it]                                                  {'loss': 0.8954, 'learning_rate': 1.8361858190709048e-05, 'epoch': 0.44}
 11%|█         | 93/844 [21:17<2:48:51, 13.49s/it] 11%|█         | 94/844 [21:31<2:50:52, 13.67s/it]                                                  {'loss': 0.7843, 'learning_rate': 1.8337408312958437e-05, 'epoch': 0.44}
 11%|█         | 94/844 [21:31<2:50:52, 13.67s/it] 11%|█▏        | 95/844 [21:45<2:50:23, 13.65s/it]                                                  {'loss': 0.8151, 'learning_rate': 1.8312958435207826e-05, 'epoch': 0.45}
 11%|█▏        | 95/844 [21:45<2:50:23, 13.65s/it] 11%|█▏        | 96/844 [21:59<2:51:14, 13.74s/it]                                                  {'loss': 0.9459, 'learning_rate': 1.8288508557457216e-05, 'epoch': 0.45}
 11%|█▏        | 96/844 [21:59<2:51:14, 13.74s/it] 11%|█▏        | 97/844 [22:12<2:51:28, 13.77s/it]                                                  {'loss': 0.9336, 'learning_rate': 1.82640586797066e-05, 'epoch': 0.46}
 11%|█▏        | 97/844 [22:12<2:51:28, 13.77s/it] 12%|█▏        | 98/844 [22:26<2:51:15, 13.77s/it]                                                  {'loss': 0.9034, 'learning_rate': 1.823960880195599e-05, 'epoch': 0.46}
 12%|█▏        | 98/844 [22:26<2:51:15, 13.77s/it] 12%|█▏        | 99/844 [22:40<2:49:33, 13.66s/it]                                                  {'loss': 0.8871, 'learning_rate': 1.821515892420538e-05, 'epoch': 0.47}
 12%|█▏        | 99/844 [22:40<2:49:33, 13.66s/it] 12%|█▏        | 100/844 [22:53<2:49:42, 13.69s/it]                                                   {'loss': 0.8208, 'learning_rate': 1.819070904645477e-05, 'epoch': 0.47}
 12%|█▏        | 100/844 [22:53<2:49:42, 13.69s/it] 12%|█▏        | 101/844 [23:07<2:49:33, 13.69s/it]                                                   {'loss': 0.8094, 'learning_rate': 1.816625916870416e-05, 'epoch': 0.48}
 12%|█▏        | 101/844 [23:07<2:49:33, 13.69s/it] 12%|█▏        | 102/844 [23:21<2:48:54, 13.66s/it]                                                   {'loss': 0.7934, 'learning_rate': 1.814180929095355e-05, 'epoch': 0.48}
 12%|█▏        | 102/844 [23:21<2:48:54, 13.66s/it] 12%|█▏        | 103/844 [23:34<2:48:51, 13.67s/it]                                                   {'loss': 0.8809, 'learning_rate': 1.8117359413202934e-05, 'epoch': 0.49}
 12%|█▏        | 103/844 [23:34<2:48:51, 13.67s/it] 12%|█▏        | 104/844 [23:48<2:48:45, 13.68s/it]                                                   {'loss': 0.8038, 'learning_rate': 1.8092909535452324e-05, 'epoch': 0.49}
 12%|█▏        | 104/844 [23:48<2:48:45, 13.68s/it] 12%|█▏        | 105/844 [24:02<2:48:56, 13.72s/it]                                                   {'loss': 0.8957, 'learning_rate': 1.8068459657701713e-05, 'epoch': 0.5}
 12%|█▏        | 105/844 [24:02<2:48:56, 13.72s/it] 13%|█▎        | 106/844 [24:16<2:49:28, 13.78s/it]                                                   {'loss': 0.9031, 'learning_rate': 1.8044009779951102e-05, 'epoch': 0.5}
 13%|█▎        | 106/844 [24:16<2:49:28, 13.78s/it] 13%|█▎        | 107/844 [24:30<2:50:33, 13.89s/it]                                                   {'loss': 0.6358, 'learning_rate': 1.8019559902200488e-05, 'epoch': 0.51}
 13%|█▎        | 107/844 [24:30<2:50:33, 13.89s/it] 13%|█▎        | 108/844 [24:44<2:49:19, 13.80s/it]                                                   {'loss': 0.9364, 'learning_rate': 1.799511002444988e-05, 'epoch': 0.51}
 13%|█▎        | 108/844 [24:44<2:49:19, 13.80s/it] 13%|█▎        | 109/844 [24:57<2:49:06, 13.80s/it]                                                   {'loss': 0.8065, 'learning_rate': 1.7970660146699267e-05, 'epoch': 0.52}
 13%|█▎        | 109/844 [24:57<2:49:06, 13.80s/it] 13%|█▎        | 110/844 [25:11<2:49:23, 13.85s/it]                                                   {'loss': 0.8637, 'learning_rate': 1.7946210268948656e-05, 'epoch': 0.52}
 13%|█▎        | 110/844 [25:11<2:49:23, 13.85s/it] 13%|█▎        | 111/844 [25:24<2:46:31, 13.63s/it]                                                   {'loss': 0.7564, 'learning_rate': 1.7921760391198046e-05, 'epoch': 0.52}
 13%|█▎        | 111/844 [25:24<2:46:31, 13.63s/it] 13%|█▎        | 112/844 [25:38<2:46:51, 13.68s/it]                                                   {'loss': 0.9362, 'learning_rate': 1.7897310513447435e-05, 'epoch': 0.53}
 13%|█▎        | 112/844 [25:38<2:46:51, 13.68s/it] 13%|█▎        | 113/844 [25:52<2:48:13, 13.81s/it]                                                   {'loss': 0.7841, 'learning_rate': 1.787286063569682e-05, 'epoch': 0.53}
 13%|█▎        | 113/844 [25:52<2:48:13, 13.81s/it] 14%|█▎        | 114/844 [26:06<2:48:13, 13.83s/it]                                                   {'loss': 0.9764, 'learning_rate': 1.784841075794621e-05, 'epoch': 0.54}
 14%|█▎        | 114/844 [26:06<2:48:13, 13.83s/it] 14%|█▎        | 115/844 [26:20<2:46:41, 13.72s/it]                                                   {'loss': 0.863, 'learning_rate': 1.78239608801956e-05, 'epoch': 0.54}
 14%|█▎        | 115/844 [26:20<2:46:41, 13.72s/it] 14%|█▎        | 116/844 [26:33<2:44:59, 13.60s/it]                                                   {'loss': 0.9417, 'learning_rate': 1.779951100244499e-05, 'epoch': 0.55}
 14%|█▎        | 116/844 [26:33<2:44:59, 13.60s/it] 14%|█▍        | 117/844 [26:47<2:46:22, 13.73s/it]                                                   {'loss': 0.9289, 'learning_rate': 1.777506112469438e-05, 'epoch': 0.55}
 14%|█▍        | 117/844 [26:47<2:46:22, 13.73s/it] 14%|█▍        | 118/844 [27:01<2:45:42, 13.69s/it]                                                   {'loss': 0.7552, 'learning_rate': 1.7750611246943768e-05, 'epoch': 0.56}
 14%|█▍        | 118/844 [27:01<2:45:42, 13.69s/it] 14%|█▍        | 119/844 [27:14<2:45:18, 13.68s/it]                                                   {'loss': 0.6969, 'learning_rate': 1.7726161369193157e-05, 'epoch': 0.56}
 14%|█▍        | 119/844 [27:14<2:45:18, 13.68s/it] 14%|█▍        | 120/844 [27:28<2:43:36, 13.56s/it]                                                   {'loss': 0.8135, 'learning_rate': 1.7701711491442543e-05, 'epoch': 0.57}
 14%|█▍        | 120/844 [27:28<2:43:36, 13.56s/it] 14%|█▍        | 121/844 [27:42<2:45:13, 13.71s/it]                                                   {'loss': 0.8382, 'learning_rate': 1.7677261613691932e-05, 'epoch': 0.57}
 14%|█▍        | 121/844 [27:42<2:45:13, 13.71s/it] 14%|█▍        | 122/844 [27:56<2:45:59, 13.79s/it]                                                   {'loss': 0.7527, 'learning_rate': 1.7652811735941322e-05, 'epoch': 0.58}
 14%|█▍        | 122/844 [27:56<2:45:59, 13.79s/it] 15%|█▍        | 123/844 [28:09<2:44:49, 13.72s/it]                                                   {'loss': 0.7743, 'learning_rate': 1.762836185819071e-05, 'epoch': 0.58}
 15%|█▍        | 123/844 [28:09<2:44:49, 13.72s/it] 15%|█▍        | 124/844 [28:23<2:44:50, 13.74s/it]                                                   {'loss': 0.8795, 'learning_rate': 1.7603911980440097e-05, 'epoch': 0.59}
 15%|█▍        | 124/844 [28:23<2:44:50, 13.74s/it] 15%|█▍        | 125/844 [28:37<2:44:08, 13.70s/it]                                                   {'loss': 0.8734, 'learning_rate': 1.757946210268949e-05, 'epoch': 0.59}
 15%|█▍        | 125/844 [28:37<2:44:08, 13.70s/it] 15%|█▍        | 126/844 [28:50<2:44:31, 13.75s/it]                                                   {'loss': 0.8152, 'learning_rate': 1.7555012224938876e-05, 'epoch': 0.6}
 15%|█▍        | 126/844 [28:50<2:44:31, 13.75s/it] 15%|█▌        | 127/844 [29:04<2:43:44, 13.70s/it]                                                   {'loss': 0.8692, 'learning_rate': 1.7530562347188265e-05, 'epoch': 0.6}
 15%|█▌        | 127/844 [29:04<2:43:44, 13.70s/it] 15%|█▌        | 128/844 [29:18<2:45:22, 13.86s/it]                                                   {'loss': 0.742, 'learning_rate': 1.7506112469437655e-05, 'epoch': 0.61}
 15%|█▌        | 128/844 [29:18<2:45:22, 13.86s/it] 15%|█▌        | 129/844 [29:32<2:44:30, 13.81s/it]                                                   {'loss': 0.769, 'learning_rate': 1.7481662591687044e-05, 'epoch': 0.61}
 15%|█▌        | 129/844 [29:32<2:44:30, 13.81s/it] 15%|█▌        | 130/844 [29:46<2:43:43, 13.76s/it]                                                   {'loss': 0.8871, 'learning_rate': 1.745721271393643e-05, 'epoch': 0.61}
 15%|█▌        | 130/844 [29:46<2:43:43, 13.76s/it] 16%|█▌        | 131/844 [29:59<2:43:04, 13.72s/it]                                                   {'loss': 0.854, 'learning_rate': 1.7432762836185823e-05, 'epoch': 0.62}
 16%|█▌        | 131/844 [29:59<2:43:04, 13.72s/it] 16%|█▌        | 132/844 [30:14<2:45:05, 13.91s/it]                                                   {'loss': 0.8991, 'learning_rate': 1.740831295843521e-05, 'epoch': 0.62}
 16%|█▌        | 132/844 [30:14<2:45:05, 13.91s/it] 16%|█▌        | 133/844 [30:27<2:43:10, 13.77s/it]                                                   {'loss': 0.9538, 'learning_rate': 1.7383863080684598e-05, 'epoch': 0.63}
 16%|█▌        | 133/844 [30:27<2:43:10, 13.77s/it] 16%|█▌        | 134/844 [30:40<2:41:34, 13.65s/it]                                                   {'loss': 0.7775, 'learning_rate': 1.7359413202933987e-05, 'epoch': 0.63}
 16%|█▌        | 134/844 [30:40<2:41:34, 13.65s/it] 16%|█▌        | 135/844 [30:55<2:43:08, 13.81s/it]                                                   {'loss': 0.9023, 'learning_rate': 1.7334963325183377e-05, 'epoch': 0.64}
 16%|█▌        | 135/844 [30:55<2:43:08, 13.81s/it] 16%|█▌        | 136/844 [31:08<2:43:06, 13.82s/it]                                                   {'loss': 0.741, 'learning_rate': 1.7310513447432763e-05, 'epoch': 0.64}
 16%|█▌        | 136/844 [31:08<2:43:06, 13.82s/it] 16%|█▌        | 137/844 [31:22<2:42:14, 13.77s/it]                                                   {'loss': 0.9291, 'learning_rate': 1.7286063569682152e-05, 'epoch': 0.65}
 16%|█▌        | 137/844 [31:22<2:42:14, 13.77s/it] 16%|█▋        | 138/844 [31:36<2:41:27, 13.72s/it]                                                   {'loss': 0.8678, 'learning_rate': 1.726161369193154e-05, 'epoch': 0.65}
 16%|█▋        | 138/844 [31:36<2:41:27, 13.72s/it] 16%|█▋        | 139/844 [31:50<2:43:00, 13.87s/it]                                                   {'loss': 0.8517, 'learning_rate': 1.723716381418093e-05, 'epoch': 0.66}
 16%|█▋        | 139/844 [31:50<2:43:00, 13.87s/it] 17%|█▋        | 140/844 [32:04<2:43:28, 13.93s/it]                                                   {'loss': 0.7503, 'learning_rate': 1.721271393643032e-05, 'epoch': 0.66}
 17%|█▋        | 140/844 [32:04<2:43:28, 13.93s/it] 17%|█▋        | 141/844 [32:18<2:43:50, 13.98s/it]                                                   {'loss': 0.7505, 'learning_rate': 1.718826405867971e-05, 'epoch': 0.67}
 17%|█▋        | 141/844 [32:18<2:43:50, 13.98s/it] 17%|█▋        | 142/844 [32:31<2:41:17, 13.79s/it]                                                   {'loss': 0.8256, 'learning_rate': 1.71638141809291e-05, 'epoch': 0.67}
 17%|█▋        | 142/844 [32:31<2:41:17, 13.79s/it] 17%|█▋        | 143/844 [32:45<2:41:50, 13.85s/it]                                                   {'loss': 0.7536, 'learning_rate': 1.7139364303178485e-05, 'epoch': 0.68}
 17%|█▋        | 143/844 [32:45<2:41:50, 13.85s/it] 17%|█▋        | 144/844 [32:59<2:41:28, 13.84s/it]                                                   {'loss': 0.7764, 'learning_rate': 1.7114914425427874e-05, 'epoch': 0.68}
 17%|█▋        | 144/844 [32:59<2:41:28, 13.84s/it] 17%|█▋        | 145/844 [33:13<2:40:52, 13.81s/it]                                                   {'loss': 0.8468, 'learning_rate': 1.7090464547677263e-05, 'epoch': 0.69}
 17%|█▋        | 145/844 [33:13<2:40:52, 13.81s/it] 17%|█▋        | 146/844 [33:26<2:39:43, 13.73s/it]                                                   {'loss': 0.9312, 'learning_rate': 1.7066014669926653e-05, 'epoch': 0.69}
 17%|█▋        | 146/844 [33:26<2:39:43, 13.73s/it] 17%|█▋        | 147/844 [33:40<2:40:12, 13.79s/it]                                                   {'loss': 0.7751, 'learning_rate': 1.704156479217604e-05, 'epoch': 0.7}
 17%|█▋        | 147/844 [33:40<2:40:12, 13.79s/it] 18%|█▊        | 148/844 [33:55<2:41:13, 13.90s/it]                                                   {'loss': 0.6584, 'learning_rate': 1.701711491442543e-05, 'epoch': 0.7}
 18%|█▊        | 148/844 [33:55<2:41:13, 13.90s/it] 18%|█▊        | 149/844 [34:08<2:40:54, 13.89s/it]                                                   {'loss': 0.7891, 'learning_rate': 1.6992665036674817e-05, 'epoch': 0.7}
 18%|█▊        | 149/844 [34:08<2:40:54, 13.89s/it] 18%|█▊        | 150/844 [34:22<2:40:26, 13.87s/it]                                                   {'loss': 0.7186, 'learning_rate': 1.6968215158924207e-05, 'epoch': 0.71}
 18%|█▊        | 150/844 [34:22<2:40:26, 13.87s/it] 18%|█▊        | 151/844 [34:36<2:40:56, 13.93s/it]                                                   {'loss': 0.7986, 'learning_rate': 1.6943765281173596e-05, 'epoch': 0.71}
 18%|█▊        | 151/844 [34:36<2:40:56, 13.93s/it] 18%|█▊        | 152/844 [34:50<2:40:13, 13.89s/it]                                                   {'loss': 0.7654, 'learning_rate': 1.6919315403422985e-05, 'epoch': 0.72}
 18%|█▊        | 152/844 [34:50<2:40:13, 13.89s/it] 18%|█▊        | 153/844 [35:04<2:39:52, 13.88s/it]                                                   {'loss': 0.7108, 'learning_rate': 1.689486552567237e-05, 'epoch': 0.72}
 18%|█▊        | 153/844 [35:04<2:39:52, 13.88s/it] 18%|█▊        | 154/844 [35:18<2:38:40, 13.80s/it]                                                   {'loss': 0.8487, 'learning_rate': 1.687041564792176e-05, 'epoch': 0.73}
 18%|█▊        | 154/844 [35:18<2:38:40, 13.80s/it] 18%|█▊        | 155/844 [35:31<2:38:48, 13.83s/it]                                                   {'loss': 0.7846, 'learning_rate': 1.684596577017115e-05, 'epoch': 0.73}
 18%|█▊        | 155/844 [35:31<2:38:48, 13.83s/it] 18%|█▊        | 156/844 [35:45<2:37:49, 13.76s/it]                                                   {'loss': 0.7357, 'learning_rate': 1.682151589242054e-05, 'epoch': 0.74}
 18%|█▊        | 156/844 [35:45<2:37:49, 13.76s/it] 19%|█▊        | 157/844 [35:59<2:37:19, 13.74s/it]                                                   {'loss': 0.8508, 'learning_rate': 1.679706601466993e-05, 'epoch': 0.74}
 19%|█▊        | 157/844 [35:59<2:37:19, 13.74s/it] 19%|█▊        | 158/844 [36:12<2:36:20, 13.67s/it]                                                   {'loss': 0.8775, 'learning_rate': 1.6772616136919318e-05, 'epoch': 0.75}
 19%|█▊        | 158/844 [36:12<2:36:20, 13.67s/it] 19%|█▉        | 159/844 [36:26<2:36:27, 13.70s/it]                                                   {'loss': 0.804, 'learning_rate': 1.6748166259168704e-05, 'epoch': 0.75}
 19%|█▉        | 159/844 [36:26<2:36:27, 13.70s/it] 19%|█▉        | 160/844 [36:40<2:36:45, 13.75s/it]                                                   {'loss': 0.7092, 'learning_rate': 1.6723716381418093e-05, 'epoch': 0.76}
 19%|█▉        | 160/844 [36:40<2:36:45, 13.75s/it] 19%|█▉        | 161/844 [36:54<2:38:09, 13.89s/it]                                                   {'loss': 0.6991, 'learning_rate': 1.6699266503667483e-05, 'epoch': 0.76}
 19%|█▉        | 161/844 [36:54<2:38:09, 13.89s/it] 19%|█▉        | 162/844 [37:08<2:36:42, 13.79s/it]                                                   {'loss': 0.7886, 'learning_rate': 1.6674816625916872e-05, 'epoch': 0.77}
 19%|█▉        | 162/844 [37:08<2:36:42, 13.79s/it] 19%|█▉        | 163/844 [37:22<2:37:02, 13.84s/it]                                                   {'loss': 0.9232, 'learning_rate': 1.665036674816626e-05, 'epoch': 0.77}
 19%|█▉        | 163/844 [37:22<2:37:02, 13.84s/it] 19%|█▉        | 164/844 [37:35<2:36:44, 13.83s/it]                                                   {'loss': 0.8714, 'learning_rate': 1.662591687041565e-05, 'epoch': 0.78}
 19%|█▉        | 164/844 [37:35<2:36:44, 13.83s/it] 20%|█▉        | 165/844 [37:49<2:36:12, 13.80s/it]                                                   {'loss': 0.8442, 'learning_rate': 1.6601466992665037e-05, 'epoch': 0.78}
 20%|█▉        | 165/844 [37:49<2:36:12, 13.80s/it] 20%|█▉        | 166/844 [38:03<2:36:06, 13.81s/it]                                                   {'loss': 0.8088, 'learning_rate': 1.6577017114914426e-05, 'epoch': 0.78}
 20%|█▉        | 166/844 [38:03<2:36:06, 13.81s/it] 20%|█▉        | 167/844 [38:17<2:35:58, 13.82s/it]                                                   {'loss': 0.8956, 'learning_rate': 1.6552567237163816e-05, 'epoch': 0.79}
 20%|█▉        | 167/844 [38:17<2:35:58, 13.82s/it] 20%|█▉        | 168/844 [38:31<2:35:18, 13.78s/it]                                                   {'loss': 0.8142, 'learning_rate': 1.6528117359413205e-05, 'epoch': 0.79}
 20%|█▉        | 168/844 [38:31<2:35:18, 13.78s/it] 20%|██        | 169/844 [38:44<2:34:49, 13.76s/it]                                                   {'loss': 0.8729, 'learning_rate': 1.6503667481662594e-05, 'epoch': 0.8}
 20%|██        | 169/844 [38:44<2:34:49, 13.76s/it] 20%|██        | 170/844 [38:58<2:34:50, 13.78s/it]                                                   {'loss': 0.7016, 'learning_rate': 1.647921760391198e-05, 'epoch': 0.8}
 20%|██        | 170/844 [38:58<2:34:50, 13.78s/it] 20%|██        | 171/844 [39:12<2:34:32, 13.78s/it]                                                   {'loss': 0.8052, 'learning_rate': 1.6454767726161373e-05, 'epoch': 0.81}
 20%|██        | 171/844 [39:12<2:34:32, 13.78s/it] 20%|██        | 172/844 [39:26<2:34:20, 13.78s/it]                                                   {'loss': 0.867, 'learning_rate': 1.643031784841076e-05, 'epoch': 0.81}
 20%|██        | 172/844 [39:26<2:34:20, 13.78s/it] 20%|██        | 173/844 [39:40<2:34:16, 13.80s/it]                                                   {'loss': 0.8028, 'learning_rate': 1.6405867970660148e-05, 'epoch': 0.82}
 20%|██        | 173/844 [39:40<2:34:16, 13.80s/it] 21%|██        | 174/844 [39:53<2:32:59, 13.70s/it]                                                   {'loss': 0.8022, 'learning_rate': 1.6381418092909538e-05, 'epoch': 0.82}
 21%|██        | 174/844 [39:53<2:32:59, 13.70s/it] 21%|██        | 175/844 [40:07<2:32:39, 13.69s/it]                                                   {'loss': 0.7497, 'learning_rate': 1.6356968215158927e-05, 'epoch': 0.83}
 21%|██        | 175/844 [40:07<2:32:39, 13.69s/it] 21%|██        | 176/844 [40:21<2:33:16, 13.77s/it]                                                   {'loss': 0.718, 'learning_rate': 1.6332518337408313e-05, 'epoch': 0.83}
 21%|██        | 176/844 [40:21<2:33:16, 13.77s/it] 21%|██        | 177/844 [40:34<2:33:23, 13.80s/it]                                                   {'loss': 0.8747, 'learning_rate': 1.6308068459657702e-05, 'epoch': 0.84}
 21%|██        | 177/844 [40:34<2:33:23, 13.80s/it] 21%|██        | 178/844 [40:48<2:32:31, 13.74s/it]                                                   {'loss': 0.8163, 'learning_rate': 1.628361858190709e-05, 'epoch': 0.84}
 21%|██        | 178/844 [40:48<2:32:31, 13.74s/it] 21%|██        | 179/844 [41:02<2:32:13, 13.73s/it]                                                   {'loss': 0.8465, 'learning_rate': 1.625916870415648e-05, 'epoch': 0.85}
 21%|██        | 179/844 [41:02<2:32:13, 13.73s/it] 21%|██▏       | 180/844 [41:15<2:31:42, 13.71s/it]                                                   {'loss': 0.7486, 'learning_rate': 1.6234718826405867e-05, 'epoch': 0.85}
 21%|██▏       | 180/844 [41:15<2:31:42, 13.71s/it] 21%|██▏       | 181/844 [41:29<2:31:21, 13.70s/it]                                                   {'loss': 0.7754, 'learning_rate': 1.621026894865526e-05, 'epoch': 0.86}
 21%|██▏       | 181/844 [41:29<2:31:21, 13.70s/it] 22%|██▏       | 182/844 [41:42<2:30:04, 13.60s/it]                                                   {'loss': 0.7717, 'learning_rate': 1.6185819070904646e-05, 'epoch': 0.86}
 22%|██▏       | 182/844 [41:42<2:30:04, 13.60s/it] 22%|██▏       | 183/844 [41:56<2:30:21, 13.65s/it]                                                   {'loss': 0.6652, 'learning_rate': 1.6161369193154035e-05, 'epoch': 0.87}
 22%|██▏       | 183/844 [41:56<2:30:21, 13.65s/it] 22%|██▏       | 184/844 [42:10<2:29:19, 13.58s/it]                                                   {'loss': 0.8126, 'learning_rate': 1.6136919315403424e-05, 'epoch': 0.87}
 22%|██▏       | 184/844 [42:10<2:29:19, 13.58s/it] 22%|██▏       | 185/844 [42:23<2:29:27, 13.61s/it]                                                   {'loss': 0.9084, 'learning_rate': 1.6112469437652814e-05, 'epoch': 0.87}
 22%|██▏       | 185/844 [42:23<2:29:27, 13.61s/it] 22%|██▏       | 186/844 [42:37<2:30:59, 13.77s/it]                                                   {'loss': 0.7098, 'learning_rate': 1.6088019559902203e-05, 'epoch': 0.88}
 22%|██▏       | 186/844 [42:37<2:30:59, 13.77s/it] 22%|██▏       | 187/844 [42:51<2:30:42, 13.76s/it]                                                   {'loss': 0.8456, 'learning_rate': 1.606356968215159e-05, 'epoch': 0.88}
 22%|██▏       | 187/844 [42:51<2:30:42, 13.76s/it] 22%|██▏       | 188/844 [43:05<2:31:09, 13.82s/it]                                                   {'loss': 0.8824, 'learning_rate': 1.603911980440098e-05, 'epoch': 0.89}
 22%|██▏       | 188/844 [43:05<2:31:09, 13.82s/it] 22%|██▏       | 189/844 [43:19<2:30:18, 13.77s/it]                                                   {'loss': 0.7518, 'learning_rate': 1.6014669926650368e-05, 'epoch': 0.89}
 22%|██▏       | 189/844 [43:19<2:30:18, 13.77s/it] 23%|██▎       | 190/844 [43:32<2:29:29, 13.71s/it]                                                   {'loss': 0.769, 'learning_rate': 1.5990220048899757e-05, 'epoch': 0.9}
 23%|██▎       | 190/844 [43:32<2:29:29, 13.71s/it] 23%|██▎       | 191/844 [43:46<2:28:43, 13.67s/it]                                                   {'loss': 0.8519, 'learning_rate': 1.5965770171149146e-05, 'epoch': 0.9}
 23%|██▎       | 191/844 [43:46<2:28:43, 13.67s/it] 23%|██▎       | 192/844 [44:00<2:28:19, 13.65s/it]                                                   {'loss': 0.7651, 'learning_rate': 1.5941320293398536e-05, 'epoch': 0.91}
 23%|██▎       | 192/844 [44:00<2:28:19, 13.65s/it] 23%|██▎       | 193/844 [44:14<2:30:40, 13.89s/it]                                                   {'loss': 0.6368, 'learning_rate': 1.5916870415647922e-05, 'epoch': 0.91}
 23%|██▎       | 193/844 [44:14<2:30:40, 13.89s/it] 23%|██▎       | 194/844 [44:28<2:29:35, 13.81s/it]                                                   {'loss': 0.9857, 'learning_rate': 1.5892420537897314e-05, 'epoch': 0.92}
 23%|██▎       | 194/844 [44:28<2:29:35, 13.81s/it] 23%|██▎       | 195/844 [44:42<2:29:47, 13.85s/it]                                                   {'loss': 0.7843, 'learning_rate': 1.58679706601467e-05, 'epoch': 0.92}
 23%|██▎       | 195/844 [44:42<2:29:47, 13.85s/it] 23%|██▎       | 196/844 [44:56<2:29:54, 13.88s/it]                                                   {'loss': 0.896, 'learning_rate': 1.584352078239609e-05, 'epoch': 0.93}
 23%|██▎       | 196/844 [44:56<2:29:54, 13.88s/it] 23%|██▎       | 197/844 [45:10<2:29:54, 13.90s/it]                                                   {'loss': 0.7579, 'learning_rate': 1.5819070904645476e-05, 'epoch': 0.93}
 23%|██▎       | 197/844 [45:10<2:29:54, 13.90s/it] 23%|██▎       | 198/844 [45:23<2:29:51, 13.92s/it]                                                   {'loss': 0.7731, 'learning_rate': 1.579462102689487e-05, 'epoch': 0.94}
 23%|██▎       | 198/844 [45:23<2:29:51, 13.92s/it] 24%|██▎       | 199/844 [45:37<2:27:39, 13.74s/it]                                                   {'loss': 0.8118, 'learning_rate': 1.5770171149144254e-05, 'epoch': 0.94}
 24%|██▎       | 199/844 [45:37<2:27:39, 13.74s/it] 24%|██▎       | 200/844 [45:50<2:26:33, 13.65s/it]                                                   {'loss': 0.7636, 'learning_rate': 1.5745721271393644e-05, 'epoch': 0.95}
 24%|██▎       | 200/844 [45:50<2:26:33, 13.65s/it] 24%|██▍       | 201/844 [46:04<2:27:04, 13.72s/it]                                                   {'loss': 0.7314, 'learning_rate': 1.5721271393643033e-05, 'epoch': 0.95}
 24%|██▍       | 201/844 [46:04<2:27:04, 13.72s/it] 24%|██▍       | 202/844 [46:18<2:26:25, 13.68s/it]                                                   {'loss': 0.7993, 'learning_rate': 1.5696821515892422e-05, 'epoch': 0.96}
 24%|██▍       | 202/844 [46:18<2:26:25, 13.68s/it] 24%|██▍       | 203/844 [46:31<2:25:48, 13.65s/it]                                                   {'loss': 0.8408, 'learning_rate': 1.567237163814181e-05, 'epoch': 0.96}
 24%|██▍       | 203/844 [46:31<2:25:48, 13.65s/it] 24%|██▍       | 204/844 [46:45<2:25:23, 13.63s/it]                                                   {'loss': 0.7755, 'learning_rate': 1.56479217603912e-05, 'epoch': 0.96}
 24%|██▍       | 204/844 [46:45<2:25:23, 13.63s/it] 24%|██▍       | 205/844 [46:59<2:25:40, 13.68s/it]                                                   {'loss': 0.7498, 'learning_rate': 1.5623471882640587e-05, 'epoch': 0.97}
 24%|██▍       | 205/844 [46:59<2:25:40, 13.68s/it] 24%|██▍       | 206/844 [47:12<2:25:32, 13.69s/it]                                                   {'loss': 0.7526, 'learning_rate': 1.5599022004889977e-05, 'epoch': 0.97}
 24%|██▍       | 206/844 [47:12<2:25:32, 13.69s/it] 25%|██▍       | 207/844 [47:26<2:24:55, 13.65s/it]                                                   {'loss': 0.7309, 'learning_rate': 1.5574572127139366e-05, 'epoch': 0.98}
 25%|██▍       | 207/844 [47:26<2:24:55, 13.65s/it] 25%|██▍       | 208/844 [47:40<2:25:05, 13.69s/it]                                                   {'loss': 0.7477, 'learning_rate': 1.5550122249388755e-05, 'epoch': 0.98}
 25%|██▍       | 208/844 [47:40<2:25:05, 13.69s/it] 25%|██▍       | 209/844 [47:53<2:24:38, 13.67s/it]                                                   {'loss': 0.7901, 'learning_rate': 1.5525672371638145e-05, 'epoch': 0.99}
 25%|██▍       | 209/844 [47:53<2:24:38, 13.67s/it] 25%|██▍       | 210/844 [48:08<2:26:05, 13.83s/it]                                                   {'loss': 0.7436, 'learning_rate': 1.550122249388753e-05, 'epoch': 0.99}
 25%|██▍       | 210/844 [48:08<2:26:05, 13.83s/it] 25%|██▌       | 211/844 [48:21<2:24:36, 13.71s/it]                                                   {'loss': 0.7196, 'learning_rate': 1.547677261613692e-05, 'epoch': 1.0}
 25%|██▌       | 211/844 [48:21<2:24:36, 13.71s/it][2024-11-30 16:28:43,807] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-30 16:28:43,817] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|trainer.py:2889] 2024-11-30 16:29:01,420 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-211
[INFO|tokenization_utils_base.py:2432] 2024-11-30 16:29:01,980 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-211/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-30 16:29:01,989 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-211/special_tokens_map.json
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/gpfs/home1/scur2847/ir2-less-data/less/train/train.py", line 192, in <module>
    main()
  File "/gpfs/home1/scur2847/ir2-less-data/less/train/train.py", line 171, in main
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2279, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2359, in _save_checkpoint
    self._save_optimizer_and_scheduler(staging_output_dir)
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2474, in _save_optimizer_and_scheduler
    torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/serialization.py", line 492, in _open_zipfile_writer
    return container(name_or_buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/serialization.py", line 463, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Parent directory /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-211 does not exist.
[1;34mwandb[0m: 🚀 View run [33mfallen-resonance-91[0m at: [34mhttps://wandb.ai/colinnyuh-university-of-amsterdam/huggingface/runs/6wynsd12[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241130_154015-6wynsd12/logs[0m
[2024-11-30 16:29:46,638] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 202213 closing signal SIGTERM
[2024-11-30 16:29:47,102] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 202212) of binary: /sw/arch/RHEL8/EB_production/2023/software/Python/3.11.3-GCCcore-12.3.0/bin/python
Traceback (most recent call last):
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
less.train.train FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-30_16:29:46
  host      : gcn101.local.snellius.surf.nl
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 202212)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

JOB STATISTICS
==============
Job ID: 8863736
Cluster: snellius
User/Group: scur2847/scur2847
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 16
CPU Utilized: 01:44:28
CPU Efficiency: 13.01% of 13:22:40 core-walltime
Job Wall-clock time: 00:50:10
Memory Utilized: 51.26 GB
Memory Efficiency: 14.24% of 360.00 GB
