  0%|          | 0/44 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-27 12:59:43,062 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 25%|██▌       | 11/44 [02:24<07:11, 13.08s/it][INFO|trainer.py:2889] 2024-11-27 13:02:32,240 >> Saving model checkpoint to ../out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-11
{'loss': 1.8833, 'learning_rate': 1e-05, 'epoch': 0.09}
{'loss': 2.143, 'learning_rate': 2e-05, 'epoch': 0.17}
{'loss': 1.9038, 'learning_rate': 1.9523809523809524e-05, 'epoch': 0.26}
{'loss': 1.8976, 'learning_rate': 1.904761904761905e-05, 'epoch': 0.34}
{'loss': 1.7964, 'learning_rate': 1.8571428571428575e-05, 'epoch': 0.43}
{'loss': 1.6915, 'learning_rate': 1.8095238095238097e-05, 'epoch': 0.51}
{'loss': 1.6827, 'learning_rate': 1.761904761904762e-05, 'epoch': 0.6}
{'loss': 1.6902, 'learning_rate': 1.7142857142857142e-05, 'epoch': 0.68}
{'loss': 1.641, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.77}
{'loss': 1.5275, 'learning_rate': 1.6190476190476193e-05, 'epoch': 0.85}
{'loss': 1.5339, 'learning_rate': 1.5714285714285715e-05, 'epoch': 0.94}
[2024-11-27 13:02:17,173] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|tokenization_utils_base.py:2432] 2024-11-27 13:02:33,073 >> tokenizer config file saved in ../out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-11/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 13:02:33,074 >> Special tokens file saved in ../out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-11/special_tokens_map.json
Traceback (most recent call last):
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/218: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/gpfs/home1/scur2847/ir2-less-data/less/train/train.py", line 192, in <module>
    main()
  File "/gpfs/home1/scur2847/ir2-less-data/less/train/train.py", line 171, in main
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2279, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2359, in _save_checkpoint
    self._save_optimizer_and_scheduler(staging_output_dir)
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2456, in _save_optimizer_and_scheduler
    save_fsdp_model(self.accelerator.state.fsdp_plugin, self.accelerator, self.model, output_dir)
  File "/home/scur2847/.local/lib/python3.11/site-packages/accelerate/utils/fsdp_utils.py", line 89, in save_fsdp_model
    torch.save(state_dict, output_model_file)
