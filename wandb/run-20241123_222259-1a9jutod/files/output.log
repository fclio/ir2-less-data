  0%|          | 0/4 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-23 22:23:00,227 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 25%|██▌       | 1/4 [00:04<00:12,  4.12s/it][INFO|trainer.py:2889] 2024-11-23 22:23:04,350 >> Saving model checkpoint to ../out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-1
{'loss': 3.1583, 'learning_rate': 2e-05, 'epoch': 1.0}
[INFO|tokenization_utils_base.py:2432] 2024-11-23 22:23:04,879 >> tokenizer config file saved in ../out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-23 22:23:04,880 >> Special tokens file saved in ../out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-1/special_tokens_map.json
 75%|███████▌  | 3/4 [00:09<00:02,  2.90s/it][INFO|trainer.py:2889] 2024-11-23 22:23:09,276 >> Saving model checkpoint to ../out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-3
{'loss': 0.3911, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.19}
{'loss': 2.516, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}
[INFO|tokenization_utils_base.py:2432] 2024-11-23 22:23:09,728 >> tokenizer config file saved in ../out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-23 22:23:09,730 >> Special tokens file saved in ../out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-3/special_tokens_map.json
100%|██████████| 4/4 [00:11<00:00,  2.73s/it][INFO|trainer.py:2889] 2024-11-23 22:23:11,738 >> Saving model checkpoint to ../out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-4
{'loss': 0.9582, 'learning_rate': 0.0, 'epoch': 2.37}
[INFO|tokenization_utils_base.py:2432] 2024-11-23 22:23:12,194 >> tokenizer config file saved in ../out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-23 22:23:12,195 >> Special tokens file saved in ../out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-4/special_tokens_map.json
[INFO|trainer.py:1947] 2024-11-23 22:23:12,840 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 4/4 [00:12<00:00,  3.15s/it]
{'train_runtime': 13.4021, 'train_samples_per_second': 8.058, 'train_steps_per_second': 0.298, 'train_loss': 1.7558990865945816, 'epoch': 2.37}
after training done!!!!
[INFO|trainer.py:2889] 2024-11-23 22:23:12,842 >> Saving model checkpoint to ../out/llama2-7b-p0.0001-lora-seed3
[INFO|tokenization_utils_base.py:2432] 2024-11-23 22:23:13,296 >> tokenizer config file saved in ../out/llama2-7b-p0.0001-lora-seed3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-23 22:23:13,296 >> Special tokens file saved in ../out/llama2-7b-p0.0001-lora-seed3/special_tokens_map.json
***** train metrics *****
  epoch                    =       2.37
  train_loss               =     1.7559
  train_runtime            = 0:00:13.40
  train_samples            =         27
  train_samples_per_second =      8.058
  train_steps_per_second   =      0.298
