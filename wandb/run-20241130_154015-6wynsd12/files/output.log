  0%|          | 0/844 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-30 15:40:16,275 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
                                                  
{'loss': 2.4764, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.0}
{'loss': 2.5226, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.01}
{'loss': 2.199, 'learning_rate': 2.307692307692308e-06, 'epoch': 0.01}
{'loss': 2.4162, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.02}
{'loss': 2.3803, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.02}
{'loss': 2.3865, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.03}
{'loss': 2.6114, 'learning_rate': 5.384615384615385e-06, 'epoch': 0.03}
{'loss': 2.8806, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.04}
{'loss': 3.0066, 'learning_rate': 6.923076923076923e-06, 'epoch': 0.04}
{'loss': 2.4412, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.05}
{'loss': 2.182, 'learning_rate': 8.461538461538462e-06, 'epoch': 0.05}
{'loss': 2.4832, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.06}
{'loss': 2.4849, 'learning_rate': 1e-05, 'epoch': 0.06}
{'loss': 2.4594, 'learning_rate': 1.076923076923077e-05, 'epoch': 0.07}
{'loss': 2.2613, 'learning_rate': 1.1538461538461538e-05, 'epoch': 0.07}
{'loss': 2.4901, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.08}
{'loss': 2.1597, 'learning_rate': 1.3076923076923078e-05, 'epoch': 0.08}
{'loss': 2.7891, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.09}
{'loss': 2.3439, 'learning_rate': 1.4615384615384615e-05, 'epoch': 0.09}
{'loss': 2.2096, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.09}
{'loss': 2.2376, 'learning_rate': 1.6153846153846154e-05, 'epoch': 0.1}
{'loss': 2.1869, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.1}
{'loss': 2.1959, 'learning_rate': 1.7692307692307694e-05, 'epoch': 0.11}
{'loss': 2.4922, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.11}
{'loss': 2.5106, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.12}
{'loss': 2.3473, 'learning_rate': 2e-05, 'epoch': 0.12}
{'loss': 2.2123, 'learning_rate': 1.997555012224939e-05, 'epoch': 0.13}
{'loss': 2.3666, 'learning_rate': 1.995110024449878e-05, 'epoch': 0.13}
{'loss': 2.1356, 'learning_rate': 1.992665036674817e-05, 'epoch': 0.14}
{'loss': 2.053, 'learning_rate': 1.9902200488997556e-05, 'epoch': 0.14}
{'loss': 2.3782, 'learning_rate': 1.9877750611246945e-05, 'epoch': 0.15}
{'loss': 2.268, 'learning_rate': 1.9853300733496334e-05, 'epoch': 0.15}
{'loss': 2.0918, 'learning_rate': 1.9828850855745724e-05, 'epoch': 0.16}
{'loss': 2.1157, 'learning_rate': 1.980440097799511e-05, 'epoch': 0.16}
{'loss': 2.2292, 'learning_rate': 1.9779951100244502e-05, 'epoch': 0.17}
{'loss': 1.7035, 'learning_rate': 1.975550122249389e-05, 'epoch': 0.17}
{'loss': 1.9529, 'learning_rate': 1.9731051344743278e-05, 'epoch': 0.17}
{'loss': 2.0806, 'learning_rate': 1.9706601466992667e-05, 'epoch': 0.18}
{'loss': 2.1443, 'learning_rate': 1.9682151589242056e-05, 'epoch': 0.18}
{'loss': 2.1338, 'learning_rate': 1.9657701711491442e-05, 'epoch': 0.19}
{'loss': 2.0432, 'learning_rate': 1.9633251833740835e-05, 'epoch': 0.19}
{'loss': 2.0778, 'learning_rate': 1.960880195599022e-05, 'epoch': 0.2}
{'loss': 1.9498, 'learning_rate': 1.958435207823961e-05, 'epoch': 0.2}
{'loss': 2.1293, 'learning_rate': 1.9559902200489e-05, 'epoch': 0.21}
{'loss': 2.0395, 'learning_rate': 1.953545232273839e-05, 'epoch': 0.21}
{'loss': 1.8693, 'learning_rate': 1.9511002444987775e-05, 'epoch': 0.22}
{'loss': 2.2897, 'learning_rate': 1.9486552567237164e-05, 'epoch': 0.22}
{'loss': 2.2415, 'learning_rate': 1.9462102689486554e-05, 'epoch': 0.23}
{'loss': 2.1184, 'learning_rate': 1.9437652811735943e-05, 'epoch': 0.23}
{'loss': 2.245, 'learning_rate': 1.9413202933985333e-05, 'epoch': 0.24}
{'loss': 2.1061, 'learning_rate': 1.9388753056234722e-05, 'epoch': 0.24}
{'loss': 1.7763, 'learning_rate': 1.936430317848411e-05, 'epoch': 0.25}
{'loss': 1.9987, 'learning_rate': 1.9339853300733497e-05, 'epoch': 0.25}
{'loss': 2.003, 'learning_rate': 1.9315403422982887e-05, 'epoch': 0.26}
{'loss': 1.8958, 'learning_rate': 1.9290953545232276e-05, 'epoch': 0.26}
{'loss': 1.7986, 'learning_rate': 1.9266503667481665e-05, 'epoch': 0.26}
{'loss': 1.8891, 'learning_rate': 1.924205378973105e-05, 'epoch': 0.27}
{'loss': 1.8146, 'learning_rate': 1.9217603911980444e-05, 'epoch': 0.27}
{'loss': 1.8426, 'learning_rate': 1.919315403422983e-05, 'epoch': 0.28}
{'loss': 1.5888, 'learning_rate': 1.916870415647922e-05, 'epoch': 0.28}
{'loss': 1.5072, 'learning_rate': 1.914425427872861e-05, 'epoch': 0.29}
{'loss': 1.5138, 'learning_rate': 1.9119804400977998e-05, 'epoch': 0.29}
{'loss': 1.2412, 'learning_rate': 1.9095354523227384e-05, 'epoch': 0.3}
{'loss': 1.2222, 'learning_rate': 1.9070904645476773e-05, 'epoch': 0.3}
{'loss': 1.1765, 'learning_rate': 1.9046454767726163e-05, 'epoch': 0.31}
{'loss': 1.1029, 'learning_rate': 1.9022004889975552e-05, 'epoch': 0.31}
{'loss': 1.1393, 'learning_rate': 1.899755501222494e-05, 'epoch': 0.32}
{'loss': 0.8616, 'learning_rate': 1.897310513447433e-05, 'epoch': 0.32}
{'loss': 0.928, 'learning_rate': 1.8948655256723717e-05, 'epoch': 0.33}
{'loss': 1.0226, 'learning_rate': 1.8924205378973106e-05, 'epoch': 0.33}
{'loss': 0.8965, 'learning_rate': 1.8899755501222495e-05, 'epoch': 0.34}
{'loss': 0.9474, 'learning_rate': 1.8875305623471885e-05, 'epoch': 0.34}
{'loss': 0.9897, 'learning_rate': 1.8850855745721274e-05, 'epoch': 0.35}
{'loss': 0.8594, 'learning_rate': 1.882640586797066e-05, 'epoch': 0.35}
{'loss': 0.9371, 'learning_rate': 1.880195599022005e-05, 'epoch': 0.35}
{'loss': 0.9229, 'learning_rate': 1.877750611246944e-05, 'epoch': 0.36}
{'loss': 0.8657, 'learning_rate': 1.8753056234718828e-05, 'epoch': 0.36}
{'loss': 0.8595, 'learning_rate': 1.8728606356968217e-05, 'epoch': 0.37}
{'loss': 0.7998, 'learning_rate': 1.8704156479217607e-05, 'epoch': 0.37}
{'loss': 0.921, 'learning_rate': 1.8679706601466993e-05, 'epoch': 0.38}
{'loss': 0.8526, 'learning_rate': 1.8655256723716385e-05, 'epoch': 0.38}
{'loss': 0.9128, 'learning_rate': 1.863080684596577e-05, 'epoch': 0.39}
{'loss': 0.9633, 'learning_rate': 1.860635696821516e-05, 'epoch': 0.39}
{'loss': 0.8204, 'learning_rate': 1.8581907090464547e-05, 'epoch': 0.4}
{'loss': 0.8409, 'learning_rate': 1.855745721271394e-05, 'epoch': 0.4}
{'loss': 0.9922, 'learning_rate': 1.8533007334963325e-05, 'epoch': 0.41}
{'loss': 0.7543, 'learning_rate': 1.8508557457212715e-05, 'epoch': 0.41}
{'loss': 1.0092, 'learning_rate': 1.8484107579462104e-05, 'epoch': 0.42}
{'loss': 0.8361, 'learning_rate': 1.8459657701711494e-05, 'epoch': 0.42}
{'loss': 0.8236, 'learning_rate': 1.843520782396088e-05, 'epoch': 0.43}
{'loss': 0.9008, 'learning_rate': 1.8410757946210272e-05, 'epoch': 0.43}
{'loss': 0.7448, 'learning_rate': 1.8386308068459658e-05, 'epoch': 0.44}
{'loss': 0.8954, 'learning_rate': 1.8361858190709048e-05, 'epoch': 0.44}
{'loss': 0.7843, 'learning_rate': 1.8337408312958437e-05, 'epoch': 0.44}
{'loss': 0.8151, 'learning_rate': 1.8312958435207826e-05, 'epoch': 0.45}
{'loss': 0.9459, 'learning_rate': 1.8288508557457216e-05, 'epoch': 0.45}
{'loss': 0.9336, 'learning_rate': 1.82640586797066e-05, 'epoch': 0.46}
{'loss': 0.9034, 'learning_rate': 1.823960880195599e-05, 'epoch': 0.46}
{'loss': 0.8871, 'learning_rate': 1.821515892420538e-05, 'epoch': 0.47}
{'loss': 0.8208, 'learning_rate': 1.819070904645477e-05, 'epoch': 0.47}
{'loss': 0.8094, 'learning_rate': 1.816625916870416e-05, 'epoch': 0.48}
{'loss': 0.7934, 'learning_rate': 1.814180929095355e-05, 'epoch': 0.48}
{'loss': 0.8809, 'learning_rate': 1.8117359413202934e-05, 'epoch': 0.49}
{'loss': 0.8038, 'learning_rate': 1.8092909535452324e-05, 'epoch': 0.49}
{'loss': 0.8957, 'learning_rate': 1.8068459657701713e-05, 'epoch': 0.5}
{'loss': 0.9031, 'learning_rate': 1.8044009779951102e-05, 'epoch': 0.5}
{'loss': 0.6358, 'learning_rate': 1.8019559902200488e-05, 'epoch': 0.51}
{'loss': 0.9364, 'learning_rate': 1.799511002444988e-05, 'epoch': 0.51}
{'loss': 0.8065, 'learning_rate': 1.7970660146699267e-05, 'epoch': 0.52}
{'loss': 0.8637, 'learning_rate': 1.7946210268948656e-05, 'epoch': 0.52}
{'loss': 0.7564, 'learning_rate': 1.7921760391198046e-05, 'epoch': 0.52}
{'loss': 0.9362, 'learning_rate': 1.7897310513447435e-05, 'epoch': 0.53}
{'loss': 0.7841, 'learning_rate': 1.787286063569682e-05, 'epoch': 0.53}
{'loss': 0.9764, 'learning_rate': 1.784841075794621e-05, 'epoch': 0.54}
{'loss': 0.863, 'learning_rate': 1.78239608801956e-05, 'epoch': 0.54}
{'loss': 0.9417, 'learning_rate': 1.779951100244499e-05, 'epoch': 0.55}
{'loss': 0.9289, 'learning_rate': 1.777506112469438e-05, 'epoch': 0.55}
{'loss': 0.7552, 'learning_rate': 1.7750611246943768e-05, 'epoch': 0.56}
{'loss': 0.6969, 'learning_rate': 1.7726161369193157e-05, 'epoch': 0.56}
{'loss': 0.8135, 'learning_rate': 1.7701711491442543e-05, 'epoch': 0.57}
{'loss': 0.8382, 'learning_rate': 1.7677261613691932e-05, 'epoch': 0.57}
{'loss': 0.7527, 'learning_rate': 1.7652811735941322e-05, 'epoch': 0.58}
{'loss': 0.7743, 'learning_rate': 1.762836185819071e-05, 'epoch': 0.58}
{'loss': 0.8795, 'learning_rate': 1.7603911980440097e-05, 'epoch': 0.59}
{'loss': 0.8734, 'learning_rate': 1.757946210268949e-05, 'epoch': 0.59}
{'loss': 0.8152, 'learning_rate': 1.7555012224938876e-05, 'epoch': 0.6}
{'loss': 0.8692, 'learning_rate': 1.7530562347188265e-05, 'epoch': 0.6}
{'loss': 0.742, 'learning_rate': 1.7506112469437655e-05, 'epoch': 0.61}
{'loss': 0.769, 'learning_rate': 1.7481662591687044e-05, 'epoch': 0.61}
{'loss': 0.8871, 'learning_rate': 1.745721271393643e-05, 'epoch': 0.61}
{'loss': 0.854, 'learning_rate': 1.7432762836185823e-05, 'epoch': 0.62}
{'loss': 0.8991, 'learning_rate': 1.740831295843521e-05, 'epoch': 0.62}
{'loss': 0.9538, 'learning_rate': 1.7383863080684598e-05, 'epoch': 0.63}
{'loss': 0.7775, 'learning_rate': 1.7359413202933987e-05, 'epoch': 0.63}
{'loss': 0.9023, 'learning_rate': 1.7334963325183377e-05, 'epoch': 0.64}
{'loss': 0.741, 'learning_rate': 1.7310513447432763e-05, 'epoch': 0.64}
{'loss': 0.9291, 'learning_rate': 1.7286063569682152e-05, 'epoch': 0.65}
{'loss': 0.8678, 'learning_rate': 1.726161369193154e-05, 'epoch': 0.65}
{'loss': 0.8517, 'learning_rate': 1.723716381418093e-05, 'epoch': 0.66}
{'loss': 0.7503, 'learning_rate': 1.721271393643032e-05, 'epoch': 0.66}
{'loss': 0.7505, 'learning_rate': 1.718826405867971e-05, 'epoch': 0.67}
{'loss': 0.8256, 'learning_rate': 1.71638141809291e-05, 'epoch': 0.67}
{'loss': 0.7536, 'learning_rate': 1.7139364303178485e-05, 'epoch': 0.68}
{'loss': 0.7764, 'learning_rate': 1.7114914425427874e-05, 'epoch': 0.68}
{'loss': 0.8468, 'learning_rate': 1.7090464547677263e-05, 'epoch': 0.69}
{'loss': 0.9312, 'learning_rate': 1.7066014669926653e-05, 'epoch': 0.69}
{'loss': 0.7751, 'learning_rate': 1.704156479217604e-05, 'epoch': 0.7}
{'loss': 0.6584, 'learning_rate': 1.701711491442543e-05, 'epoch': 0.7}
{'loss': 0.7891, 'learning_rate': 1.6992665036674817e-05, 'epoch': 0.7}
{'loss': 0.7186, 'learning_rate': 1.6968215158924207e-05, 'epoch': 0.71}
{'loss': 0.7986, 'learning_rate': 1.6943765281173596e-05, 'epoch': 0.71}
{'loss': 0.7654, 'learning_rate': 1.6919315403422985e-05, 'epoch': 0.72}
{'loss': 0.7108, 'learning_rate': 1.689486552567237e-05, 'epoch': 0.72}
{'loss': 0.8487, 'learning_rate': 1.687041564792176e-05, 'epoch': 0.73}
{'loss': 0.7846, 'learning_rate': 1.684596577017115e-05, 'epoch': 0.73}
{'loss': 0.7357, 'learning_rate': 1.682151589242054e-05, 'epoch': 0.74}
{'loss': 0.8508, 'learning_rate': 1.679706601466993e-05, 'epoch': 0.74}
{'loss': 0.8775, 'learning_rate': 1.6772616136919318e-05, 'epoch': 0.75}
{'loss': 0.804, 'learning_rate': 1.6748166259168704e-05, 'epoch': 0.75}
{'loss': 0.7092, 'learning_rate': 1.6723716381418093e-05, 'epoch': 0.76}
{'loss': 0.6991, 'learning_rate': 1.6699266503667483e-05, 'epoch': 0.76}
{'loss': 0.7886, 'learning_rate': 1.6674816625916872e-05, 'epoch': 0.77}
{'loss': 0.9232, 'learning_rate': 1.665036674816626e-05, 'epoch': 0.77}
{'loss': 0.8714, 'learning_rate': 1.662591687041565e-05, 'epoch': 0.78}
{'loss': 0.8442, 'learning_rate': 1.6601466992665037e-05, 'epoch': 0.78}
{'loss': 0.8088, 'learning_rate': 1.6577017114914426e-05, 'epoch': 0.78}
{'loss': 0.8956, 'learning_rate': 1.6552567237163816e-05, 'epoch': 0.79}
{'loss': 0.8142, 'learning_rate': 1.6528117359413205e-05, 'epoch': 0.79}
{'loss': 0.8729, 'learning_rate': 1.6503667481662594e-05, 'epoch': 0.8}
{'loss': 0.7016, 'learning_rate': 1.647921760391198e-05, 'epoch': 0.8}
{'loss': 0.8052, 'learning_rate': 1.6454767726161373e-05, 'epoch': 0.81}
{'loss': 0.867, 'learning_rate': 1.643031784841076e-05, 'epoch': 0.81}
{'loss': 0.8028, 'learning_rate': 1.6405867970660148e-05, 'epoch': 0.82}
{'loss': 0.8022, 'learning_rate': 1.6381418092909538e-05, 'epoch': 0.82}
{'loss': 0.7497, 'learning_rate': 1.6356968215158927e-05, 'epoch': 0.83}
{'loss': 0.718, 'learning_rate': 1.6332518337408313e-05, 'epoch': 0.83}
{'loss': 0.8747, 'learning_rate': 1.6308068459657702e-05, 'epoch': 0.84}
{'loss': 0.8163, 'learning_rate': 1.628361858190709e-05, 'epoch': 0.84}
{'loss': 0.8465, 'learning_rate': 1.625916870415648e-05, 'epoch': 0.85}
{'loss': 0.7486, 'learning_rate': 1.6234718826405867e-05, 'epoch': 0.85}
{'loss': 0.7754, 'learning_rate': 1.621026894865526e-05, 'epoch': 0.86}
{'loss': 0.7717, 'learning_rate': 1.6185819070904646e-05, 'epoch': 0.86}
{'loss': 0.6652, 'learning_rate': 1.6161369193154035e-05, 'epoch': 0.87}
{'loss': 0.8126, 'learning_rate': 1.6136919315403424e-05, 'epoch': 0.87}
{'loss': 0.9084, 'learning_rate': 1.6112469437652814e-05, 'epoch': 0.87}
{'loss': 0.7098, 'learning_rate': 1.6088019559902203e-05, 'epoch': 0.88}
{'loss': 0.8456, 'learning_rate': 1.606356968215159e-05, 'epoch': 0.88}
{'loss': 0.8824, 'learning_rate': 1.603911980440098e-05, 'epoch': 0.89}
{'loss': 0.7518, 'learning_rate': 1.6014669926650368e-05, 'epoch': 0.89}
{'loss': 0.769, 'learning_rate': 1.5990220048899757e-05, 'epoch': 0.9}
{'loss': 0.8519, 'learning_rate': 1.5965770171149146e-05, 'epoch': 0.9}
{'loss': 0.7651, 'learning_rate': 1.5941320293398536e-05, 'epoch': 0.91}
{'loss': 0.6368, 'learning_rate': 1.5916870415647922e-05, 'epoch': 0.91}
{'loss': 0.9857, 'learning_rate': 1.5892420537897314e-05, 'epoch': 0.92}
{'loss': 0.7843, 'learning_rate': 1.58679706601467e-05, 'epoch': 0.92}
{'loss': 0.896, 'learning_rate': 1.584352078239609e-05, 'epoch': 0.93}
{'loss': 0.7579, 'learning_rate': 1.5819070904645476e-05, 'epoch': 0.93}
{'loss': 0.7731, 'learning_rate': 1.579462102689487e-05, 'epoch': 0.94}
{'loss': 0.8118, 'learning_rate': 1.5770171149144254e-05, 'epoch': 0.94}
{'loss': 0.7636, 'learning_rate': 1.5745721271393644e-05, 'epoch': 0.95}
{'loss': 0.7314, 'learning_rate': 1.5721271393643033e-05, 'epoch': 0.95}
{'loss': 0.7993, 'learning_rate': 1.5696821515892422e-05, 'epoch': 0.96}
{'loss': 0.8408, 'learning_rate': 1.567237163814181e-05, 'epoch': 0.96}
{'loss': 0.7755, 'learning_rate': 1.56479217603912e-05, 'epoch': 0.96}
{'loss': 0.7498, 'learning_rate': 1.5623471882640587e-05, 'epoch': 0.97}
{'loss': 0.7526, 'learning_rate': 1.5599022004889977e-05, 'epoch': 0.97}
{'loss': 0.7309, 'learning_rate': 1.5574572127139366e-05, 'epoch': 0.98}
{'loss': 0.7477, 'learning_rate': 1.5550122249388755e-05, 'epoch': 0.98}
{'loss': 0.7901, 'learning_rate': 1.5525672371638145e-05, 'epoch': 0.99}
{'loss': 0.7436, 'learning_rate': 1.550122249388753e-05, 'epoch': 0.99}
{'loss': 0.7196, 'learning_rate': 1.547677261613692e-05, 'epoch': 1.0}
[2024-11-30 16:28:43,807] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|tokenization_utils_base.py:2432] 2024-11-30 16:29:01,980 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-211/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-30 16:29:01,989 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-211/special_tokens_map.json
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/gpfs/home1/scur2847/ir2-less-data/less/train/train.py", line 192, in <module>
    main()
  File "/gpfs/home1/scur2847/ir2-less-data/less/train/train.py", line 171, in main
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2279, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2359, in _save_checkpoint
    self._save_optimizer_and_scheduler(staging_output_dir)
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2474, in _save_optimizer_and_scheduler
    torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/serialization.py", line 492, in _open_zipfile_writer
    return container(name_or_buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/serialization.py", line 463, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Parent directory /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-211 does not exist.
