  0%|          | 0/4 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-20 20:16:09,126 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 25%|██▌       | 1/4 [00:02<00:07,  2.58s/it][INFO|trainer.py:2889] 2024-11-20 20:16:11,714 >> Saving model checkpoint to ../out/llama2-7b-less-p0.001-lora/tmp-checkpoint-1
{'loss': 3.3221, 'learning_rate': 2e-05, 'epoch': 1.0}
[INFO|tokenization_utils_base.py:2432] 2024-11-20 20:16:12,157 >> tokenizer config file saved in ../out/llama2-7b-less-p0.001-lora/tmp-checkpoint-1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-20 20:16:12,158 >> Special tokens file saved in ../out/llama2-7b-less-p0.001-lora/tmp-checkpoint-1/special_tokens_map.json
 50%|█████     | 2/4 [00:05<00:05,  2.83s/it][INFO|trainer.py:2889] 2024-11-20 20:16:14,717 >> Saving model checkpoint to ../out/llama2-7b-less-p0.001-lora/tmp-checkpoint-2
{'loss': 3.3221, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.0}
[INFO|tokenization_utils_base.py:2432] 2024-11-20 20:16:15,016 >> tokenizer config file saved in ../out/llama2-7b-less-p0.001-lora/tmp-checkpoint-2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-20 20:16:15,017 >> Special tokens file saved in ../out/llama2-7b-less-p0.001-lora/tmp-checkpoint-2/special_tokens_map.json
100%|██████████| 4/4 [00:08<00:00,  1.92s/it][INFO|trainer.py:2889] 2024-11-20 20:16:17,579 >> Saving model checkpoint to ../out/llama2-7b-less-p0.001-lora/tmp-checkpoint-4
{'loss': 0.2977, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.13}
{'loss': 1.8889, 'learning_rate': 0.0, 'epoch': 3.0}
[INFO|tokenization_utils_base.py:2432] 2024-11-20 20:16:17,939 >> tokenizer config file saved in ../out/llama2-7b-less-p0.001-lora/tmp-checkpoint-4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-20 20:16:17,940 >> Special tokens file saved in ../out/llama2-7b-less-p0.001-lora/tmp-checkpoint-4/special_tokens_map.json
[INFO|trainer.py:1947] 2024-11-20 20:16:18,559 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 4/4 [00:09<00:00,  2.36s/it]
{'train_runtime': 10.4944, 'train_samples_per_second': 5.717, 'train_steps_per_second': 0.381, 'train_loss': 2.2076833248138428, 'epoch': 3.0}
after training done!!!!
[INFO|trainer.py:2889] 2024-11-20 20:16:18,561 >> Saving model checkpoint to ../out/llama2-7b-less-p0.001-lora
[INFO|tokenization_utils_base.py:2432] 2024-11-20 20:16:18,921 >> tokenizer config file saved in ../out/llama2-7b-less-p0.001-lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-20 20:16:18,922 >> Special tokens file saved in ../out/llama2-7b-less-p0.001-lora/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     2.2077
  train_runtime            = 0:00:10.49
  train_samples            =         15
  train_samples_per_second =      5.717
  train_steps_per_second   =      0.381
