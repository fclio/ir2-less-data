  0%|          | 0/1688 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-28 13:52:15,692 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  2%|‚ñè         | 29/1688 [02:08<2:01:03,  4.38s/it]
{'loss': 3.0989, 'learning_rate': 3.921568627450981e-07, 'epoch': 0.0}
{'loss': 3.2977, 'learning_rate': 7.843137254901962e-07, 'epoch': 0.0}
{'loss': 3.66, 'learning_rate': 1.1764705882352942e-06, 'epoch': 0.01}
{'loss': 2.4116, 'learning_rate': 1.5686274509803923e-06, 'epoch': 0.01}
{'loss': 3.147, 'learning_rate': 1.96078431372549e-06, 'epoch': 0.01}
{'loss': 2.789, 'learning_rate': 2.3529411764705885e-06, 'epoch': 0.01}
{'loss': 2.6483, 'learning_rate': 2.7450980392156867e-06, 'epoch': 0.02}
{'loss': 2.2014, 'learning_rate': 3.1372549019607846e-06, 'epoch': 0.02}
{'loss': 2.6188, 'learning_rate': 3.529411764705883e-06, 'epoch': 0.02}
{'loss': 2.5954, 'learning_rate': 3.92156862745098e-06, 'epoch': 0.02}
{'loss': 2.5283, 'learning_rate': 4.313725490196079e-06, 'epoch': 0.03}
{'loss': 2.8802, 'learning_rate': 4.705882352941177e-06, 'epoch': 0.03}
{'loss': 3.002, 'learning_rate': 5.098039215686274e-06, 'epoch': 0.03}
{'loss': 2.2597, 'learning_rate': 5.4901960784313735e-06, 'epoch': 0.03}
{'loss': 2.4389, 'learning_rate': 5.882352941176471e-06, 'epoch': 0.04}
{'loss': 2.5368, 'learning_rate': 6.274509803921569e-06, 'epoch': 0.04}
{'loss': 2.5809, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.04}
{'loss': 2.0549, 'learning_rate': 7.058823529411766e-06, 'epoch': 0.04}
{'loss': 2.206, 'learning_rate': 7.450980392156863e-06, 'epoch': 0.04}
{'loss': 2.017, 'learning_rate': 7.84313725490196e-06, 'epoch': 0.05}
{'loss': 2.372, 'learning_rate': 8.23529411764706e-06, 'epoch': 0.05}
{'loss': 1.7495, 'learning_rate': 8.627450980392157e-06, 'epoch': 0.05}
{'loss': 1.6277, 'learning_rate': 9.019607843137256e-06, 'epoch': 0.05}
{'loss': 1.3603, 'learning_rate': 9.411764705882354e-06, 'epoch': 0.06}
{'loss': 1.3403, 'learning_rate': 9.803921568627451e-06, 'epoch': 0.06}
{'loss': 1.4321, 'learning_rate': 1.0196078431372549e-05, 'epoch': 0.06}
{'loss': 1.155, 'learning_rate': 1.0588235294117648e-05, 'epoch': 0.06}
{'loss': 1.229, 'learning_rate': 1.0980392156862747e-05, 'epoch': 0.07}
{'loss': 1.3386, 'learning_rate': 1.1372549019607844e-05, 'epoch': 0.07}
