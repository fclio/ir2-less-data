  0%|          | 0/92 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-26 19:47:53,512 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 25%|██▌       | 23/92 [01:34<04:44,  4.12s/it][WARNING|trainer.py:2348] 2024-11-26 19:49:30,173 >> Checkpoint destination directory ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-23 already exists and is non-empty.Saving will proceed but saved results may be invalid.
{'loss': 2.7183, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.04}
{'loss': 3.5622, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.09}
{'loss': 4.2726, 'learning_rate': 2e-05, 'epoch': 0.13}
{'loss': 3.1395, 'learning_rate': 1.9775280898876404e-05, 'epoch': 0.17}
{'loss': 2.5322, 'learning_rate': 1.955056179775281e-05, 'epoch': 0.21}
{'loss': 2.2066, 'learning_rate': 1.9325842696629215e-05, 'epoch': 0.26}
{'loss': 1.7032, 'learning_rate': 1.910112359550562e-05, 'epoch': 0.3}
{'loss': 1.7308, 'learning_rate': 1.8876404494382024e-05, 'epoch': 0.34}
{'loss': 1.4097, 'learning_rate': 1.8651685393258426e-05, 'epoch': 0.38}
{'loss': 1.1928, 'learning_rate': 1.8426966292134835e-05, 'epoch': 0.43}
{'loss': 0.9281, 'learning_rate': 1.8202247191011237e-05, 'epoch': 0.47}
{'loss': 0.9946, 'learning_rate': 1.7977528089887643e-05, 'epoch': 0.51}
{'loss': 1.0568, 'learning_rate': 1.7752808988764045e-05, 'epoch': 0.55}
{'loss': 0.9236, 'learning_rate': 1.752808988764045e-05, 'epoch': 0.6}
{'loss': 0.6766, 'learning_rate': 1.7303370786516857e-05, 'epoch': 0.64}
{'loss': 0.8292, 'learning_rate': 1.707865168539326e-05, 'epoch': 0.68}
{'loss': 0.8872, 'learning_rate': 1.6853932584269665e-05, 'epoch': 0.73}
{'loss': 0.807, 'learning_rate': 1.662921348314607e-05, 'epoch': 0.77}
{'loss': 1.0398, 'learning_rate': 1.6404494382022473e-05, 'epoch': 0.81}
{'loss': 0.7134, 'learning_rate': 1.617977528089888e-05, 'epoch': 0.85}
{'loss': 0.7945, 'learning_rate': 1.595505617977528e-05, 'epoch': 0.9}
{'loss': 0.7375, 'learning_rate': 1.5730337078651687e-05, 'epoch': 0.94}
{'loss': 0.81, 'learning_rate': 1.5505617977528093e-05, 'epoch': 0.98}
[INFO|trainer.py:2889] 2024-11-26 19:49:30,174 >> Saving model checkpoint to ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-23
[INFO|tokenization_utils_base.py:2432] 2024-11-26 19:49:30,370 >> tokenizer config file saved in ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-23/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-26 19:49:30,372 >> Special tokens file saved in ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-23/special_tokens_map.json
 50%|█████     | 46/92 [03:10<03:09,  4.11s/it][WARNING|trainer.py:2348] 2024-11-26 19:51:07,820 >> Checkpoint destination directory ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-46 already exists and is non-empty.Saving will proceed but saved results may be invalid.
{'loss': 0.8675, 'learning_rate': 1.5280898876404495e-05, 'epoch': 1.02}
{'loss': 0.838, 'learning_rate': 1.5056179775280899e-05, 'epoch': 1.07}
{'loss': 0.8117, 'learning_rate': 1.4831460674157305e-05, 'epoch': 1.11}
{'loss': 0.993, 'learning_rate': 1.4606741573033709e-05, 'epoch': 1.15}
{'loss': 0.7895, 'learning_rate': 1.4382022471910113e-05, 'epoch': 1.19}
{'loss': 0.5603, 'learning_rate': 1.4157303370786517e-05, 'epoch': 1.24}
{'loss': 0.8689, 'learning_rate': 1.3932584269662923e-05, 'epoch': 1.28}
{'loss': 0.9165, 'learning_rate': 1.3707865168539327e-05, 'epoch': 1.32}
{'loss': 0.9694, 'learning_rate': 1.348314606741573e-05, 'epoch': 1.37}
{'loss': 0.6969, 'learning_rate': 1.3258426966292135e-05, 'epoch': 1.41}
{'loss': 0.8049, 'learning_rate': 1.303370786516854e-05, 'epoch': 1.45}
{'loss': 0.8643, 'learning_rate': 1.2808988764044944e-05, 'epoch': 1.49}
{'loss': 0.8371, 'learning_rate': 1.2584269662921348e-05, 'epoch': 1.54}
{'loss': 0.7838, 'learning_rate': 1.2359550561797752e-05, 'epoch': 1.58}
{'loss': 0.8486, 'learning_rate': 1.213483146067416e-05, 'epoch': 1.62}
{'loss': 0.6727, 'learning_rate': 1.1910112359550562e-05, 'epoch': 1.66}
{'loss': 0.7383, 'learning_rate': 1.1685393258426966e-05, 'epoch': 1.71}
{'loss': 0.8152, 'learning_rate': 1.146067415730337e-05, 'epoch': 1.75}
{'loss': 0.8048, 'learning_rate': 1.1235955056179778e-05, 'epoch': 1.79}
{'loss': 0.6621, 'learning_rate': 1.101123595505618e-05, 'epoch': 1.83}
{'loss': 0.8234, 'learning_rate': 1.0786516853932584e-05, 'epoch': 1.88}
{'loss': 0.7604, 'learning_rate': 1.0561797752808988e-05, 'epoch': 1.92}
{'loss': 0.8484, 'learning_rate': 1.0337078651685396e-05, 'epoch': 1.96}
[INFO|trainer.py:2889] 2024-11-26 19:51:07,820 >> Saving model checkpoint to ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-46
[INFO|tokenization_utils_base.py:2432] 2024-11-26 19:51:08,013 >> tokenizer config file saved in ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-46/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-26 19:51:08,014 >> Special tokens file saved in ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-46/special_tokens_map.json
 76%|███████▌  | 70/92 [04:50<01:30,  4.13s/it][WARNING|trainer.py:2348] 2024-11-26 19:52:45,445 >> Checkpoint destination directory ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-70 already exists and is non-empty.Saving will proceed but saved results may be invalid.
{'loss': 0.7704, 'learning_rate': 1.01123595505618e-05, 'epoch': 2.01}
{'loss': 0.5616, 'learning_rate': 9.887640449438202e-06, 'epoch': 2.05}
{'loss': 0.689, 'learning_rate': 9.662921348314608e-06, 'epoch': 2.09}
{'loss': 0.5852, 'learning_rate': 9.438202247191012e-06, 'epoch': 2.13}
{'loss': 0.6427, 'learning_rate': 9.213483146067417e-06, 'epoch': 2.18}
{'loss': 0.6271, 'learning_rate': 8.988764044943822e-06, 'epoch': 2.22}
{'loss': 0.7774, 'learning_rate': 8.764044943820226e-06, 'epoch': 2.26}
{'loss': 0.8466, 'learning_rate': 8.53932584269663e-06, 'epoch': 2.3}
{'loss': 0.7849, 'learning_rate': 8.314606741573035e-06, 'epoch': 2.35}
{'loss': 0.5884, 'learning_rate': 8.08988764044944e-06, 'epoch': 2.39}
{'loss': 0.751, 'learning_rate': 7.865168539325843e-06, 'epoch': 2.43}
{'loss': 0.8874, 'learning_rate': 7.640449438202247e-06, 'epoch': 2.47}
{'loss': 0.7975, 'learning_rate': 7.415730337078652e-06, 'epoch': 2.52}
{'loss': 0.5892, 'learning_rate': 7.191011235955056e-06, 'epoch': 2.56}
{'loss': 0.7431, 'learning_rate': 6.966292134831461e-06, 'epoch': 2.6}
{'loss': 0.5777, 'learning_rate': 6.741573033707865e-06, 'epoch': 2.65}
{'loss': 0.7553, 'learning_rate': 6.51685393258427e-06, 'epoch': 2.69}
{'loss': 0.6382, 'learning_rate': 6.292134831460674e-06, 'epoch': 2.73}
{'loss': 0.7733, 'learning_rate': 6.06741573033708e-06, 'epoch': 2.77}
{'loss': 0.883, 'learning_rate': 5.842696629213483e-06, 'epoch': 2.82}
{'loss': 0.7597, 'learning_rate': 5.617977528089889e-06, 'epoch': 2.86}
{'loss': 0.8603, 'learning_rate': 5.393258426966292e-06, 'epoch': 2.9}
{'loss': 0.6753, 'learning_rate': 5.168539325842698e-06, 'epoch': 2.94}
{'loss': 0.6751, 'learning_rate': 4.943820224719101e-06, 'epoch': 2.99}
[INFO|trainer.py:2889] 2024-11-26 19:52:45,445 >> Saving model checkpoint to ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-70
[INFO|tokenization_utils_base.py:2432] 2024-11-26 19:52:45,646 >> tokenizer config file saved in ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-70/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-26 19:52:45,647 >> Special tokens file saved in ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-70/special_tokens_map.json
100%|██████████| 92/92 [06:21<00:00,  4.02s/it][WARNING|trainer.py:2348] 2024-11-26 19:54:14,951 >> Checkpoint destination directory ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-92 already exists and is non-empty.Saving will proceed but saved results may be invalid.
{'loss': 1.0765, 'learning_rate': 4.719101123595506e-06, 'epoch': 3.03}
{'loss': 0.5657, 'learning_rate': 4.494382022471911e-06, 'epoch': 3.07}
{'loss': 0.489, 'learning_rate': 4.269662921348315e-06, 'epoch': 3.11}
{'loss': 0.5395, 'learning_rate': 4.04494382022472e-06, 'epoch': 3.16}
{'loss': 0.5649, 'learning_rate': 3.820224719101124e-06, 'epoch': 3.2}
{'loss': 0.6187, 'learning_rate': 3.595505617977528e-06, 'epoch': 3.24}
{'loss': 0.6361, 'learning_rate': 3.3707865168539327e-06, 'epoch': 3.29}
{'loss': 0.7393, 'learning_rate': 3.146067415730337e-06, 'epoch': 3.33}
{'loss': 0.8337, 'learning_rate': 2.9213483146067416e-06, 'epoch': 3.37}
{'loss': 0.7486, 'learning_rate': 2.696629213483146e-06, 'epoch': 3.41}
{'loss': 0.7046, 'learning_rate': 2.4719101123595505e-06, 'epoch': 3.46}
{'loss': 0.602, 'learning_rate': 2.2471910112359554e-06, 'epoch': 3.5}
{'loss': 0.6827, 'learning_rate': 2.02247191011236e-06, 'epoch': 3.54}
{'loss': 0.8346, 'learning_rate': 1.797752808988764e-06, 'epoch': 3.58}
{'loss': 0.8495, 'learning_rate': 1.5730337078651686e-06, 'epoch': 3.63}
{'loss': 0.6781, 'learning_rate': 1.348314606741573e-06, 'epoch': 3.67}
{'loss': 0.7723, 'learning_rate': 1.1235955056179777e-06, 'epoch': 3.71}
{'loss': 0.6946, 'learning_rate': 8.98876404494382e-07, 'epoch': 3.75}
{'loss': 0.6462, 'learning_rate': 6.741573033707865e-07, 'epoch': 3.8}
{'loss': 0.6568, 'learning_rate': 4.49438202247191e-07, 'epoch': 3.84}
{'loss': 0.8033, 'learning_rate': 2.247191011235955e-07, 'epoch': 3.88}
{'loss': 0.6345, 'learning_rate': 0.0, 'epoch': 3.93}
[INFO|trainer.py:2889] 2024-11-26 19:54:14,951 >> Saving model checkpoint to ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-92
[INFO|tokenization_utils_base.py:2432] 2024-11-26 19:54:15,150 >> tokenizer config file saved in ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-92/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-26 19:54:15,151 >> Special tokens file saved in ../out/llama2-7b-less-p0.05-lora-seed4/checkpoint-92/special_tokens_map.json
[INFO|trainer.py:1947] 2024-11-26 19:54:15,817 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 92/92 [06:22<00:00,  4.16s/it]
{'train_runtime': 383.2265, 'train_samples_per_second': 7.828, 'train_steps_per_second': 0.24, 'train_loss': 0.9440443376484124, 'epoch': 3.93}
after training done!!!!
[INFO|trainer.py:2889] 2024-11-26 19:54:15,819 >> Saving model checkpoint to ../out/llama2-7b-less-p0.05-lora-seed4
[INFO|tokenization_utils_base.py:2432] 2024-11-26 19:54:16,017 >> tokenizer config file saved in ../out/llama2-7b-less-p0.05-lora-seed4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-26 19:54:16,019 >> Special tokens file saved in ../out/llama2-7b-less-p0.05-lora-seed4/special_tokens_map.json
***** train metrics *****
  epoch                    =       3.93
  train_loss               =      0.944
  train_runtime            = 0:06:23.22
  train_samples            =        750
  train_samples_per_second =      7.828
  train_steps_per_second   =       0.24
