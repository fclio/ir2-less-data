  0%|          | 0/4 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-23 13:06:13,305 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 25%|██▌       | 1/4 [00:07<00:23,  7.85s/it][INFO|trainer.py:2889] 2024-11-23 13:06:25,416 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-1
{'loss': 3.1633, 'learning_rate': 2e-05, 'epoch': 1.0}
[2024-11-23 13:06:21,272] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|tokenization_utils_base.py:2432] 2024-11-23 13:06:25,995 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-23 13:06:25,996 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-1/special_tokens_map.json
 75%|███████▌  | 3/4 [00:54<00:17, 17.54s/it][INFO|trainer.py:2889] 2024-11-23 13:07:08,151 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-3
{'loss': 0.3919, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.19}
{'loss': 2.5196, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}
[INFO|tokenization_utils_base.py:2432] 2024-11-23 13:07:08,721 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-23 13:07:08,722 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-3/special_tokens_map.json
100%|██████████| 4/4 [01:37<00:00, 27.28s/it][INFO|trainer.py:2889] 2024-11-23 13:07:50,370 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-4
{'loss': 0.9613, 'learning_rate': 0.0, 'epoch': 2.37}
[INFO|tokenization_utils_base.py:2432] 2024-11-23 13:07:50,888 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-23 13:07:50,889 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-4/special_tokens_map.json
[INFO|trainer.py:1947] 2024-11-23 13:08:29,498 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 4/4 [02:16<00:00, 34.05s/it]
{'train_runtime': 137.0277, 'train_samples_per_second': 0.788, 'train_steps_per_second': 0.029, 'train_loss': 1.7590190842747688, 'epoch': 2.37}
after training done!!!!
[INFO|trainer.py:2889] 2024-11-23 13:08:29,569 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3
[INFO|tokenization_utils_base.py:2432] 2024-11-23 13:08:30,128 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-23 13:08:30,128 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/special_tokens_map.json
***** train metrics *****
  epoch                    =       2.37
  train_loss               =      1.759
  train_runtime            = 0:02:17.02
  train_samples            =         27
  train_samples_per_second =      0.788
  train_steps_per_second   =      0.029
