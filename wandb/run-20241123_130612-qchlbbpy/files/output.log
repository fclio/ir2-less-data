  0%|          | 0/4 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-23 13:06:13,305 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 25%|██▌       | 1/4 [00:07<00:23,  7.85s/it][INFO|trainer.py:2889] 2024-11-23 13:06:25,416 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-1
{'loss': 3.1633, 'learning_rate': 2e-05, 'epoch': 1.0}
[2024-11-23 13:06:21,272] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|tokenization_utils_base.py:2432] 2024-11-23 13:06:25,995 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-23 13:06:25,996 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-1/special_tokens_map.json
 75%|███████▌  | 3/4 [00:54<00:17, 17.54s/it][INFO|trainer.py:2889] 2024-11-23 13:07:08,151 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-3
{'loss': 0.3919, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.19}
{'loss': 2.5196, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}
[INFO|tokenization_utils_base.py:2432] 2024-11-23 13:07:08,721 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-23 13:07:08,722 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-3/special_tokens_map.json
100%|██████████| 4/4 [01:37<00:00, 27.28s/it][INFO|trainer.py:2889] 2024-11-23 13:07:50,370 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-4
{'loss': 0.9613, 'learning_rate': 0.0, 'epoch': 2.37}
[INFO|tokenization_utils_base.py:2432] 2024-11-23 13:07:50,888 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-23 13:07:50,889 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-7b-p0.0001-lora-seed3/tmp-checkpoint-4/special_tokens_map.json
