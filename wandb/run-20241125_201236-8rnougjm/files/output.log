  0%|          | 0/92 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-25 20:12:37,686 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 25%|██▌       | 23/92 [01:44<05:15,  4.58s/it][INFO|trainer.py:2889] 2024-11-25 20:14:23,634 >> Saving model checkpoint to ../out/llama2-7b-less-p0.05-lora/tmp-checkpoint-23
{'loss': 6.3512, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.04}
{'loss': 6.4833, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.09}
{'loss': 3.6616, 'learning_rate': 2e-05, 'epoch': 0.13}
{'loss': 5.2797, 'learning_rate': 1.9775280898876404e-05, 'epoch': 0.17}
{'loss': 2.441, 'learning_rate': 1.955056179775281e-05, 'epoch': 0.21}
{'loss': 2.3644, 'learning_rate': 1.9325842696629215e-05, 'epoch': 0.26}
{'loss': 1.6612, 'learning_rate': 1.910112359550562e-05, 'epoch': 0.3}
{'loss': 2.3164, 'learning_rate': 1.8876404494382024e-05, 'epoch': 0.34}
{'loss': 1.7331, 'learning_rate': 1.8651685393258426e-05, 'epoch': 0.38}
{'loss': 4.2783, 'learning_rate': 1.8426966292134835e-05, 'epoch': 0.43}
{'loss': 4.9528, 'learning_rate': 1.8202247191011237e-05, 'epoch': 0.47}
{'loss': 0.8964, 'learning_rate': 1.7977528089887643e-05, 'epoch': 0.51}
{'loss': 1.343, 'learning_rate': 1.7752808988764045e-05, 'epoch': 0.55}
{'loss': 1.0277, 'learning_rate': 1.752808988764045e-05, 'epoch': 0.6}
{'loss': 2.1315, 'learning_rate': 1.7303370786516857e-05, 'epoch': 0.64}
{'loss': 2.2388, 'learning_rate': 1.707865168539326e-05, 'epoch': 0.68}
{'loss': 0.9384, 'learning_rate': 1.6853932584269665e-05, 'epoch': 0.73}
{'loss': 1.9265, 'learning_rate': 1.662921348314607e-05, 'epoch': 0.77}
{'loss': 2.1603, 'learning_rate': 1.6404494382022473e-05, 'epoch': 0.81}
{'loss': 1.1154, 'learning_rate': 1.617977528089888e-05, 'epoch': 0.85}
{'loss': 1.8494, 'learning_rate': 1.595505617977528e-05, 'epoch': 0.9}
{'loss': 2.612, 'learning_rate': 1.5730337078651687e-05, 'epoch': 0.94}
{'loss': 1.7956, 'learning_rate': 1.5505617977528093e-05, 'epoch': 0.98}
[INFO|tokenization_utils_base.py:2432] 2024-11-25 20:14:24,050 >> tokenizer config file saved in ../out/llama2-7b-less-p0.05-lora/tmp-checkpoint-23/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-25 20:14:24,050 >> Special tokens file saved in ../out/llama2-7b-less-p0.05-lora/tmp-checkpoint-23/special_tokens_map.json
 50%|█████     | 46/92 [03:29<03:24,  4.44s/it][INFO|trainer.py:2889] 2024-11-25 20:16:10,319 >> Saving model checkpoint to ../out/llama2-7b-less-p0.05-lora/tmp-checkpoint-46
{'loss': 1.215, 'learning_rate': 1.5280898876404495e-05, 'epoch': 1.02}
{'loss': 1.3018, 'learning_rate': 1.5056179775280899e-05, 'epoch': 1.07}
{'loss': 1.1339, 'learning_rate': 1.4831460674157305e-05, 'epoch': 1.11}
{'loss': 1.5773, 'learning_rate': 1.4606741573033709e-05, 'epoch': 1.15}
{'loss': 2.0141, 'learning_rate': 1.4382022471910113e-05, 'epoch': 1.19}
{'loss': 1.5332, 'learning_rate': 1.4157303370786517e-05, 'epoch': 1.24}
{'loss': 2.3302, 'learning_rate': 1.3932584269662923e-05, 'epoch': 1.28}
{'loss': 3.0978, 'learning_rate': 1.3707865168539327e-05, 'epoch': 1.32}
{'loss': 1.5916, 'learning_rate': 1.348314606741573e-05, 'epoch': 1.37}
{'loss': 0.5812, 'learning_rate': 1.3258426966292135e-05, 'epoch': 1.41}
{'loss': 2.0273, 'learning_rate': 1.303370786516854e-05, 'epoch': 1.45}
{'loss': 2.715, 'learning_rate': 1.2808988764044944e-05, 'epoch': 1.49}
{'loss': 1.9099, 'learning_rate': 1.2584269662921348e-05, 'epoch': 1.54}
{'loss': 1.052, 'learning_rate': 1.2359550561797752e-05, 'epoch': 1.58}
{'loss': 1.6221, 'learning_rate': 1.213483146067416e-05, 'epoch': 1.62}
{'loss': 2.3364, 'learning_rate': 1.1910112359550562e-05, 'epoch': 1.66}
{'loss': 1.8799, 'learning_rate': 1.1685393258426966e-05, 'epoch': 1.71}
{'loss': 1.0114, 'learning_rate': 1.146067415730337e-05, 'epoch': 1.75}
{'loss': 1.6262, 'learning_rate': 1.1235955056179778e-05, 'epoch': 1.79}
{'loss': 0.74, 'learning_rate': 1.101123595505618e-05, 'epoch': 1.83}
{'loss': 2.2664, 'learning_rate': 1.0786516853932584e-05, 'epoch': 1.88}
{'loss': 1.4447, 'learning_rate': 1.0561797752808988e-05, 'epoch': 1.92}
{'loss': 2.3423, 'learning_rate': 1.0337078651685396e-05, 'epoch': 1.96}
[INFO|tokenization_utils_base.py:2432] 2024-11-25 20:16:10,624 >> tokenizer config file saved in ../out/llama2-7b-less-p0.05-lora/tmp-checkpoint-46/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-25 20:16:10,625 >> Special tokens file saved in ../out/llama2-7b-less-p0.05-lora/tmp-checkpoint-46/special_tokens_map.json
 76%|███████▌  | 70/92 [05:17<01:40,  4.57s/it][INFO|trainer.py:2889] 2024-11-25 20:17:56,625 >> Saving model checkpoint to ../out/llama2-7b-less-p0.05-lora/tmp-checkpoint-70
{'loss': 0.7824, 'learning_rate': 1.01123595505618e-05, 'epoch': 2.01}
{'loss': 6.1659, 'learning_rate': 9.887640449438202e-06, 'epoch': 2.05}
{'loss': 1.4086, 'learning_rate': 9.662921348314608e-06, 'epoch': 2.09}
{'loss': 1.0524, 'learning_rate': 9.438202247191012e-06, 'epoch': 2.13}
{'loss': 0.684, 'learning_rate': 9.213483146067417e-06, 'epoch': 2.18}
{'loss': 0.922, 'learning_rate': 8.988764044943822e-06, 'epoch': 2.22}
{'loss': 0.9697, 'learning_rate': 8.764044943820226e-06, 'epoch': 2.26}
{'loss': 0.9976, 'learning_rate': 8.53932584269663e-06, 'epoch': 2.3}
{'loss': 1.9793, 'learning_rate': 8.314606741573035e-06, 'epoch': 2.35}
{'loss': 1.6841, 'learning_rate': 8.08988764044944e-06, 'epoch': 2.39}
{'loss': 1.6541, 'learning_rate': 7.865168539325843e-06, 'epoch': 2.43}
{'loss': 0.9733, 'learning_rate': 7.640449438202247e-06, 'epoch': 2.47}
{'loss': 0.6849, 'learning_rate': 7.415730337078652e-06, 'epoch': 2.52}
{'loss': 2.5693, 'learning_rate': 7.191011235955056e-06, 'epoch': 2.56}
{'loss': 0.9066, 'learning_rate': 6.966292134831461e-06, 'epoch': 2.6}
{'loss': 1.3354, 'learning_rate': 6.741573033707865e-06, 'epoch': 2.65}
{'loss': 1.1909, 'learning_rate': 6.51685393258427e-06, 'epoch': 2.69}
{'loss': 3.1492, 'learning_rate': 6.292134831460674e-06, 'epoch': 2.73}
{'loss': 9.4422, 'learning_rate': 6.06741573033708e-06, 'epoch': 2.77}
{'loss': 4.6136, 'learning_rate': 5.842696629213483e-06, 'epoch': 2.82}
{'loss': 1.551, 'learning_rate': 5.617977528089889e-06, 'epoch': 2.86}
{'loss': 1.0192, 'learning_rate': 5.393258426966292e-06, 'epoch': 2.9}
{'loss': 1.0428, 'learning_rate': 5.168539325842698e-06, 'epoch': 2.94}
{'loss': 1.97, 'learning_rate': 4.943820224719101e-06, 'epoch': 2.99}
[INFO|tokenization_utils_base.py:2432] 2024-11-25 20:17:56,929 >> tokenizer config file saved in ../out/llama2-7b-less-p0.05-lora/tmp-checkpoint-70/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-25 20:17:56,929 >> Special tokens file saved in ../out/llama2-7b-less-p0.05-lora/tmp-checkpoint-70/special_tokens_map.json
100%|██████████| 92/92 [06:57<00:00,  4.50s/it][INFO|trainer.py:2889] 2024-11-25 20:19:35,447 >> Saving model checkpoint to ../out/llama2-7b-less-p0.05-lora/tmp-checkpoint-92
{'loss': 0.9638, 'learning_rate': 4.719101123595506e-06, 'epoch': 3.03}
{'loss': 0.7684, 'learning_rate': 4.494382022471911e-06, 'epoch': 3.07}
{'loss': 1.9643, 'learning_rate': 4.269662921348315e-06, 'epoch': 3.11}
{'loss': 1.9775, 'learning_rate': 4.04494382022472e-06, 'epoch': 3.16}
{'loss': 6.5572, 'learning_rate': 3.820224719101124e-06, 'epoch': 3.2}
{'loss': 1.2881, 'learning_rate': 3.595505617977528e-06, 'epoch': 3.24}
{'loss': 0.8017, 'learning_rate': 3.3707865168539327e-06, 'epoch': 3.29}
{'loss': 0.9891, 'learning_rate': 3.146067415730337e-06, 'epoch': 3.33}
{'loss': 2.0693, 'learning_rate': 2.9213483146067416e-06, 'epoch': 3.37}
{'loss': 1.1053, 'learning_rate': 2.696629213483146e-06, 'epoch': 3.41}
{'loss': 1.0866, 'learning_rate': 2.4719101123595505e-06, 'epoch': 3.46}
{'loss': 1.3002, 'learning_rate': 2.2471910112359554e-06, 'epoch': 3.5}
{'loss': 0.8538, 'learning_rate': 2.02247191011236e-06, 'epoch': 3.54}
{'loss': 1.2897, 'learning_rate': 1.797752808988764e-06, 'epoch': 3.58}
{'loss': 0.6408, 'learning_rate': 1.5730337078651686e-06, 'epoch': 3.63}
{'loss': 0.9139, 'learning_rate': 1.348314606741573e-06, 'epoch': 3.67}
{'loss': 1.578, 'learning_rate': 1.1235955056179777e-06, 'epoch': 3.71}
{'loss': 1.2342, 'learning_rate': 8.98876404494382e-07, 'epoch': 3.75}
{'loss': 1.0597, 'learning_rate': 6.741573033707865e-07, 'epoch': 3.8}
{'loss': 2.3544, 'learning_rate': 4.49438202247191e-07, 'epoch': 3.84}
{'loss': 2.0744, 'learning_rate': 2.247191011235955e-07, 'epoch': 3.88}
{'loss': 2.3402, 'learning_rate': 0.0, 'epoch': 3.93}
[INFO|tokenization_utils_base.py:2432] 2024-11-25 20:19:35,745 >> tokenizer config file saved in ../out/llama2-7b-less-p0.05-lora/tmp-checkpoint-92/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-25 20:19:35,746 >> Special tokens file saved in ../out/llama2-7b-less-p0.05-lora/tmp-checkpoint-92/special_tokens_map.json
[INFO|trainer.py:1947] 2024-11-25 20:19:36,393 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 92/92 [06:58<00:00,  4.55s/it]
{'train_runtime': 419.5846, 'train_samples_per_second': 7.15, 'train_steps_per_second': 0.219, 'train_loss': 2.009422830913378, 'epoch': 3.93}
after training done!!!!
[INFO|trainer.py:2889] 2024-11-25 20:19:36,395 >> Saving model checkpoint to ../out/llama2-7b-less-p0.05-lora
[INFO|tokenization_utils_base.py:2432] 2024-11-25 20:19:36,579 >> tokenizer config file saved in ../out/llama2-7b-less-p0.05-lora/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-25 20:19:36,580 >> Special tokens file saved in ../out/llama2-7b-less-p0.05-lora/special_tokens_map.json
***** train metrics *****
  epoch                    =       3.93
  train_loss               =     2.0094
  train_runtime            = 0:06:59.58
  train_samples            =        750
  train_samples_per_second =       7.15
  train_steps_per_second   =      0.219
