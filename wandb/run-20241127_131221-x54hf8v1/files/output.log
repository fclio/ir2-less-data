  0%|          | 0/44 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-27 13:12:22,303 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 25%|██▌       | 11/44 [02:24<07:12, 13.10s/it][INFO|trainer.py:2889] 2024-11-27 13:15:09,552 >> Saving model checkpoint to ../out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-11
{'loss': 1.8833, 'learning_rate': 1e-05, 'epoch': 0.09}
{'loss': 2.143, 'learning_rate': 2e-05, 'epoch': 0.17}
{'loss': 1.904, 'learning_rate': 1.9523809523809524e-05, 'epoch': 0.26}
{'loss': 1.8979, 'learning_rate': 1.904761904761905e-05, 'epoch': 0.34}
{'loss': 1.7965, 'learning_rate': 1.8571428571428575e-05, 'epoch': 0.43}
{'loss': 1.6924, 'learning_rate': 1.8095238095238097e-05, 'epoch': 0.51}
{'loss': 1.6828, 'learning_rate': 1.761904761904762e-05, 'epoch': 0.6}
{'loss': 1.6897, 'learning_rate': 1.7142857142857142e-05, 'epoch': 0.68}
{'loss': 1.6406, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.77}
{'loss': 1.5281, 'learning_rate': 1.6190476190476193e-05, 'epoch': 0.85}
{'loss': 1.5342, 'learning_rate': 1.5714285714285715e-05, 'epoch': 0.94}
[2024-11-27 13:14:56,275] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|tokenization_utils_base.py:2432] 2024-11-27 13:15:10,104 >> tokenizer config file saved in ../out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-11/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 13:15:10,105 >> Special tokens file saved in ../out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-11/special_tokens_map.json
 52%|█████▏    | 23/44 [05:55<04:41, 13.40s/it][INFO|trainer.py:2889] 2024-11-27 13:18:33,424 >> Saving model checkpoint to ../out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-23
{'loss': 1.5607, 'learning_rate': 1.523809523809524e-05, 'epoch': 1.02}
{'loss': 1.5541, 'learning_rate': 1.4761904761904763e-05, 'epoch': 1.11}
{'loss': 1.4701, 'learning_rate': 1.4285714285714287e-05, 'epoch': 1.19}
{'loss': 1.5318, 'learning_rate': 1.3809523809523811e-05, 'epoch': 1.28}
{'loss': 1.5551, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.37}
{'loss': 1.6756, 'learning_rate': 1.2857142857142859e-05, 'epoch': 1.45}
{'loss': 1.6543, 'learning_rate': 1.2380952380952383e-05, 'epoch': 1.54}
{'loss': 1.6722, 'learning_rate': 1.1904761904761905e-05, 'epoch': 1.62}
{'loss': 1.4681, 'learning_rate': 1.1428571428571429e-05, 'epoch': 1.71}
{'loss': 1.5626, 'learning_rate': 1.0952380952380955e-05, 'epoch': 1.79}
{'loss': 1.5411, 'learning_rate': 1.0476190476190477e-05, 'epoch': 1.88}
{'loss': 1.5545, 'learning_rate': 1e-05, 'epoch': 1.96}
[INFO|tokenization_utils_base.py:2432] 2024-11-27 13:18:33,938 >> tokenizer config file saved in ../out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-23/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 13:18:33,939 >> Special tokens file saved in ../out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-23/special_tokens_map.json
 80%|███████▉  | 35/44 [09:21<01:59, 13.27s/it][INFO|trainer.py:2889] 2024-11-27 13:21:55,609 >> Saving model checkpoint to ../out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-35
{'loss': 1.6043, 'learning_rate': 9.523809523809525e-06, 'epoch': 2.05}
{'loss': 1.5484, 'learning_rate': 9.047619047619049e-06, 'epoch': 2.13}
{'loss': 1.5838, 'learning_rate': 8.571428571428571e-06, 'epoch': 2.22}
{'loss': 1.4264, 'learning_rate': 8.095238095238097e-06, 'epoch': 2.3}
{'loss': 1.3812, 'learning_rate': 7.61904761904762e-06, 'epoch': 2.39}
{'loss': 1.4637, 'learning_rate': 7.1428571428571436e-06, 'epoch': 2.47}
{'loss': 1.5648, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.56}
{'loss': 1.5783, 'learning_rate': 6.1904761904761914e-06, 'epoch': 2.65}
{'loss': 1.5747, 'learning_rate': 5.7142857142857145e-06, 'epoch': 2.73}
{'loss': 1.5433, 'learning_rate': 5.2380952380952384e-06, 'epoch': 2.82}
{'loss': 1.5285, 'learning_rate': 4.761904761904762e-06, 'epoch': 2.9}
{'loss': 1.4027, 'learning_rate': 4.2857142857142855e-06, 'epoch': 2.99}
[INFO|tokenization_utils_base.py:2432] 2024-11-27 13:21:56,167 >> tokenizer config file saved in ../out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-35/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 13:21:56,168 >> Special tokens file saved in ../out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-35/special_tokens_map.json
Traceback (most recent call last):
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/serialization.py", line 619, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/serialization.py", line 853, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/319: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/gpfs/home1/scur2847/ir2-less-data/less/train/train.py", line 192, in <module>
    main()
  File "/gpfs/home1/scur2847/ir2-less-data/less/train/train.py", line 171, in main
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 1929, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2279, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2359, in _save_checkpoint
    self._save_optimizer_and_scheduler(staging_output_dir)
  File "/home/scur2847/.local/lib/python3.11/site-packages/transformers/trainer.py", line 2456, in _save_optimizer_and_scheduler
    save_fsdp_model(self.accelerator.state.fsdp_plugin, self.accelerator, self.model, output_dir)
  File "/home/scur2847/.local/lib/python3.11/site-packages/accelerate/utils/fsdp_utils.py", line 89, in save_fsdp_model
    torch.save(state_dict, output_model_file)
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/serialization.py", line 618, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/serialization.py", line 466, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 18933036160 vs 18933036056
