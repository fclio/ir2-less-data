  0%|          | 0/32 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-28 17:31:14,601 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  9%|â–‰         | 3/32 [00:24<03:51,  7.99s/it]
{'loss': 3.5511, 'learning_rate': 2e-05, 'epoch': 0.12}
{'loss': 4.621, 'learning_rate': 1.935483870967742e-05, 'epoch': 0.24}
{'loss': 2.972, 'learning_rate': 1.870967741935484e-05, 'epoch': 0.36}
