  0%|          | 0/44 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-27 18:10:02,225 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 25%|██▌       | 11/44 [02:24<07:12, 13.10s/it][INFO|trainer.py:2889] 2024-11-27 18:12:50,788 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-11
{'loss': 1.8833, 'learning_rate': 1e-05, 'epoch': 0.09}
{'loss': 2.143, 'learning_rate': 2e-05, 'epoch': 0.17}
{'loss': 1.9042, 'learning_rate': 1.9523809523809524e-05, 'epoch': 0.26}
{'loss': 1.8996, 'learning_rate': 1.904761904761905e-05, 'epoch': 0.34}
{'loss': 1.7962, 'learning_rate': 1.8571428571428575e-05, 'epoch': 0.43}
{'loss': 1.6928, 'learning_rate': 1.8095238095238097e-05, 'epoch': 0.51}
{'loss': 1.6823, 'learning_rate': 1.761904761904762e-05, 'epoch': 0.6}
{'loss': 1.69, 'learning_rate': 1.7142857142857142e-05, 'epoch': 0.68}
{'loss': 1.6414, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.77}
{'loss': 1.528, 'learning_rate': 1.6190476190476193e-05, 'epoch': 0.85}
{'loss': 1.5334, 'learning_rate': 1.5714285714285715e-05, 'epoch': 0.94}
[2024-11-27 18:12:36,268] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|tokenization_utils_base.py:2432] 2024-11-27 18:12:51,372 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-11/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 18:12:51,372 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-11/special_tokens_map.json
 52%|█████▏    | 23/44 [06:28<04:44, 13.55s/it][INFO|trainer.py:2889] 2024-11-27 18:16:45,478 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-23
{'loss': 1.5609, 'learning_rate': 1.523809523809524e-05, 'epoch': 1.02}
{'loss': 1.5536, 'learning_rate': 1.4761904761904763e-05, 'epoch': 1.11}
{'loss': 1.4699, 'learning_rate': 1.4285714285714287e-05, 'epoch': 1.19}
{'loss': 1.5317, 'learning_rate': 1.3809523809523811e-05, 'epoch': 1.28}
{'loss': 1.5557, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.37}
{'loss': 1.6756, 'learning_rate': 1.2857142857142859e-05, 'epoch': 1.45}
{'loss': 1.6534, 'learning_rate': 1.2380952380952383e-05, 'epoch': 1.54}
{'loss': 1.6723, 'learning_rate': 1.1904761904761905e-05, 'epoch': 1.62}
{'loss': 1.4678, 'learning_rate': 1.1428571428571429e-05, 'epoch': 1.71}
{'loss': 1.5624, 'learning_rate': 1.0952380952380955e-05, 'epoch': 1.79}
{'loss': 1.5417, 'learning_rate': 1.0476190476190477e-05, 'epoch': 1.88}
{'loss': 1.5553, 'learning_rate': 1e-05, 'epoch': 1.96}
[INFO|tokenization_utils_base.py:2432] 2024-11-27 18:16:46,052 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-23/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 18:16:46,053 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-23/special_tokens_map.json
 80%|███████▉  | 35/44 [10:25<02:01, 13.53s/it][INFO|trainer.py:2889] 2024-11-27 18:20:39,786 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-35
{'loss': 1.6044, 'learning_rate': 9.523809523809525e-06, 'epoch': 2.05}
{'loss': 1.5476, 'learning_rate': 9.047619047619049e-06, 'epoch': 2.13}
{'loss': 1.5838, 'learning_rate': 8.571428571428571e-06, 'epoch': 2.22}
{'loss': 1.4267, 'learning_rate': 8.095238095238097e-06, 'epoch': 2.3}
{'loss': 1.3808, 'learning_rate': 7.61904761904762e-06, 'epoch': 2.39}
{'loss': 1.4637, 'learning_rate': 7.1428571428571436e-06, 'epoch': 2.47}
{'loss': 1.5656, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.56}
{'loss': 1.5781, 'learning_rate': 6.1904761904761914e-06, 'epoch': 2.65}
{'loss': 1.5741, 'learning_rate': 5.7142857142857145e-06, 'epoch': 2.73}
{'loss': 1.5434, 'learning_rate': 5.2380952380952384e-06, 'epoch': 2.82}
{'loss': 1.5292, 'learning_rate': 4.761904761904762e-06, 'epoch': 2.9}
{'loss': 1.4038, 'learning_rate': 4.2857142857142855e-06, 'epoch': 2.99}
[INFO|tokenization_utils_base.py:2432] 2024-11-27 18:20:40,663 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-35/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 18:20:40,663 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-35/special_tokens_map.json
100%|██████████| 44/44 [13:39<00:00, 14.45s/it][INFO|trainer.py:2889] 2024-11-27 18:23:51,720 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-44
{'loss': 1.6308, 'learning_rate': 3.80952380952381e-06, 'epoch': 3.07}
{'loss': 1.4931, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.16}
{'loss': 1.4258, 'learning_rate': 2.8571428571428573e-06, 'epoch': 3.24}
{'loss': 1.4524, 'learning_rate': 2.380952380952381e-06, 'epoch': 3.33}
{'loss': 1.465, 'learning_rate': 1.904761904761905e-06, 'epoch': 3.41}
{'loss': 1.3677, 'learning_rate': 1.4285714285714286e-06, 'epoch': 3.5}
{'loss': 1.5405, 'learning_rate': 9.523809523809525e-07, 'epoch': 3.58}
{'loss': 1.3623, 'learning_rate': 4.7619047619047623e-07, 'epoch': 3.67}
{'loss': 1.4704, 'learning_rate': 0.0, 'epoch': 3.75}
[INFO|tokenization_utils_base.py:2432] 2024-11-27 18:23:52,481 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-44/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 18:23:52,481 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tmp-checkpoint-44/special_tokens_map.json
[INFO|trainer.py:1947] 2024-11-27 18:24:58,104 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 44/44 [14:55<00:00, 20.36s/it]
{'train_runtime': 896.8944, 'train_samples_per_second': 3.345, 'train_steps_per_second': 0.049, 'train_loss': 1.581904492594979, 'epoch': 3.75}
after training done!!!!
[INFO|trainer.py:2889] 2024-11-27 18:25:07,622 >> Saving model checkpoint to /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4
[INFO|tokenization_utils_base.py:2432] 2024-11-27 18:25:08,205 >> tokenizer config file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-27 18:25:08,206 >> Special tokens file saved in /scratch-shared/ir2-less/out/llama2-13b-less-p0.05-lora-seed4/special_tokens_map.json
***** train metrics *****
  epoch                    =       3.75
  train_loss               =     1.5819
  train_runtime            = 0:14:56.89
  train_samples            =        750
  train_samples_per_second =      3.345
  train_steps_per_second   =      0.049
