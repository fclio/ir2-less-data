  0%|          | 0/32 [00:00<?, ?it/s][WARNING|logging.py:314] 2024-11-18 15:22:12,418 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 25%|██▌       | 8/32 [01:03<03:07,  7.83s/it][WARNING|trainer.py:2348] 2024-11-18 15:23:19,449 >> Checkpoint destination directory ../out/llama2-7b-p0.001-lora-seed3/checkpoint-8 already exists and is non-empty.Saving will proceed but saved results may be invalid.
{'loss': 3.5511, 'learning_rate': 2e-05, 'epoch': 0.12}
{'loss': 4.621, 'learning_rate': 1.935483870967742e-05, 'epoch': 0.24}
{'loss': 2.9731, 'learning_rate': 1.870967741935484e-05, 'epoch': 0.36}
{'loss': 3.3247, 'learning_rate': 1.806451612903226e-05, 'epoch': 0.47}
{'loss': 2.483, 'learning_rate': 1.741935483870968e-05, 'epoch': 0.59}
{'loss': 3.1775, 'learning_rate': 1.6774193548387098e-05, 'epoch': 0.71}
{'loss': 1.9437, 'learning_rate': 1.6129032258064517e-05, 'epoch': 0.83}
{'loss': 1.6411, 'learning_rate': 1.5483870967741936e-05, 'epoch': 0.95}
[INFO|trainer.py:2889] 2024-11-18 15:23:19,580 >> Saving model checkpoint to ../out/llama2-7b-p0.001-lora-seed3/checkpoint-8
[INFO|tokenization_utils_base.py:2432] 2024-11-18 15:23:19,933 >> tokenizer config file saved in ../out/llama2-7b-p0.001-lora-seed3/checkpoint-8/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-18 15:23:19,937 >> Special tokens file saved in ../out/llama2-7b-p0.001-lora-seed3/checkpoint-8/special_tokens_map.json
 50%|█████     | 16/32 [02:21<02:11,  8.20s/it][WARNING|trainer.py:2348] 2024-11-18 15:24:40,277 >> Checkpoint destination directory ../out/llama2-7b-p0.001-lora-seed3/checkpoint-16 already exists and is non-empty.Saving will proceed but saved results may be invalid.
{'loss': 1.5983, 'learning_rate': 1.4838709677419357e-05, 'epoch': 1.07}
{'loss': 1.4594, 'learning_rate': 1.4193548387096776e-05, 'epoch': 1.19}
{'loss': 1.6228, 'learning_rate': 1.3548387096774194e-05, 'epoch': 1.3}
{'loss': 1.2629, 'learning_rate': 1.2903225806451613e-05, 'epoch': 1.42}
{'loss': 1.2212, 'learning_rate': 1.2258064516129034e-05, 'epoch': 1.54}
{'loss': 1.274, 'learning_rate': 1.1612903225806453e-05, 'epoch': 1.66}
{'loss': 1.2343, 'learning_rate': 1.096774193548387e-05, 'epoch': 1.78}
{'loss': 1.2514, 'learning_rate': 1.0322580645161291e-05, 'epoch': 1.9}
[INFO|trainer.py:2889] 2024-11-18 15:24:40,432 >> Saving model checkpoint to ../out/llama2-7b-p0.001-lora-seed3/checkpoint-16
[INFO|tokenization_utils_base.py:2432] 2024-11-18 15:24:40,771 >> tokenizer config file saved in ../out/llama2-7b-p0.001-lora-seed3/checkpoint-16/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-18 15:24:40,774 >> Special tokens file saved in ../out/llama2-7b-p0.001-lora-seed3/checkpoint-16/special_tokens_map.json
 78%|███████▊  | 25/32 [03:46<00:56,  8.12s/it][WARNING|trainer.py:2348] 2024-11-18 15:26:01,050 >> Checkpoint destination directory ../out/llama2-7b-p0.001-lora-seed3/checkpoint-25 already exists and is non-empty.Saving will proceed but saved results may be invalid.
{'loss': 1.2155, 'learning_rate': 9.67741935483871e-06, 'epoch': 2.01}
{'loss': 0.9649, 'learning_rate': 9.03225806451613e-06, 'epoch': 2.13}
{'loss': 1.1894, 'learning_rate': 8.387096774193549e-06, 'epoch': 2.25}
{'loss': 1.1849, 'learning_rate': 7.741935483870968e-06, 'epoch': 2.37}
{'loss': 1.2559, 'learning_rate': 7.096774193548388e-06, 'epoch': 2.49}
{'loss': 1.2332, 'learning_rate': 6.451612903225806e-06, 'epoch': 2.61}
{'loss': 1.0197, 'learning_rate': 5.806451612903226e-06, 'epoch': 2.73}
{'loss': 1.1801, 'learning_rate': 5.161290322580646e-06, 'epoch': 2.84}
{'loss': 1.1599, 'learning_rate': 4.516129032258065e-06, 'epoch': 2.96}
[INFO|trainer.py:2889] 2024-11-18 15:26:01,200 >> Saving model checkpoint to ../out/llama2-7b-p0.001-lora-seed3/checkpoint-25
[INFO|tokenization_utils_base.py:2432] 2024-11-18 15:26:01,534 >> tokenizer config file saved in ../out/llama2-7b-p0.001-lora-seed3/checkpoint-25/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-18 15:26:01,537 >> Special tokens file saved in ../out/llama2-7b-p0.001-lora-seed3/checkpoint-25/special_tokens_map.json
100%|██████████| 32/32 [04:55<00:00,  8.45s/it][WARNING|trainer.py:2348] 2024-11-18 15:27:08,106 >> Checkpoint destination directory ../out/llama2-7b-p0.001-lora-seed3/checkpoint-32 already exists and is non-empty.Saving will proceed but saved results may be invalid.
{'loss': 1.1986, 'learning_rate': 3.870967741935484e-06, 'epoch': 3.08}
{'loss': 0.9487, 'learning_rate': 3.225806451612903e-06, 'epoch': 3.2}
{'loss': 0.9426, 'learning_rate': 2.580645161290323e-06, 'epoch': 3.32}
{'loss': 1.1871, 'learning_rate': 1.935483870967742e-06, 'epoch': 3.44}
{'loss': 1.1138, 'learning_rate': 1.2903225806451614e-06, 'epoch': 3.56}
{'loss': 1.0968, 'learning_rate': 6.451612903225807e-07, 'epoch': 3.67}
{'loss': 1.3581, 'learning_rate': 0.0, 'epoch': 3.79}
[INFO|trainer.py:2889] 2024-11-18 15:27:08,250 >> Saving model checkpoint to ../out/llama2-7b-p0.001-lora-seed3/checkpoint-32
[INFO|tokenization_utils_base.py:2432] 2024-11-18 15:27:08,588 >> tokenizer config file saved in ../out/llama2-7b-p0.001-lora-seed3/checkpoint-32/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-18 15:27:08,591 >> Special tokens file saved in ../out/llama2-7b-p0.001-lora-seed3/checkpoint-32/special_tokens_map.json
[INFO|trainer.py:1947] 2024-11-18 15:27:22,127 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████| 32/32 [05:09<00:00,  9.68s/it]
{'train_runtime': 310.3002, 'train_samples_per_second': 3.481, 'train_steps_per_second': 0.103, 'train_loss': 1.6527687348425388, 'epoch': 3.79}
after training done!!!!
[INFO|trainer.py:2889] 2024-11-18 15:27:22,174 >> Saving model checkpoint to ../out/llama2-7b-p0.001-lora-seed3
[INFO|tokenization_utils_base.py:2432] 2024-11-18 15:27:22,504 >> tokenizer config file saved in ../out/llama2-7b-p0.001-lora-seed3/tokenizer_config.json
[INFO|tokenization_utils_base.py:2441] 2024-11-18 15:27:22,507 >> Special tokens file saved in ../out/llama2-7b-p0.001-lora-seed3/special_tokens_map.json
***** train metrics *****
  epoch                    =       3.79
  train_loss               =     1.6528
  train_runtime            = 0:05:10.30
  train_samples            =        270
  train_samples_per_second =      3.481
  train_steps_per_second   =      0.103
