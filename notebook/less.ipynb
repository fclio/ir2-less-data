{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1524,
     "status": "ok",
     "timestamp": 1730929473302,
     "user": {
      "displayName": "Feng Clio",
      "userId": "12683812963779754580"
     },
     "user_tz": -60
    },
    "id": "R6JL6uKgkPXe",
    "outputId": "ad0e9b46-bb14-4954-a6b5-88bcaaa19ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LESS'...\n",
      "remote: Enumerating objects: 209, done.\u001b[K\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
      "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
      "remote: Total 209 (delta 22), reused 21 (delta 19), pack-reused 170 (from 1)\u001b[K\n",
      "Receiving objects: 100% (209/209), 386.64 KiB | 2.48 MiB/s, done.\n",
      "Resolving deltas: 100% (80/80), done.\n",
      "/content/LESS\n"
     ]
    }
   ],
   "source": [
    "# Clone the LESS repository\n",
    "!git clone https://github.com/princeton-nlp/LESS.git\n",
    "%cd LESS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1730929475767,
     "user": {
      "displayName": "Feng Clio",
      "userId": "12683812963779754580"
     },
     "user_tz": -60
    },
    "id": "BpaKXmm0kdIL",
    "outputId": "486a2f3a-b463-4889-e67d-f6f96bc10730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment.py  less.ipynb\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167906,
     "status": "ok",
     "timestamp": 1730929645370,
     "user": {
      "displayName": "Feng Clio",
      "userId": "12683812963779754580"
     },
     "user_tz": -60
    },
    "id": "ingQUuuvkT7s",
    "outputId": "370597b4-ef1a-40b9-8df0-ec9506191e5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft==0.7.1 (from -r requirement.txt (line 1))\n",
      "  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting transformers==4.36.2 (from -r requirement.txt (line 2))\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting traker==0.1.3 (from traker[fast]==0.1.3->-r requirement.txt (line 3))\n",
      "  Downloading traker-0.1.3.tar.gz (24 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r requirement.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r requirement.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r requirement.txt (line 1)) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r requirement.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r requirement.txt (line 1)) (2.5.0+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r requirement.txt (line 1)) (4.66.6)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r requirement.txt (line 1)) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r requirement.txt (line 1)) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.1->-r requirement.txt (line 1)) (0.24.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2->-r requirement.txt (line 2)) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2->-r requirement.txt (line 2)) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2->-r requirement.txt (line 2)) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.2->-r requirement.txt (line 2))\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting fast_jl (from traker[fast]==0.1.3->-r requirement.txt (line 3))\n",
      "  Downloading fast_jl-0.1.3.tar.gz (5.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.1->-r requirement.txt (line 1)) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.1->-r requirement.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.1->-r requirement.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.1->-r requirement.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.1->-r requirement.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.7.1->-r requirement.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2->-r requirement.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2->-r requirement.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2->-r requirement.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2->-r requirement.txt (line 2)) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.7.1->-r requirement.txt (line 1)) (3.0.2)\n",
      "Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: traker, fast_jl\n",
      "  Building wheel for traker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for traker: filename=traker-0.1.3-py3-none-any.whl size=20665 sha256=d75825d5eb6d72992532eeb922d4452d08ac949ff2b1815317b9ac5f37764caf\n",
      "  Stored in directory: /root/.cache/pip/wheels/88/78/0d/a7832935317836db11135fe894ad3e3e170b37abff2893517d\n",
      "  Building wheel for fast_jl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fast_jl: filename=fast_jl-0.1.3-cp310-cp310-linux_x86_64.whl size=971501 sha256=48de911cc589237484ffcb6b7dac5862d67512bcb40889b2028ba3a3a2b8f2ce\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/5e/37/3a9d828d49fbbcb9ba54a11d3b48a5a5ac627bd8beb46bf05b\n",
      "Successfully built traker fast_jl\n",
      "Installing collected packages: traker, tokenizers, fast_jl, transformers, peft\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.44.2\n",
      "    Uninstalling transformers-4.44.2:\n",
      "      Successfully uninstalled transformers-4.44.2\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.13.2\n",
      "    Uninstalling peft-0.13.2:\n",
      "      Successfully uninstalled peft-0.13.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fast_jl-0.1.3 peft-0.7.1 tokenizers-0.15.2 traker-0.1.3 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirement.txt\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16895,
     "status": "ok",
     "timestamp": 1730929848933,
     "user": {
      "displayName": "Feng Clio",
      "userId": "12683812963779754580"
     },
     "user_tz": -60
    },
    "id": "rWbJUAX5stze",
    "outputId": "33f9b65d-b834-4165-db10-8195d933cc90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197,
     "status": "ok",
     "timestamp": 1730930112180,
     "user": {
      "displayName": "Feng Clio",
      "userId": "12683812963779754580"
     },
     "user_tz": -60
    },
    "id": "0udcEOUOdbzV",
    "outputId": "bc2e123e-9bfe-451f-a232-a358ae7ed63d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1730930154228,
     "user": {
      "displayName": "Feng Clio",
      "userId": "12683812963779754580"
     },
     "user_tz": -60
    },
    "id": "c3UN_obadmek",
    "outputId": "4472d45f-a465-4173-8742-971ec7ace4f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/LESS\n"
     ]
    }
   ],
   "source": [
    "%cd LESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1730932104650,
     "user": {
      "displayName": "Feng Clio",
      "userId": "12683812963779754580"
     },
     "user_tz": -60
    },
    "id": "myP6zd5egfEj",
    "outputId": "46be59f6-7e35-4217-ecfe-a5cbfdf46148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your_token' with your actual Hugging Face access token. and enable public setting.\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1730932334623,
     "user": {
      "displayName": "Feng Clio",
      "userId": "12683812963779754580"
     },
     "user_tz": -60
    },
    "id": "TuK23Rwszb6p"
   },
   "outputs": [],
   "source": [
    "# Define variables\n",
    "DATA_DIR=\"../drive/MyDrive/school/ir2/less-data/data\"  # Change to a mounted path if using Google Drive\n",
    "MODEL_PATH=\"meta-llama/Llama-2-7b-hf\"  # Ensure this model path is accessible\n",
    "# MODEL_PATH=\"mistralai/Mistral-7B-v0.1\"  # Ensure this model path is accessible\n",
    "# MODEL_PATH=\"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "PERCENTAGE=0.05\n",
    "DATA_SEED=3\n",
    "JOB_NAME=f\"llama2-7b-p{PERCENTAGE}-lora-seed{DATA_SEED}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67040,
     "status": "ok",
     "timestamp": 1730933123912,
     "user": {
      "displayName": "Feng Clio",
      "userId": "12683812963779754580"
     },
     "user_tz": -60
    },
    "id": "otRNi6vH0kpv",
    "outputId": "fcc77c86-8c10-4567-ade0-9dc3c1821dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1106 22:44:23.793000 15869 torch/distributed/run.py:785] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-11-06 22:44:36.134538: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-06 22:44:36.498221: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-06 22:44:36.592406: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-06 22:44:39.915946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/state.py:245: UserWarning: OMP_NUM_THREADS/MKL_NUM_THREADS unset, we set it at 1 to improve oob performance.\n",
      "  warnings.warn(\n",
      "11/06/2024 22:44:41 - WARNING - __main__ - Process rank: 0, device: cpu:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "11/06/2024 22:44:41 - INFO - __main__ - Training parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "analysis_dataset=bbh,\n",
      "analysis_mode=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=3,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=32,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../out/llama2-7b-p0.05-lora-seed3/runs/Nov06_22-44-41_02368561dd1f,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=4.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=../out/llama2-7b-p0.05-lora-seed3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../out/llama2-7b-p0.05-lora-seed3,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=0,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataset_names=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "11/06/2024 22:44:41 - INFO - __main__ - Model parameters ModelArguments(model_name_or_path='meta-llama/Llama-2-7b-hf', config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False, torch_dtype=None, lora=True, lora_r=128, lora_alpha=512.0, lora_dropout=0.1, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'])\n",
      "11/06/2024 22:44:41 - INFO - __main__ - Dataset parameters DataArguments(train_files=['../drive/MyDrive/school/ir2/less-data/data/train/processed/flan_v2/flan_v2_data.jsonl', '../drive/MyDrive/school/ir2/less-data/data/train/processed/cot/cot_data.jsonl', '../drive/MyDrive/school/ir2/less-data/data/train/processed/dolly/dolly_data.jsonl', '../drive/MyDrive/school/ir2/less-data/data/train/processed/oasst1/oasst1_data.jsonl'], overwrite_cache=False, preprocessing_num_workers=None, max_seq_length=2048, sample_data_seed=42, percentage=0.05)\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2026] 2024-11-06 22:44:41,738 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2026] 2024-11-06 22:44:41,739 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2024-11-06 22:44:41,739 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2026] 2024-11-06 22:44:41,739 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2026] 2024-11-06 22:44:41,739 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/tokenizer_config.json\n",
      "Using custom data configuration default-9517500a5323b38a\n",
      "11/06/2024 22:44:42 - INFO - datasets.builder - Using custom data configuration default-9517500a5323b38a\n",
      "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
      "11/06/2024 22:44:42 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "11/06/2024 22:44:42 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "11/06/2024 22:44:42 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "11/06/2024 22:44:42 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "11/06/2024 22:44:42 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "Process #0 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00000_of_00010.arrow\n",
      "11/06/2024 22:44:44 - INFO - datasets.arrow_dataset - Process #0 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00000_of_00010.arrow\n",
      "Process #1 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00001_of_00010.arrow\n",
      "11/06/2024 22:44:44 - INFO - datasets.arrow_dataset - Process #1 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00001_of_00010.arrow\n",
      "Process #2 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00002_of_00010.arrow\n",
      "11/06/2024 22:44:44 - INFO - datasets.arrow_dataset - Process #2 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00002_of_00010.arrow\n",
      "Process #3 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00003_of_00010.arrow\n",
      "11/06/2024 22:44:44 - INFO - datasets.arrow_dataset - Process #3 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00003_of_00010.arrow\n",
      "Process #4 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00004_of_00010.arrow\n",
      "11/06/2024 22:44:44 - INFO - datasets.arrow_dataset - Process #4 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00004_of_00010.arrow\n",
      "Process #5 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00005_of_00010.arrow\n",
      "11/06/2024 22:44:44 - INFO - datasets.arrow_dataset - Process #5 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00005_of_00010.arrow\n",
      "Process #6 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00006_of_00010.arrow\n",
      "11/06/2024 22:44:44 - INFO - datasets.arrow_dataset - Process #6 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00006_of_00010.arrow\n",
      "Process #7 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00007_of_00010.arrow\n",
      "11/06/2024 22:44:44 - INFO - datasets.arrow_dataset - Process #7 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00007_of_00010.arrow\n",
      "Process #8 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00008_of_00010.arrow\n",
      "11/06/2024 22:44:44 - INFO - datasets.arrow_dataset - Process #8 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00008_of_00010.arrow\n",
      "Process #9 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00009_of_00010.arrow\n",
      "11/06/2024 22:44:44 - INFO - datasets.arrow_dataset - Process #9 will write at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_00009_of_00010.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_*_of_00010.arrow\n",
      "11/06/2024 22:44:44 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-9517500a5323b38a/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-2122180f8b00a2e8_*_of_00010.arrow\n",
      "Concatenating 10 shards\n",
      "11/06/2024 22:44:44 - INFO - datasets.arrow_dataset - Concatenating 10 shards\n",
      "[INFO|configuration_utils.py:739] 2024-11-06 22:44:44,526 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/config.json\n",
      "[INFO|configuration_utils.py:802] 2024-11-06 22:44:44,530 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3344] 2024-11-06 22:44:44,685 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/model.safetensors.index.json\n",
      "[INFO|configuration_utils.py:826] 2024-11-06 22:44:44,691 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]y\n",
      "E1106 22:45:20.808000 15869 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -9) local_rank: 0 (pid: 15902) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "======================================================\n",
      "less.train.train FAILED\n",
      "------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-11-06_22:45:20\n",
      "  host      : 02368561dd1f\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : -9 (pid: 15902)\n",
      "  error_file: <N/A>\n",
      "  traceback : Signal 9 (SIGKILL) received by PID 15902\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the script with specified arguments\n",
    "!bash ./less/scripts/train/warmup_lora_train.sh \"$DATA_DIR\" \"$MODEL_PATH\" \"$PERCENTAGE\" \"$DATA_SEED\" \"$JOB_NAME\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2plbv30n0og7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJ1PvKSp8W/5JvPSwwPtwk",
   "mount_file_id": "1HGvhVLPvIMCS9-4shDIJNmzlYBLZS5cq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
